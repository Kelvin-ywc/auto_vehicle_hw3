[2024-11-10 17:42:46 cros_tiny_patch4_group7_224] (main.py 386): INFO Full config saved to output/log/debug/config.json
[2024-11-10 17:42:46 cros_tiny_patch4_group7_224] (main.py 389): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 1
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: true
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/crossformer-t.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 9.765625e-07
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 9.765625e-09
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 9.765625e-10
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 17:42:49 cros_tiny_patch4_group7_224] (main.py 94): INFO Creating model:cross-scale/cros_tiny_patch4_group7_224
[2024-11-10 17:42:50 cros_tiny_patch4_group7_224] (main.py 97): INFO CrossFormer(
  (patch_embed): PatchEmbed(
    (projs): ModuleList(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(4, 4))
      (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (2): Conv2d(3, 8, kernel_size=(16, 16), stride=(4, 4), padding=(6, 6))
      (3): Conv2d(3, 8, kernel_size=(32, 32), stride=(4, 4), padding=(14, 14))
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): Stage(
      dim=64, input_resolution=(56, 56), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=64, input_resolution=(56, 56), num_heads=2, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=8
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=64, group_size=(7, 7), num_heads=2
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=4, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=2, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=64, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=64
        (reductions): ModuleList(
          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Stage(
      dim=128, input_resolution=(28, 28), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=128, input_resolution=(28, 28), num_heads=4, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=4
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=128, group_size=(7, 7), num_heads=4
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=8, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=4, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.007)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=128
        (reductions): ModuleList(
          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): Stage(
      dim=256, input_resolution=(14, 14), depth=8
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.020)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.047)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.060)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=256
        (reductions): ModuleList(
          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): Stage(
      dim=512, input_resolution=(7, 7), depth=6
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=512, out_features=1000, bias=True)
)
[2024-11-10 17:42:50 cros_tiny_patch4_group7_224] (main.py 107): INFO number of params: 27804848
[2024-11-10 17:42:50 cros_tiny_patch4_group7_224] (main.py 110): INFO number of GFLOPs: 2.858292352
[2024-11-10 17:42:50 cros_tiny_patch4_group7_224] (utils.py 42): INFO ==============> Resuming from ./model_ckpt/crossformer-t.pth....................
[2024-11-10 17:42:50 cros_tiny_patch4_group7_224] (utils.py 49): INFO _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.rpb.relative_position_bias_table', 'layers.1.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.1.attn.rpb.relative_position_bias_table', 'layers.2.blocks.2.attn.rpb.relative_position_bias_table', 'layers.2.blocks.3.attn.rpb.relative_position_bias_table', 'layers.2.blocks.4.attn.rpb.relative_position_bias_table', 'layers.2.blocks.5.attn.rpb.relative_position_bias_table', 'layers.2.blocks.6.attn.rpb.relative_position_bias_table', 'layers.2.blocks.7.attn.rpb.relative_position_bias_table', 'layers.3.blocks.0.attn.rpb.relative_position_bias_table', 'layers.3.blocks.1.attn.rpb.relative_position_bias_table', 'layers.3.blocks.2.attn.rpb.relative_position_bias_table', 'layers.3.blocks.3.attn.rpb.relative_position_bias_table', 'layers.3.blocks.4.attn.rpb.relative_position_bias_table', 'layers.3.blocks.5.attn.rpb.relative_position_bias_table'], unexpected_keys=['layers.0.blocks.0.attn.biases', 'layers.0.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.attn.biases', 'layers.1.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.attn.biases', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.biases', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.biases', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.biases', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.biases', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.biases', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.biases', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.biases', 'layers.2.blocks.7.attn.relative_position_index', 'layers.3.blocks.0.attn.biases', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.biases', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.2.attn.biases', 'layers.3.blocks.2.attn.relative_position_index', 'layers.3.blocks.3.attn.biases', 'layers.3.blocks.3.attn.relative_position_index', 'layers.3.blocks.4.attn.biases', 'layers.3.blocks.4.attn.relative_position_index', 'layers.3.blocks.5.attn.biases', 'layers.3.blocks.5.attn.relative_position_index'])
[2024-11-10 17:42:52 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [0/50000]	Time 1.888 (1.888)	Loss 0.1725 (0.1725)	Epoch 0	Acc@1 100.000 (100.000)	Acc@5 100.000 (100.000)	Mem 269MB
[2024-11-10 17:42:54 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [50/50000]	Time 0.032 (0.069)	Loss 4.1953 (0.7927)	Epoch 0	Acc@1 0.000 (82.353)	Acc@5 100.000 (98.039)	Mem 269MB
[2024-11-10 17:42:55 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [100/50000]	Time 0.031 (0.051)	Loss 6.4805 (1.0855)	Epoch 0	Acc@1 0.000 (75.248)	Acc@5 0.000 (94.059)	Mem 269MB
[2024-11-10 17:42:57 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [150/50000]	Time 0.031 (0.045)	Loss 1.0283 (0.9941)	Epoch 0	Acc@1 0.000 (77.483)	Acc@5 100.000 (93.377)	Mem 269MB
[2024-11-10 17:42:58 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [200/50000]	Time 0.031 (0.041)	Loss 2.7480 (0.9429)	Epoch 0	Acc@1 0.000 (79.602)	Acc@5 100.000 (94.527)	Mem 269MB
[2024-11-10 17:43:00 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [250/50000]	Time 0.031 (0.039)	Loss 4.1953 (0.9172)	Epoch 0	Acc@1 0.000 (80.080)	Acc@5 100.000 (95.219)	Mem 269MB
[2024-11-10 17:43:01 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [300/50000]	Time 0.032 (0.038)	Loss 3.8047 (0.9659)	Epoch 0	Acc@1 0.000 (79.070)	Acc@5 100.000 (94.684)	Mem 269MB
[2024-11-10 17:43:03 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [350/50000]	Time 0.031 (0.037)	Loss 0.1979 (0.9795)	Epoch 0	Acc@1 100.000 (78.348)	Acc@5 100.000 (95.157)	Mem 269MB
[2024-11-10 17:43:05 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [400/50000]	Time 0.031 (0.036)	Loss 0.0893 (0.9974)	Epoch 0	Acc@1 100.000 (78.554)	Acc@5 100.000 (94.264)	Mem 269MB
[2024-11-10 17:43:06 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [450/50000]	Time 0.031 (0.036)	Loss 0.1541 (0.9324)	Epoch 0	Acc@1 100.000 (80.044)	Acc@5 100.000 (94.900)	Mem 269MB
[2024-11-10 17:43:08 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [500/50000]	Time 0.031 (0.035)	Loss 0.1122 (0.9773)	Epoch 0	Acc@1 100.000 (79.441)	Acc@5 100.000 (94.212)	Mem 269MB
[2024-11-10 17:43:09 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [550/50000]	Time 0.031 (0.035)	Loss 0.1694 (0.9318)	Epoch 0	Acc@1 100.000 (80.399)	Acc@5 100.000 (94.555)	Mem 269MB
[2024-11-10 17:43:11 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [600/50000]	Time 0.031 (0.035)	Loss 0.1986 (0.9187)	Epoch 0	Acc@1 100.000 (80.532)	Acc@5 100.000 (94.343)	Mem 269MB
[2024-11-10 17:43:12 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [650/50000]	Time 0.032 (0.034)	Loss 0.1283 (0.9195)	Epoch 0	Acc@1 100.000 (80.031)	Acc@5 100.000 (94.470)	Mem 269MB
[2024-11-10 17:43:14 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [700/50000]	Time 0.030 (0.034)	Loss 7.1172 (0.9281)	Epoch 0	Acc@1 0.000 (80.171)	Acc@5 0.000 (94.437)	Mem 269MB
[2024-11-10 17:43:15 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [750/50000]	Time 0.031 (0.034)	Loss 0.1257 (0.9293)	Epoch 0	Acc@1 100.000 (79.627)	Acc@5 100.000 (94.541)	Mem 269MB
[2024-11-10 17:43:17 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [800/50000]	Time 0.032 (0.034)	Loss 0.8735 (0.9178)	Epoch 0	Acc@1 100.000 (79.900)	Acc@5 100.000 (94.881)	Mem 269MB
[2024-11-10 17:43:19 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [850/50000]	Time 0.031 (0.034)	Loss 0.3789 (0.9001)	Epoch 0	Acc@1 100.000 (80.141)	Acc@5 100.000 (95.182)	Mem 269MB
[2024-11-10 17:43:20 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [900/50000]	Time 0.030 (0.033)	Loss 0.2051 (0.9060)	Epoch 0	Acc@1 100.000 (80.133)	Acc@5 100.000 (95.117)	Mem 269MB
[2024-11-10 17:43:22 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [950/50000]	Time 0.031 (0.033)	Loss 0.1384 (0.9003)	Epoch 0	Acc@1 100.000 (80.231)	Acc@5 100.000 (95.163)	Mem 269MB
[2024-11-10 17:43:23 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [1000/50000]	Time 0.031 (0.033)	Loss 0.0505 (0.8807)	Epoch 0	Acc@1 100.000 (80.619)	Acc@5 100.000 (95.305)	Mem 269MB
[2024-11-10 17:43:25 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [1050/50000]	Time 0.031 (0.033)	Loss 0.1816 (0.8730)	Epoch 0	Acc@1 100.000 (80.685)	Acc@5 100.000 (95.243)	Mem 269MB
[2024-11-10 17:43:26 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [1100/50000]	Time 0.031 (0.033)	Loss 0.1254 (0.8714)	Epoch 0	Acc@1 100.000 (80.563)	Acc@5 100.000 (95.277)	Mem 269MB
[2024-11-10 17:43:28 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [1150/50000]	Time 0.031 (0.033)	Loss 0.1019 (0.8647)	Epoch 0	Acc@1 100.000 (80.799)	Acc@5 100.000 (95.308)	Mem 269MB
[2024-11-10 17:43:29 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [1200/50000]	Time 0.031 (0.033)	Loss 0.1891 (0.8545)	Epoch 0	Acc@1 100.000 (81.099)	Acc@5 100.000 (95.337)	Mem 269MB
[2024-11-10 17:43:31 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [1250/50000]	Time 0.031 (0.033)	Loss 0.2637 (0.8554)	Epoch 0	Acc@1 100.000 (80.815)	Acc@5 100.000 (95.364)	Mem 269MB
[2024-11-10 17:43:33 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [1300/50000]	Time 0.031 (0.033)	Loss 0.1061 (0.8508)	Epoch 0	Acc@1 100.000 (81.091)	Acc@5 100.000 (95.388)	Mem 269MB
[2024-11-10 17:43:34 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [1350/50000]	Time 0.031 (0.033)	Loss 2.0605 (0.8423)	Epoch 0	Acc@1 0.000 (81.273)	Acc@5 100.000 (95.411)	Mem 269MB
[2024-11-10 17:43:36 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [1400/50000]	Time 0.031 (0.033)	Loss 0.1265 (0.8370)	Epoch 0	Acc@1 100.000 (81.442)	Acc@5 100.000 (95.432)	Mem 269MB
[2024-11-10 17:43:37 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [1450/50000]	Time 0.031 (0.033)	Loss 0.1888 (0.8396)	Epoch 0	Acc@1 100.000 (81.530)	Acc@5 100.000 (95.314)	Mem 269MB
[2024-11-10 17:43:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [1500/50000]	Time 0.031 (0.032)	Loss 0.1719 (0.8530)	Epoch 0	Acc@1 100.000 (81.279)	Acc@5 100.000 (95.003)	Mem 269MB
[2024-11-10 17:43:40 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [1550/50000]	Time 0.030 (0.032)	Loss 0.1415 (0.8491)	Epoch 0	Acc@1 100.000 (81.496)	Acc@5 100.000 (95.164)	Mem 269MB
[2024-11-10 17:43:42 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [1600/50000]	Time 0.031 (0.032)	Loss 0.2015 (0.8663)	Epoch 0	Acc@1 100.000 (81.137)	Acc@5 100.000 (95.128)	Mem 269MB
[2024-11-10 17:43:43 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [1650/50000]	Time 0.031 (0.032)	Loss 0.1903 (0.8569)	Epoch 0	Acc@1 100.000 (81.345)	Acc@5 100.000 (95.215)	Mem 269MB
[2024-11-10 17:43:45 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [1700/50000]	Time 0.031 (0.032)	Loss 0.1294 (0.8529)	Epoch 0	Acc@1 100.000 (81.423)	Acc@5 100.000 (95.297)	Mem 269MB
[2024-11-10 17:43:47 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [1750/50000]	Time 0.030 (0.032)	Loss 2.7832 (0.8534)	Epoch 0	Acc@1 0.000 (81.439)	Acc@5 100.000 (95.203)	Mem 269MB
[2024-11-10 17:43:48 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [1800/50000]	Time 0.031 (0.032)	Loss 3.7773 (0.8565)	Epoch 0	Acc@1 0.000 (81.455)	Acc@5 0.000 (95.058)	Mem 269MB
[2024-11-10 17:43:50 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [1850/50000]	Time 0.031 (0.032)	Loss 0.1277 (0.8543)	Epoch 0	Acc@1 100.000 (81.469)	Acc@5 100.000 (95.084)	Mem 269MB
[2024-11-10 17:43:51 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [1900/50000]	Time 0.031 (0.032)	Loss 0.1348 (0.8663)	Epoch 0	Acc@1 100.000 (81.220)	Acc@5 100.000 (95.055)	Mem 269MB
[2024-11-10 17:43:53 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [1950/50000]	Time 0.031 (0.032)	Loss 6.1602 (0.8671)	Epoch 0	Acc@1 0.000 (81.138)	Acc@5 0.000 (95.028)	Mem 269MB
[2024-11-10 17:43:54 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [2000/50000]	Time 0.033 (0.032)	Loss 0.1004 (0.8642)	Epoch 0	Acc@1 100.000 (81.009)	Acc@5 100.000 (95.152)	Mem 269MB
[2024-11-10 17:43:56 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [2050/50000]	Time 0.031 (0.032)	Loss 6.2969 (0.8663)	Epoch 0	Acc@1 0.000 (80.936)	Acc@5 0.000 (95.124)	Mem 269MB
[2024-11-10 17:43:58 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [2100/50000]	Time 0.031 (0.032)	Loss 0.0884 (0.8691)	Epoch 0	Acc@1 100.000 (81.009)	Acc@5 100.000 (95.050)	Mem 269MB
[2024-11-10 17:43:59 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [2150/50000]	Time 0.031 (0.032)	Loss 0.1647 (0.8759)	Epoch 0	Acc@1 100.000 (80.846)	Acc@5 100.000 (95.072)	Mem 269MB
[2024-11-10 17:44:01 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [2200/50000]	Time 0.031 (0.032)	Loss 0.6714 (0.8768)	Epoch 0	Acc@1 100.000 (80.827)	Acc@5 100.000 (95.093)	Mem 269MB
[2024-11-10 17:44:02 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [2250/50000]	Time 0.031 (0.032)	Loss 1.3887 (0.8740)	Epoch 0	Acc@1 0.000 (80.809)	Acc@5 100.000 (95.158)	Mem 269MB
[2024-11-10 17:44:04 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [2300/50000]	Time 0.031 (0.032)	Loss 0.1545 (0.8745)	Epoch 0	Acc@1 100.000 (80.791)	Acc@5 100.000 (95.133)	Mem 269MB
[2024-11-10 17:44:05 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [2350/50000]	Time 0.031 (0.032)	Loss 1.7480 (0.8770)	Epoch 0	Acc@1 100.000 (80.647)	Acc@5 100.000 (95.151)	Mem 269MB
[2024-11-10 17:44:07 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [2400/50000]	Time 0.031 (0.032)	Loss 0.1879 (0.8753)	Epoch 0	Acc@1 100.000 (80.716)	Acc@5 100.000 (95.127)	Mem 269MB
[2024-11-10 17:44:08 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [2450/50000]	Time 0.031 (0.032)	Loss 0.2163 (0.8764)	Epoch 0	Acc@1 100.000 (80.661)	Acc@5 100.000 (95.145)	Mem 269MB
[2024-11-10 17:44:10 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [2500/50000]	Time 0.031 (0.032)	Loss 0.1454 (0.8865)	Epoch 0	Acc@1 100.000 (80.528)	Acc@5 100.000 (95.122)	Mem 269MB
[2024-11-10 17:44:12 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [2550/50000]	Time 0.031 (0.032)	Loss 9.4141 (0.8903)	Epoch 0	Acc@1 0.000 (80.596)	Acc@5 0.000 (95.061)	Mem 269MB
[2024-11-10 17:44:13 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [2600/50000]	Time 0.031 (0.032)	Loss 0.1798 (0.8920)	Epoch 0	Acc@1 100.000 (80.661)	Acc@5 100.000 (95.040)	Mem 269MB
[2024-11-10 17:44:15 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [2650/50000]	Time 0.033 (0.032)	Loss 0.1205 (0.8925)	Epoch 0	Acc@1 100.000 (80.611)	Acc@5 100.000 (95.096)	Mem 269MB
[2024-11-10 17:44:16 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [2700/50000]	Time 0.031 (0.032)	Loss 0.0891 (0.9014)	Epoch 0	Acc@1 100.000 (80.452)	Acc@5 100.000 (94.965)	Mem 269MB
[2024-11-10 17:44:18 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [2750/50000]	Time 0.031 (0.032)	Loss 0.1475 (0.8940)	Epoch 0	Acc@1 100.000 (80.662)	Acc@5 100.000 (95.056)	Mem 269MB
[2024-11-10 17:44:19 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [2800/50000]	Time 0.031 (0.032)	Loss 0.1987 (0.8898)	Epoch 0	Acc@1 100.000 (80.685)	Acc@5 100.000 (95.109)	Mem 269MB
[2024-11-10 17:44:21 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [2850/50000]	Time 0.031 (0.032)	Loss 0.2302 (0.8966)	Epoch 0	Acc@1 100.000 (80.463)	Acc@5 100.000 (95.054)	Mem 269MB
[2024-11-10 17:44:23 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [2900/50000]	Time 0.032 (0.032)	Loss 0.5093 (0.8924)	Epoch 0	Acc@1 100.000 (80.558)	Acc@5 100.000 (95.036)	Mem 269MB
[2024-11-10 17:44:24 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [2950/50000]	Time 0.032 (0.032)	Loss 0.1917 (0.8941)	Epoch 0	Acc@1 100.000 (80.583)	Acc@5 100.000 (94.917)	Mem 269MB
[2024-11-10 17:44:26 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [3000/50000]	Time 0.032 (0.032)	Loss 0.3623 (0.8923)	Epoch 0	Acc@1 100.000 (80.573)	Acc@5 100.000 (94.935)	Mem 269MB
[2024-11-10 17:44:28 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [3050/50000]	Time 0.032 (0.032)	Loss 0.2546 (0.8932)	Epoch 0	Acc@1 100.000 (80.531)	Acc@5 100.000 (94.920)	Mem 269MB
[2024-11-10 17:44:29 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [3100/50000]	Time 0.033 (0.032)	Loss 0.1539 (0.8935)	Epoch 0	Acc@1 100.000 (80.490)	Acc@5 100.000 (94.969)	Mem 269MB
[2024-11-10 17:44:31 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [3150/50000]	Time 0.032 (0.032)	Loss 0.2957 (0.8941)	Epoch 0	Acc@1 100.000 (80.546)	Acc@5 100.000 (94.954)	Mem 269MB
[2024-11-10 17:44:33 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [3200/50000]	Time 0.032 (0.032)	Loss 0.1299 (0.8915)	Epoch 0	Acc@1 100.000 (80.506)	Acc@5 100.000 (94.970)	Mem 269MB
[2024-11-10 17:44:34 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [3250/50000]	Time 0.032 (0.032)	Loss 0.6821 (0.8891)	Epoch 0	Acc@1 100.000 (80.437)	Acc@5 100.000 (95.048)	Mem 269MB
[2024-11-10 17:44:36 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [3300/50000]	Time 0.032 (0.032)	Loss 0.1880 (0.8900)	Epoch 0	Acc@1 100.000 (80.430)	Acc@5 100.000 (94.971)	Mem 269MB
[2024-11-10 17:44:38 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [3350/50000]	Time 0.032 (0.032)	Loss 2.2324 (0.8863)	Epoch 0	Acc@1 0.000 (80.543)	Acc@5 100.000 (94.987)	Mem 269MB
[2024-11-10 17:44:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [3400/50000]	Time 0.032 (0.032)	Loss 0.0621 (0.8822)	Epoch 0	Acc@1 100.000 (80.623)	Acc@5 100.000 (95.060)	Mem 269MB
[2024-11-10 17:44:41 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [3450/50000]	Time 0.032 (0.032)	Loss 0.0245 (0.8819)	Epoch 0	Acc@1 100.000 (80.672)	Acc@5 100.000 (95.045)	Mem 269MB
[2024-11-10 17:44:42 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [3500/50000]	Time 0.032 (0.032)	Loss 0.6035 (0.8821)	Epoch 0	Acc@1 100.000 (80.691)	Acc@5 100.000 (95.030)	Mem 269MB
[2024-11-10 17:44:44 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [3550/50000]	Time 0.032 (0.032)	Loss 0.1582 (0.8769)	Epoch 0	Acc@1 100.000 (80.794)	Acc@5 100.000 (95.100)	Mem 269MB
[2024-11-10 17:44:46 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [3600/50000]	Time 0.032 (0.032)	Loss 5.3438 (0.8816)	Epoch 0	Acc@1 0.000 (80.728)	Acc@5 100.000 (95.140)	Mem 269MB
[2024-11-10 17:44:47 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [3650/50000]	Time 0.032 (0.032)	Loss 0.1385 (0.8784)	Epoch 0	Acc@1 100.000 (80.800)	Acc@5 100.000 (95.152)	Mem 269MB
[2024-11-10 17:44:49 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [3700/50000]	Time 0.032 (0.032)	Loss 2.4609 (0.8739)	Epoch 0	Acc@1 0.000 (80.897)	Acc@5 100.000 (95.163)	Mem 269MB
[2024-11-10 17:44:51 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [3750/50000]	Time 0.032 (0.032)	Loss 0.1153 (0.8709)	Epoch 0	Acc@1 100.000 (80.938)	Acc@5 100.000 (95.201)	Mem 269MB
[2024-11-10 17:44:52 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [3800/50000]	Time 0.032 (0.032)	Loss 5.2383 (0.8705)	Epoch 0	Acc@1 0.000 (81.031)	Acc@5 0.000 (95.159)	Mem 269MB
[2024-11-10 17:44:54 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [3850/50000]	Time 0.032 (0.032)	Loss 0.2384 (0.8661)	Epoch 0	Acc@1 100.000 (81.096)	Acc@5 100.000 (95.222)	Mem 269MB
[2024-11-10 17:44:56 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [3900/50000]	Time 0.032 (0.032)	Loss 0.1361 (0.8638)	Epoch 0	Acc@1 100.000 (81.133)	Acc@5 100.000 (95.232)	Mem 269MB
[2024-11-10 17:44:57 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [3950/50000]	Time 0.032 (0.032)	Loss 0.1517 (0.8638)	Epoch 0	Acc@1 100.000 (81.119)	Acc@5 100.000 (95.242)	Mem 269MB
[2024-11-10 17:44:59 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [4000/50000]	Time 0.032 (0.032)	Loss 3.8242 (0.8627)	Epoch 0	Acc@1 0.000 (81.155)	Acc@5 0.000 (95.226)	Mem 269MB
[2024-11-10 17:45:01 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [4050/50000]	Time 0.032 (0.032)	Loss 0.1453 (0.8596)	Epoch 0	Acc@1 100.000 (81.215)	Acc@5 100.000 (95.285)	Mem 269MB
[2024-11-10 17:45:02 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [4100/50000]	Time 0.032 (0.032)	Loss 0.3228 (0.8586)	Epoch 0	Acc@1 100.000 (81.224)	Acc@5 100.000 (95.269)	Mem 269MB
[2024-11-10 17:45:04 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [4150/50000]	Time 0.032 (0.032)	Loss 0.8599 (0.8547)	Epoch 0	Acc@1 100.000 (81.306)	Acc@5 100.000 (95.278)	Mem 269MB
[2024-11-10 17:45:05 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [4200/50000]	Time 0.033 (0.032)	Loss 0.1035 (0.8535)	Epoch 0	Acc@1 100.000 (81.385)	Acc@5 100.000 (95.263)	Mem 269MB
[2024-11-10 17:45:07 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [4250/50000]	Time 0.032 (0.032)	Loss 2.8008 (0.8530)	Epoch 0	Acc@1 0.000 (81.322)	Acc@5 100.000 (95.295)	Mem 269MB
[2024-11-10 17:45:09 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [4300/50000]	Time 0.033 (0.032)	Loss 0.9927 (0.8524)	Epoch 0	Acc@1 100.000 (81.376)	Acc@5 100.000 (95.280)	Mem 269MB
[2024-11-10 17:45:10 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [4350/50000]	Time 0.033 (0.032)	Loss 0.1135 (0.8505)	Epoch 0	Acc@1 100.000 (81.384)	Acc@5 100.000 (95.288)	Mem 269MB
[2024-11-10 17:45:12 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [4400/50000]	Time 0.032 (0.032)	Loss 0.1545 (0.8496)	Epoch 0	Acc@1 100.000 (81.413)	Acc@5 100.000 (95.342)	Mem 269MB
[2024-11-10 17:45:13 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [4450/50000]	Time 0.032 (0.032)	Loss 2.0703 (0.8493)	Epoch 0	Acc@1 0.000 (81.353)	Acc@5 100.000 (95.349)	Mem 269MB
[2024-11-10 17:45:15 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [4500/50000]	Time 0.033 (0.032)	Loss 0.1775 (0.8474)	Epoch 0	Acc@1 100.000 (81.382)	Acc@5 100.000 (95.379)	Mem 269MB
[2024-11-10 17:45:17 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [4550/50000]	Time 0.032 (0.032)	Loss 0.1849 (0.8483)	Epoch 0	Acc@1 100.000 (81.367)	Acc@5 100.000 (95.364)	Mem 269MB
[2024-11-10 17:45:18 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [4600/50000]	Time 0.032 (0.032)	Loss 7.2383 (0.8503)	Epoch 0	Acc@1 0.000 (81.395)	Acc@5 0.000 (95.305)	Mem 269MB
[2024-11-10 17:45:20 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [4650/50000]	Time 0.032 (0.032)	Loss 6.6094 (0.8570)	Epoch 0	Acc@1 0.000 (81.316)	Acc@5 100.000 (95.227)	Mem 269MB
[2024-11-10 17:45:22 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [4700/50000]	Time 0.032 (0.032)	Loss 6.4531 (0.8584)	Epoch 0	Acc@1 0.000 (81.323)	Acc@5 0.000 (95.193)	Mem 269MB
[2024-11-10 17:45:23 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [4750/50000]	Time 0.033 (0.032)	Loss 0.2408 (0.8596)	Epoch 0	Acc@1 100.000 (81.267)	Acc@5 100.000 (95.180)	Mem 269MB
[2024-11-10 17:45:25 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [4800/50000]	Time 0.032 (0.032)	Loss 0.3445 (0.8604)	Epoch 0	Acc@1 100.000 (81.254)	Acc@5 100.000 (95.168)	Mem 269MB
[2024-11-10 17:45:26 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [4850/50000]	Time 0.032 (0.032)	Loss 0.1931 (0.8632)	Epoch 0	Acc@1 100.000 (81.220)	Acc@5 100.000 (95.114)	Mem 269MB
[2024-11-10 17:45:28 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [4900/50000]	Time 0.032 (0.032)	Loss 4.5078 (0.8664)	Epoch 0	Acc@1 0.000 (81.167)	Acc@5 100.000 (95.123)	Mem 269MB
[2024-11-10 17:45:30 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [4950/50000]	Time 0.032 (0.032)	Loss 0.1198 (0.8671)	Epoch 0	Acc@1 100.000 (81.176)	Acc@5 100.000 (95.152)	Mem 269MB
[2024-11-10 17:45:31 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [5000/50000]	Time 0.033 (0.032)	Loss 0.1182 (0.8680)	Epoch 0	Acc@1 100.000 (81.184)	Acc@5 100.000 (95.121)	Mem 269MB
[2024-11-10 17:45:33 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [5050/50000]	Time 0.032 (0.032)	Loss 4.6016 (0.8653)	Epoch 0	Acc@1 0.000 (81.251)	Acc@5 100.000 (95.169)	Mem 269MB
[2024-11-10 17:45:35 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [5100/50000]	Time 0.032 (0.032)	Loss 0.1649 (0.8639)	Epoch 0	Acc@1 100.000 (81.278)	Acc@5 100.000 (95.197)	Mem 269MB
[2024-11-10 17:45:36 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [5150/50000]	Time 0.032 (0.032)	Loss 0.0971 (0.8625)	Epoch 0	Acc@1 100.000 (81.324)	Acc@5 100.000 (95.224)	Mem 269MB
[2024-11-10 17:45:38 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [5200/50000]	Time 0.032 (0.032)	Loss 0.1307 (0.8621)	Epoch 0	Acc@1 100.000 (81.292)	Acc@5 100.000 (95.251)	Mem 269MB
[2024-11-10 17:45:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [5250/50000]	Time 0.031 (0.032)	Loss 0.0942 (0.8605)	Epoch 0	Acc@1 100.000 (81.280)	Acc@5 100.000 (95.258)	Mem 269MB
[2024-11-10 17:45:41 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [5300/50000]	Time 0.032 (0.032)	Loss 0.2391 (0.8598)	Epoch 0	Acc@1 100.000 (81.305)	Acc@5 100.000 (95.246)	Mem 269MB
[2024-11-10 17:45:43 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [5350/50000]	Time 0.035 (0.032)	Loss 0.0903 (0.8594)	Epoch 0	Acc@1 100.000 (81.312)	Acc@5 100.000 (95.253)	Mem 269MB
[2024-11-10 17:45:44 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [5400/50000]	Time 0.033 (0.032)	Loss 0.3687 (0.8582)	Epoch 0	Acc@1 100.000 (81.318)	Acc@5 100.000 (95.260)	Mem 269MB
[2024-11-10 17:45:46 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [5450/50000]	Time 0.032 (0.032)	Loss 0.2981 (0.8562)	Epoch 0	Acc@1 100.000 (81.416)	Acc@5 100.000 (95.285)	Mem 269MB
[2024-11-10 17:45:47 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [5500/50000]	Time 0.033 (0.032)	Loss 0.1805 (0.8543)	Epoch 0	Acc@1 100.000 (81.458)	Acc@5 100.000 (95.274)	Mem 269MB
[2024-11-10 17:45:49 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [5550/50000]	Time 0.033 (0.032)	Loss 2.9805 (0.8561)	Epoch 0	Acc@1 0.000 (81.463)	Acc@5 100.000 (95.280)	Mem 269MB
[2024-11-10 17:45:51 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [5600/50000]	Time 0.032 (0.032)	Loss 0.6958 (0.8539)	Epoch 0	Acc@1 100.000 (81.521)	Acc@5 100.000 (95.304)	Mem 269MB
[2024-11-10 17:45:52 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [5650/50000]	Time 0.033 (0.032)	Loss 0.1067 (0.8529)	Epoch 0	Acc@1 100.000 (81.578)	Acc@5 100.000 (95.328)	Mem 269MB
[2024-11-10 17:45:54 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [5700/50000]	Time 0.033 (0.032)	Loss 0.8687 (0.8527)	Epoch 0	Acc@1 0.000 (81.617)	Acc@5 100.000 (95.334)	Mem 269MB
[2024-11-10 17:45:56 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [5750/50000]	Time 0.032 (0.032)	Loss 0.2302 (0.8565)	Epoch 0	Acc@1 100.000 (81.516)	Acc@5 100.000 (95.288)	Mem 269MB
[2024-11-10 17:45:57 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [5800/50000]	Time 0.032 (0.032)	Loss 0.1290 (0.8544)	Epoch 0	Acc@1 100.000 (81.572)	Acc@5 100.000 (95.294)	Mem 269MB
[2024-11-10 17:45:59 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [5850/50000]	Time 0.032 (0.032)	Loss 0.1122 (0.8524)	Epoch 0	Acc@1 100.000 (81.610)	Acc@5 100.000 (95.317)	Mem 269MB
[2024-11-10 17:46:00 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [5900/50000]	Time 0.032 (0.032)	Loss 2.0000 (0.8502)	Epoch 0	Acc@1 0.000 (81.647)	Acc@5 100.000 (95.340)	Mem 269MB
[2024-11-10 17:46:02 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [5950/50000]	Time 0.032 (0.032)	Loss 0.2385 (0.8507)	Epoch 0	Acc@1 100.000 (81.633)	Acc@5 100.000 (95.329)	Mem 269MB
[2024-11-10 17:46:04 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [6000/50000]	Time 0.038 (0.032)	Loss 0.0747 (0.8516)	Epoch 0	Acc@1 100.000 (81.620)	Acc@5 100.000 (95.351)	Mem 269MB
[2024-11-10 17:46:06 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [6050/50000]	Time 0.040 (0.032)	Loss 0.3416 (0.8502)	Epoch 0	Acc@1 100.000 (81.672)	Acc@5 100.000 (95.340)	Mem 269MB
[2024-11-10 17:46:08 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [6100/50000]	Time 0.039 (0.032)	Loss 1.7744 (0.8517)	Epoch 0	Acc@1 0.000 (81.642)	Acc@5 100.000 (95.296)	Mem 269MB
[2024-11-10 17:46:10 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [6150/50000]	Time 0.039 (0.032)	Loss 0.1460 (0.8500)	Epoch 0	Acc@1 100.000 (81.645)	Acc@5 100.000 (95.318)	Mem 269MB
[2024-11-10 17:46:12 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [6200/50000]	Time 0.039 (0.033)	Loss 0.2107 (0.8494)	Epoch 0	Acc@1 100.000 (81.664)	Acc@5 100.000 (95.356)	Mem 269MB
[2024-11-10 17:46:14 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [6250/50000]	Time 0.039 (0.033)	Loss 0.1127 (0.8470)	Epoch 0	Acc@1 100.000 (81.699)	Acc@5 100.000 (95.393)	Mem 269MB
[2024-11-10 17:46:16 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [6300/50000]	Time 0.039 (0.033)	Loss 0.1504 (0.8492)	Epoch 0	Acc@1 100.000 (81.638)	Acc@5 100.000 (95.382)	Mem 269MB
[2024-11-10 17:46:18 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [6350/50000]	Time 0.039 (0.033)	Loss 0.1718 (0.8514)	Epoch 0	Acc@1 100.000 (81.593)	Acc@5 100.000 (95.355)	Mem 269MB
[2024-11-10 17:46:20 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [6400/50000]	Time 0.038 (0.033)	Loss 0.0985 (0.8497)	Epoch 0	Acc@1 100.000 (81.643)	Acc@5 100.000 (95.360)	Mem 269MB
[2024-11-10 17:46:22 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [6450/50000]	Time 0.038 (0.033)	Loss 0.1460 (0.8489)	Epoch 0	Acc@1 100.000 (81.662)	Acc@5 100.000 (95.381)	Mem 269MB
[2024-11-10 17:46:24 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [6500/50000]	Time 0.039 (0.033)	Loss 0.1117 (0.8514)	Epoch 0	Acc@1 100.000 (81.634)	Acc@5 100.000 (95.339)	Mem 269MB
[2024-11-10 17:46:26 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [6550/50000]	Time 0.039 (0.033)	Loss 0.5024 (0.8496)	Epoch 0	Acc@1 100.000 (81.667)	Acc@5 100.000 (95.359)	Mem 269MB
[2024-11-10 17:46:28 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [6600/50000]	Time 0.039 (0.033)	Loss 0.2012 (0.8497)	Epoch 0	Acc@1 100.000 (81.639)	Acc@5 100.000 (95.364)	Mem 269MB
[2024-11-10 17:46:30 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [6650/50000]	Time 0.038 (0.033)	Loss 0.4182 (0.8484)	Epoch 0	Acc@1 100.000 (81.642)	Acc@5 100.000 (95.384)	Mem 269MB
[2024-11-10 17:46:32 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [6700/50000]	Time 0.039 (0.033)	Loss 0.0374 (0.8470)	Epoch 0	Acc@1 100.000 (81.689)	Acc@5 100.000 (95.404)	Mem 269MB
[2024-11-10 17:46:34 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [6750/50000]	Time 0.039 (0.033)	Loss 0.1691 (0.8486)	Epoch 0	Acc@1 100.000 (81.618)	Acc@5 100.000 (95.393)	Mem 269MB
[2024-11-10 17:46:36 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [6800/50000]	Time 0.039 (0.033)	Loss 0.2003 (0.8497)	Epoch 0	Acc@1 100.000 (81.591)	Acc@5 100.000 (95.412)	Mem 269MB
[2024-11-10 17:46:38 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [6850/50000]	Time 0.039 (0.033)	Loss 0.9985 (0.8482)	Epoch 0	Acc@1 100.000 (81.638)	Acc@5 100.000 (95.431)	Mem 269MB
[2024-11-10 17:46:40 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [6900/50000]	Time 0.039 (0.033)	Loss 0.6660 (0.8484)	Epoch 0	Acc@1 100.000 (81.626)	Acc@5 100.000 (95.435)	Mem 269MB
[2024-11-10 17:46:42 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [6950/50000]	Time 0.039 (0.033)	Loss 0.1394 (0.8504)	Epoch 0	Acc@1 100.000 (81.585)	Acc@5 100.000 (95.411)	Mem 269MB
[2024-11-10 17:46:44 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [7000/50000]	Time 0.039 (0.033)	Loss 0.1724 (0.8503)	Epoch 0	Acc@1 100.000 (81.574)	Acc@5 100.000 (95.429)	Mem 269MB
[2024-11-10 17:46:46 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [7050/50000]	Time 0.039 (0.033)	Loss 0.5537 (0.8490)	Epoch 0	Acc@1 100.000 (81.634)	Acc@5 100.000 (95.447)	Mem 269MB
[2024-11-10 17:46:48 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [7100/50000]	Time 0.039 (0.033)	Loss 0.0733 (0.8473)	Epoch 0	Acc@1 100.000 (81.679)	Acc@5 100.000 (95.465)	Mem 269MB
[2024-11-10 17:46:50 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [7150/50000]	Time 0.039 (0.033)	Loss 0.1327 (0.8476)	Epoch 0	Acc@1 100.000 (81.695)	Acc@5 100.000 (95.441)	Mem 269MB
[2024-11-10 17:46:52 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [7200/50000]	Time 0.039 (0.034)	Loss 0.1636 (0.8490)	Epoch 0	Acc@1 100.000 (81.697)	Acc@5 100.000 (95.417)	Mem 269MB
[2024-11-10 17:46:54 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [7250/50000]	Time 0.039 (0.034)	Loss 2.9570 (0.8510)	Epoch 0	Acc@1 0.000 (81.603)	Acc@5 0.000 (95.421)	Mem 269MB
[2024-11-10 17:46:56 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [7300/50000]	Time 0.039 (0.034)	Loss 0.2493 (0.8503)	Epoch 0	Acc@1 100.000 (81.633)	Acc@5 100.000 (95.398)	Mem 269MB
[2024-11-10 17:46:57 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [7350/50000]	Time 0.040 (0.034)	Loss 0.0902 (0.8501)	Epoch 0	Acc@1 100.000 (81.622)	Acc@5 100.000 (95.402)	Mem 269MB
[2024-11-10 17:46:59 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [7400/50000]	Time 0.032 (0.034)	Loss 0.4858 (0.8496)	Epoch 0	Acc@1 100.000 (81.597)	Acc@5 100.000 (95.406)	Mem 269MB
[2024-11-10 17:47:01 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [7450/50000]	Time 0.031 (0.034)	Loss 0.1321 (0.8483)	Epoch 0	Acc@1 100.000 (81.600)	Acc@5 100.000 (95.437)	Mem 269MB
[2024-11-10 17:47:02 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [7500/50000]	Time 0.032 (0.034)	Loss 1.2373 (0.8477)	Epoch 0	Acc@1 0.000 (81.616)	Acc@5 100.000 (95.454)	Mem 269MB
[2024-11-10 17:47:04 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [7550/50000]	Time 0.037 (0.034)	Loss 0.1323 (0.8498)	Epoch 0	Acc@1 100.000 (81.565)	Acc@5 100.000 (95.458)	Mem 269MB
[2024-11-10 17:47:06 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [7600/50000]	Time 0.032 (0.034)	Loss 0.1114 (0.8486)	Epoch 0	Acc@1 100.000 (81.595)	Acc@5 100.000 (95.448)	Mem 269MB
[2024-11-10 17:47:07 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [7650/50000]	Time 0.032 (0.034)	Loss 0.1561 (0.8491)	Epoch 0	Acc@1 100.000 (81.571)	Acc@5 100.000 (95.452)	Mem 269MB
[2024-11-10 17:47:09 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [7700/50000]	Time 0.035 (0.034)	Loss 0.0677 (0.8493)	Epoch 0	Acc@1 100.000 (81.587)	Acc@5 100.000 (95.429)	Mem 269MB
[2024-11-10 17:47:11 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [7750/50000]	Time 0.032 (0.034)	Loss 0.4968 (0.8482)	Epoch 0	Acc@1 100.000 (81.641)	Acc@5 100.000 (95.420)	Mem 269MB
[2024-11-10 17:47:12 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [7800/50000]	Time 0.031 (0.034)	Loss 0.1215 (0.8474)	Epoch 0	Acc@1 100.000 (81.631)	Acc@5 100.000 (95.449)	Mem 269MB
[2024-11-10 17:47:14 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [7850/50000]	Time 0.032 (0.034)	Loss 0.1511 (0.8468)	Epoch 0	Acc@1 100.000 (81.646)	Acc@5 100.000 (95.453)	Mem 269MB
[2024-11-10 17:47:16 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [7900/50000]	Time 0.032 (0.034)	Loss 0.1593 (0.8451)	Epoch 0	Acc@1 100.000 (81.661)	Acc@5 100.000 (95.469)	Mem 269MB
[2024-11-10 17:47:17 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [7950/50000]	Time 0.032 (0.034)	Loss 0.2054 (0.8435)	Epoch 0	Acc@1 100.000 (81.700)	Acc@5 100.000 (95.472)	Mem 269MB
[2024-11-10 17:47:19 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [8000/50000]	Time 0.032 (0.034)	Loss 0.4641 (0.8428)	Epoch 0	Acc@1 100.000 (81.715)	Acc@5 100.000 (95.476)	Mem 269MB
[2024-11-10 17:47:21 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [8050/50000]	Time 0.032 (0.034)	Loss 0.1467 (0.8441)	Epoch 0	Acc@1 100.000 (81.692)	Acc@5 100.000 (95.466)	Mem 269MB
[2024-11-10 17:47:22 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [8100/50000]	Time 0.032 (0.034)	Loss 0.1033 (0.8438)	Epoch 0	Acc@1 100.000 (81.706)	Acc@5 100.000 (95.457)	Mem 269MB
[2024-11-10 17:47:24 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [8150/50000]	Time 0.032 (0.034)	Loss 0.1251 (0.8441)	Epoch 0	Acc@1 100.000 (81.708)	Acc@5 100.000 (95.436)	Mem 269MB
[2024-11-10 17:47:25 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [8200/50000]	Time 0.032 (0.034)	Loss 0.1345 (0.8475)	Epoch 0	Acc@1 100.000 (81.624)	Acc@5 100.000 (95.391)	Mem 269MB
[2024-11-10 17:47:27 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [8250/50000]	Time 0.031 (0.034)	Loss 0.1578 (0.8469)	Epoch 0	Acc@1 100.000 (81.651)	Acc@5 100.000 (95.394)	Mem 269MB
[2024-11-10 17:47:29 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [8300/50000]	Time 0.032 (0.034)	Loss 0.1788 (0.8464)	Epoch 0	Acc@1 100.000 (81.641)	Acc@5 100.000 (95.410)	Mem 269MB
[2024-11-10 17:47:30 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [8350/50000]	Time 0.032 (0.034)	Loss 0.0992 (0.8449)	Epoch 0	Acc@1 100.000 (81.667)	Acc@5 100.000 (95.426)	Mem 269MB
[2024-11-10 17:47:32 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [8400/50000]	Time 0.032 (0.034)	Loss 8.5859 (0.8446)	Epoch 0	Acc@1 0.000 (81.681)	Acc@5 0.000 (95.405)	Mem 269MB
[2024-11-10 17:47:34 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [8450/50000]	Time 0.034 (0.034)	Loss 0.5269 (0.8459)	Epoch 0	Acc@1 100.000 (81.635)	Acc@5 100.000 (95.409)	Mem 269MB
[2024-11-10 17:47:35 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [8500/50000]	Time 0.032 (0.034)	Loss 0.1078 (0.8468)	Epoch 0	Acc@1 100.000 (81.614)	Acc@5 100.000 (95.377)	Mem 269MB
[2024-11-10 17:47:37 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [8550/50000]	Time 0.032 (0.034)	Loss 4.8516 (0.8454)	Epoch 0	Acc@1 0.000 (81.663)	Acc@5 100.000 (95.404)	Mem 269MB
[2024-11-10 17:47:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [8600/50000]	Time 0.032 (0.034)	Loss 1.3994 (0.8455)	Epoch 0	Acc@1 0.000 (81.630)	Acc@5 100.000 (95.419)	Mem 269MB
[2024-11-10 17:47:40 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [8650/50000]	Time 0.032 (0.034)	Loss 0.0826 (0.8444)	Epoch 0	Acc@1 100.000 (81.632)	Acc@5 100.000 (95.422)	Mem 269MB
[2024-11-10 17:47:42 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [8700/50000]	Time 0.039 (0.034)	Loss 0.8691 (0.8439)	Epoch 0	Acc@1 100.000 (81.657)	Acc@5 100.000 (95.403)	Mem 269MB
[2024-11-10 17:47:44 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [8750/50000]	Time 0.039 (0.034)	Loss 0.1047 (0.8438)	Epoch 0	Acc@1 100.000 (81.659)	Acc@5 100.000 (95.418)	Mem 269MB
[2024-11-10 17:47:46 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [8800/50000]	Time 0.039 (0.034)	Loss 0.5176 (0.8444)	Epoch 0	Acc@1 100.000 (81.650)	Acc@5 100.000 (95.444)	Mem 269MB
[2024-11-10 17:47:48 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [8850/50000]	Time 0.040 (0.034)	Loss 0.3994 (0.8425)	Epoch 0	Acc@1 100.000 (81.708)	Acc@5 100.000 (95.469)	Mem 269MB
[2024-11-10 17:47:50 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [8900/50000]	Time 0.039 (0.034)	Loss 0.1171 (0.8419)	Epoch 0	Acc@1 100.000 (81.732)	Acc@5 100.000 (95.461)	Mem 269MB
[2024-11-10 17:47:52 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [8950/50000]	Time 0.039 (0.034)	Loss 0.1196 (0.8414)	Epoch 0	Acc@1 100.000 (81.745)	Acc@5 100.000 (95.464)	Mem 269MB
[2024-11-10 17:47:54 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [9000/50000]	Time 0.039 (0.034)	Loss 0.1892 (0.8423)	Epoch 0	Acc@1 100.000 (81.746)	Acc@5 100.000 (95.456)	Mem 269MB
[2024-11-10 17:47:56 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [9050/50000]	Time 0.039 (0.034)	Loss 2.8008 (0.8411)	Epoch 0	Acc@1 0.000 (81.759)	Acc@5 100.000 (95.470)	Mem 269MB
[2024-11-10 17:47:58 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [9100/50000]	Time 0.039 (0.034)	Loss 0.2111 (0.8416)	Epoch 0	Acc@1 100.000 (81.771)	Acc@5 100.000 (95.462)	Mem 269MB
[2024-11-10 17:48:00 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [9150/50000]	Time 0.038 (0.034)	Loss 2.9609 (0.8417)	Epoch 0	Acc@1 0.000 (81.772)	Acc@5 100.000 (95.465)	Mem 269MB
[2024-11-10 17:48:01 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [9200/50000]	Time 0.039 (0.034)	Loss 0.1052 (0.8432)	Epoch 0	Acc@1 100.000 (81.730)	Acc@5 100.000 (95.457)	Mem 269MB
[2024-11-10 17:48:03 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [9250/50000]	Time 0.039 (0.034)	Loss 0.1718 (0.8428)	Epoch 0	Acc@1 100.000 (81.753)	Acc@5 100.000 (95.471)	Mem 269MB
[2024-11-10 17:48:05 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [9300/50000]	Time 0.039 (0.034)	Loss 0.1646 (0.8441)	Epoch 0	Acc@1 100.000 (81.701)	Acc@5 100.000 (95.463)	Mem 269MB
[2024-11-10 17:48:07 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [9350/50000]	Time 0.039 (0.034)	Loss 0.0930 (0.8446)	Epoch 0	Acc@1 100.000 (81.702)	Acc@5 100.000 (95.455)	Mem 269MB
[2024-11-10 17:48:09 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [9400/50000]	Time 0.039 (0.034)	Loss 3.6035 (0.8444)	Epoch 0	Acc@1 0.000 (81.672)	Acc@5 100.000 (95.469)	Mem 269MB
[2024-11-10 17:48:11 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [9450/50000]	Time 0.038 (0.034)	Loss 0.1227 (0.8444)	Epoch 0	Acc@1 100.000 (81.642)	Acc@5 100.000 (95.482)	Mem 269MB
[2024-11-10 17:48:13 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [9500/50000]	Time 0.038 (0.034)	Loss 0.0800 (0.8455)	Epoch 0	Acc@1 100.000 (81.634)	Acc@5 100.000 (95.464)	Mem 269MB
[2024-11-10 17:48:15 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [9550/50000]	Time 0.039 (0.034)	Loss 0.1367 (0.8450)	Epoch 0	Acc@1 100.000 (81.625)	Acc@5 100.000 (95.466)	Mem 269MB
[2024-11-10 17:48:17 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [9600/50000]	Time 0.038 (0.034)	Loss 0.1224 (0.8458)	Epoch 0	Acc@1 100.000 (81.575)	Acc@5 100.000 (95.469)	Mem 269MB
[2024-11-10 17:48:19 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [9650/50000]	Time 0.039 (0.034)	Loss 2.5312 (0.8458)	Epoch 0	Acc@1 0.000 (81.546)	Acc@5 100.000 (95.493)	Mem 269MB
[2024-11-10 17:48:21 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [9700/50000]	Time 0.039 (0.034)	Loss 0.7119 (0.8450)	Epoch 0	Acc@1 100.000 (81.559)	Acc@5 100.000 (95.495)	Mem 269MB
[2024-11-10 17:48:23 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [9750/50000]	Time 0.039 (0.034)	Loss 0.1754 (0.8447)	Epoch 0	Acc@1 100.000 (81.581)	Acc@5 100.000 (95.498)	Mem 269MB
[2024-11-10 17:48:25 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [9800/50000]	Time 0.039 (0.034)	Loss 0.0649 (0.8432)	Epoch 0	Acc@1 100.000 (81.624)	Acc@5 100.000 (95.500)	Mem 269MB
[2024-11-10 17:48:27 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [9850/50000]	Time 0.032 (0.034)	Loss 0.1849 (0.8428)	Epoch 0	Acc@1 100.000 (81.626)	Acc@5 100.000 (95.503)	Mem 269MB
[2024-11-10 17:48:28 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [9900/50000]	Time 0.032 (0.034)	Loss 0.0607 (0.8430)	Epoch 0	Acc@1 100.000 (81.608)	Acc@5 100.000 (95.516)	Mem 269MB
[2024-11-10 17:48:30 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [9950/50000]	Time 0.034 (0.034)	Loss 0.5845 (0.8417)	Epoch 0	Acc@1 100.000 (81.630)	Acc@5 100.000 (95.528)	Mem 269MB
[2024-11-10 17:48:32 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [10000/50000]	Time 0.032 (0.034)	Loss 1.3594 (0.8432)	Epoch 0	Acc@1 0.000 (81.582)	Acc@5 100.000 (95.510)	Mem 269MB
[2024-11-10 17:48:33 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [10050/50000]	Time 0.032 (0.034)	Loss 0.0396 (0.8437)	Epoch 0	Acc@1 100.000 (81.574)	Acc@5 100.000 (95.493)	Mem 269MB
[2024-11-10 17:48:35 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [10100/50000]	Time 0.032 (0.034)	Loss 0.1880 (0.8440)	Epoch 0	Acc@1 100.000 (81.576)	Acc@5 100.000 (95.495)	Mem 269MB
[2024-11-10 17:48:36 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [10150/50000]	Time 0.034 (0.034)	Loss 2.0938 (0.8454)	Epoch 0	Acc@1 0.000 (81.558)	Acc@5 100.000 (95.478)	Mem 269MB
[2024-11-10 17:48:38 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [10200/50000]	Time 0.031 (0.034)	Loss 0.0470 (0.8454)	Epoch 0	Acc@1 100.000 (81.570)	Acc@5 100.000 (95.471)	Mem 269MB
[2024-11-10 17:48:40 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [10250/50000]	Time 0.032 (0.034)	Loss 0.0185 (0.8438)	Epoch 0	Acc@1 100.000 (81.602)	Acc@5 100.000 (95.483)	Mem 269MB
[2024-11-10 17:48:41 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [10300/50000]	Time 0.032 (0.034)	Loss 0.1027 (0.8448)	Epoch 0	Acc@1 100.000 (81.565)	Acc@5 100.000 (95.496)	Mem 269MB
[2024-11-10 17:48:43 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [10350/50000]	Time 0.036 (0.034)	Loss 0.2886 (0.8438)	Epoch 0	Acc@1 100.000 (81.586)	Acc@5 100.000 (95.508)	Mem 269MB
[2024-11-10 17:48:44 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [10400/50000]	Time 0.032 (0.034)	Loss 0.1849 (0.8434)	Epoch 0	Acc@1 100.000 (81.617)	Acc@5 100.000 (95.500)	Mem 269MB
[2024-11-10 17:48:46 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [10450/50000]	Time 0.032 (0.034)	Loss 0.1445 (0.8430)	Epoch 0	Acc@1 100.000 (81.609)	Acc@5 100.000 (95.493)	Mem 269MB
[2024-11-10 17:48:48 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [10500/50000]	Time 0.032 (0.034)	Loss 0.0930 (0.8432)	Epoch 0	Acc@1 100.000 (81.611)	Acc@5 100.000 (95.505)	Mem 269MB
[2024-11-10 17:48:49 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [10550/50000]	Time 0.032 (0.034)	Loss 0.2759 (0.8444)	Epoch 0	Acc@1 100.000 (81.613)	Acc@5 100.000 (95.489)	Mem 269MB
[2024-11-10 17:48:51 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [10600/50000]	Time 0.032 (0.034)	Loss 2.9238 (0.8463)	Epoch 0	Acc@1 0.000 (81.577)	Acc@5 100.000 (95.463)	Mem 269MB
[2024-11-10 17:48:53 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [10650/50000]	Time 0.032 (0.034)	Loss 0.1051 (0.8464)	Epoch 0	Acc@1 100.000 (81.589)	Acc@5 100.000 (95.456)	Mem 269MB
[2024-11-10 17:48:54 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [10700/50000]	Time 0.032 (0.034)	Loss 0.2915 (0.8484)	Epoch 0	Acc@1 100.000 (81.581)	Acc@5 100.000 (95.412)	Mem 269MB
[2024-11-10 17:48:56 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [10750/50000]	Time 0.032 (0.034)	Loss 0.3875 (0.8470)	Epoch 0	Acc@1 100.000 (81.620)	Acc@5 100.000 (95.433)	Mem 269MB
[2024-11-10 17:48:57 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [10800/50000]	Time 0.032 (0.034)	Loss 0.6841 (0.8476)	Epoch 0	Acc@1 100.000 (81.631)	Acc@5 100.000 (95.417)	Mem 269MB
[2024-11-10 17:48:59 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [10850/50000]	Time 0.032 (0.034)	Loss 0.1583 (0.8498)	Epoch 0	Acc@1 100.000 (81.596)	Acc@5 100.000 (95.401)	Mem 269MB
[2024-11-10 17:49:01 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [10900/50000]	Time 0.032 (0.034)	Loss 0.0941 (0.8501)	Epoch 0	Acc@1 100.000 (81.589)	Acc@5 100.000 (95.395)	Mem 269MB
[2024-11-10 17:49:02 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [10950/50000]	Time 0.032 (0.034)	Loss 0.0803 (0.8503)	Epoch 0	Acc@1 100.000 (81.563)	Acc@5 100.000 (95.398)	Mem 269MB
[2024-11-10 17:49:04 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [11000/50000]	Time 0.032 (0.034)	Loss 0.1025 (0.8497)	Epoch 0	Acc@1 100.000 (81.565)	Acc@5 100.000 (95.410)	Mem 269MB
[2024-11-10 17:49:05 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [11050/50000]	Time 0.031 (0.034)	Loss 0.1481 (0.8513)	Epoch 0	Acc@1 100.000 (81.522)	Acc@5 100.000 (95.403)	Mem 269MB
[2024-11-10 17:49:07 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [11100/50000]	Time 0.032 (0.034)	Loss 3.9434 (0.8514)	Epoch 0	Acc@1 0.000 (81.515)	Acc@5 100.000 (95.397)	Mem 269MB
[2024-11-10 17:49:09 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [11150/50000]	Time 0.032 (0.034)	Loss 0.0891 (0.8507)	Epoch 0	Acc@1 100.000 (81.535)	Acc@5 100.000 (95.400)	Mem 269MB
[2024-11-10 17:49:10 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [11200/50000]	Time 0.033 (0.034)	Loss 0.1719 (0.8502)	Epoch 0	Acc@1 100.000 (81.546)	Acc@5 100.000 (95.393)	Mem 269MB
[2024-11-10 17:49:12 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [11250/50000]	Time 0.032 (0.034)	Loss 0.0834 (0.8494)	Epoch 0	Acc@1 100.000 (81.566)	Acc@5 100.000 (95.405)	Mem 269MB
[2024-11-10 17:49:14 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [11300/50000]	Time 0.032 (0.034)	Loss 0.1377 (0.8474)	Epoch 0	Acc@1 100.000 (81.603)	Acc@5 100.000 (95.425)	Mem 269MB
[2024-11-10 17:49:15 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [11350/50000]	Time 0.032 (0.034)	Loss 3.5312 (0.8493)	Epoch 0	Acc@1 0.000 (81.570)	Acc@5 100.000 (95.410)	Mem 269MB
[2024-11-10 17:49:17 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [11400/50000]	Time 0.032 (0.034)	Loss 1.1445 (0.8490)	Epoch 0	Acc@1 100.000 (81.581)	Acc@5 100.000 (95.404)	Mem 269MB
[2024-11-10 17:49:18 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [11450/50000]	Time 0.032 (0.034)	Loss 0.1760 (0.8473)	Epoch 0	Acc@1 100.000 (81.626)	Acc@5 100.000 (95.424)	Mem 269MB
[2024-11-10 17:49:20 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [11500/50000]	Time 0.032 (0.034)	Loss 0.0901 (0.8460)	Epoch 0	Acc@1 100.000 (81.628)	Acc@5 100.000 (95.444)	Mem 269MB
[2024-11-10 17:49:22 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [11550/50000]	Time 0.039 (0.034)	Loss 1.4795 (0.8455)	Epoch 0	Acc@1 0.000 (81.612)	Acc@5 100.000 (95.455)	Mem 269MB
[2024-11-10 17:49:24 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [11600/50000]	Time 0.039 (0.034)	Loss 0.1158 (0.8474)	Epoch 0	Acc@1 100.000 (81.545)	Acc@5 100.000 (95.449)	Mem 269MB
[2024-11-10 17:49:26 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [11650/50000]	Time 0.039 (0.034)	Loss 0.0910 (0.8496)	Epoch 0	Acc@1 100.000 (81.469)	Acc@5 100.000 (95.417)	Mem 269MB
[2024-11-10 17:49:28 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [11700/50000]	Time 0.039 (0.034)	Loss 0.1293 (0.8503)	Epoch 0	Acc@1 100.000 (81.480)	Acc@5 100.000 (95.411)	Mem 269MB
[2024-11-10 17:49:30 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [11750/50000]	Time 0.039 (0.034)	Loss 5.6445 (0.8501)	Epoch 0	Acc@1 0.000 (81.508)	Acc@5 0.000 (95.396)	Mem 269MB
[2024-11-10 17:49:32 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [11800/50000]	Time 0.039 (0.034)	Loss 0.2891 (0.8486)	Epoch 0	Acc@1 100.000 (81.527)	Acc@5 100.000 (95.416)	Mem 269MB
[2024-11-10 17:49:34 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [11850/50000]	Time 0.039 (0.034)	Loss 3.8906 (0.8499)	Epoch 0	Acc@1 0.000 (81.504)	Acc@5 0.000 (95.393)	Mem 269MB
[2024-11-10 17:49:36 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [11900/50000]	Time 0.039 (0.034)	Loss 1.9834 (0.8504)	Epoch 0	Acc@1 100.000 (81.523)	Acc@5 100.000 (95.387)	Mem 269MB
[2024-11-10 17:49:38 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [11950/50000]	Time 0.039 (0.034)	Loss 4.1602 (0.8491)	Epoch 0	Acc@1 0.000 (81.541)	Acc@5 0.000 (95.398)	Mem 269MB
[2024-11-10 17:49:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [12000/50000]	Time 0.039 (0.034)	Loss 0.0995 (0.8503)	Epoch 0	Acc@1 100.000 (81.527)	Acc@5 100.000 (95.384)	Mem 269MB
[2024-11-10 17:49:41 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [12050/50000]	Time 0.039 (0.034)	Loss 0.1625 (0.8499)	Epoch 0	Acc@1 100.000 (81.495)	Acc@5 100.000 (95.395)	Mem 269MB
[2024-11-10 17:49:43 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [12100/50000]	Time 0.039 (0.034)	Loss 0.4167 (0.8508)	Epoch 0	Acc@1 100.000 (81.464)	Acc@5 100.000 (95.381)	Mem 269MB
[2024-11-10 17:49:45 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [12150/50000]	Time 0.039 (0.034)	Loss 0.1646 (0.8502)	Epoch 0	Acc@1 100.000 (81.491)	Acc@5 100.000 (95.383)	Mem 269MB
[2024-11-10 17:49:47 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [12200/50000]	Time 0.039 (0.034)	Loss 0.2299 (0.8511)	Epoch 0	Acc@1 100.000 (81.493)	Acc@5 100.000 (95.377)	Mem 269MB
[2024-11-10 17:49:49 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [12250/50000]	Time 0.039 (0.034)	Loss 0.1544 (0.8499)	Epoch 0	Acc@1 100.000 (81.528)	Acc@5 100.000 (95.388)	Mem 269MB
[2024-11-10 17:49:51 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [12300/50000]	Time 0.039 (0.034)	Loss 0.1178 (0.8501)	Epoch 0	Acc@1 100.000 (81.497)	Acc@5 100.000 (95.382)	Mem 269MB
[2024-11-10 17:49:53 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [12350/50000]	Time 0.039 (0.034)	Loss 0.1163 (0.8509)	Epoch 0	Acc@1 100.000 (81.467)	Acc@5 100.000 (95.393)	Mem 269MB
[2024-11-10 17:49:55 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [12400/50000]	Time 0.039 (0.034)	Loss 0.1588 (0.8492)	Epoch 0	Acc@1 100.000 (81.510)	Acc@5 100.000 (95.404)	Mem 269MB
[2024-11-10 17:49:57 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [12450/50000]	Time 0.039 (0.034)	Loss 0.1154 (0.8501)	Epoch 0	Acc@1 100.000 (81.495)	Acc@5 100.000 (95.398)	Mem 269MB
[2024-11-10 17:49:59 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [12500/50000]	Time 0.039 (0.034)	Loss 0.1287 (0.8483)	Epoch 0	Acc@1 100.000 (81.521)	Acc@5 100.000 (95.416)	Mem 269MB
[2024-11-10 17:50:01 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [12550/50000]	Time 0.039 (0.034)	Loss 0.1967 (0.8493)	Epoch 0	Acc@1 100.000 (81.507)	Acc@5 100.000 (95.411)	Mem 269MB
[2024-11-10 17:50:03 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [12600/50000]	Time 0.039 (0.034)	Loss 0.1188 (0.8481)	Epoch 0	Acc@1 100.000 (81.541)	Acc@5 100.000 (95.421)	Mem 269MB
[2024-11-10 17:50:05 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [12650/50000]	Time 0.039 (0.034)	Loss 0.1782 (0.8469)	Epoch 0	Acc@1 100.000 (81.559)	Acc@5 100.000 (95.439)	Mem 269MB
[2024-11-10 17:50:07 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [12700/50000]	Time 0.038 (0.034)	Loss 1.4678 (0.8475)	Epoch 0	Acc@1 0.000 (81.561)	Acc@5 100.000 (95.426)	Mem 269MB
[2024-11-10 17:50:09 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [12750/50000]	Time 0.039 (0.034)	Loss 0.1348 (0.8480)	Epoch 0	Acc@1 100.000 (81.547)	Acc@5 100.000 (95.428)	Mem 269MB
[2024-11-10 17:50:11 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [12800/50000]	Time 0.039 (0.034)	Loss 0.2542 (0.8470)	Epoch 0	Acc@1 100.000 (81.572)	Acc@5 100.000 (95.422)	Mem 269MB
[2024-11-10 17:50:13 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [12850/50000]	Time 0.039 (0.034)	Loss 0.1406 (0.8475)	Epoch 0	Acc@1 100.000 (81.581)	Acc@5 100.000 (95.409)	Mem 269MB
[2024-11-10 17:50:15 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [12900/50000]	Time 0.039 (0.034)	Loss 0.5225 (0.8465)	Epoch 0	Acc@1 100.000 (81.606)	Acc@5 100.000 (95.419)	Mem 269MB
[2024-11-10 17:50:17 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [12950/50000]	Time 0.039 (0.034)	Loss 6.8672 (0.8468)	Epoch 0	Acc@1 0.000 (81.600)	Acc@5 100.000 (95.421)	Mem 269MB
[2024-11-10 17:50:18 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [13000/50000]	Time 0.032 (0.034)	Loss 0.2195 (0.8470)	Epoch 0	Acc@1 100.000 (81.578)	Acc@5 100.000 (95.423)	Mem 269MB
[2024-11-10 17:50:20 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [13050/50000]	Time 0.032 (0.034)	Loss 0.1372 (0.8476)	Epoch 0	Acc@1 100.000 (81.565)	Acc@5 100.000 (95.410)	Mem 269MB
[2024-11-10 17:50:22 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [13100/50000]	Time 0.035 (0.034)	Loss 0.0870 (0.8465)	Epoch 0	Acc@1 100.000 (81.589)	Acc@5 100.000 (95.420)	Mem 269MB
[2024-11-10 17:50:23 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [13150/50000]	Time 0.031 (0.034)	Loss 0.1902 (0.8465)	Epoch 0	Acc@1 100.000 (81.598)	Acc@5 100.000 (95.415)	Mem 269MB
[2024-11-10 17:50:25 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [13200/50000]	Time 0.032 (0.034)	Loss 0.1791 (0.8473)	Epoch 0	Acc@1 100.000 (81.577)	Acc@5 100.000 (95.409)	Mem 269MB
[2024-11-10 17:50:26 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [13250/50000]	Time 0.032 (0.034)	Loss 0.2800 (0.8463)	Epoch 0	Acc@1 100.000 (81.594)	Acc@5 100.000 (95.427)	Mem 269MB
[2024-11-10 17:50:28 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [13300/50000]	Time 0.031 (0.034)	Loss 0.1436 (0.8459)	Epoch 0	Acc@1 100.000 (81.580)	Acc@5 100.000 (95.436)	Mem 269MB
[2024-11-10 17:50:30 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [13350/50000]	Time 0.032 (0.034)	Loss 0.1233 (0.8467)	Epoch 0	Acc@1 100.000 (81.567)	Acc@5 100.000 (95.439)	Mem 269MB
[2024-11-10 17:50:31 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [13400/50000]	Time 0.032 (0.034)	Loss 0.1797 (0.8475)	Epoch 0	Acc@1 100.000 (81.554)	Acc@5 100.000 (95.418)	Mem 269MB
[2024-11-10 17:50:33 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [13450/50000]	Time 0.035 (0.034)	Loss 1.6250 (0.8485)	Epoch 0	Acc@1 0.000 (81.518)	Acc@5 100.000 (95.406)	Mem 269MB
[2024-11-10 17:50:34 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [13500/50000]	Time 0.032 (0.034)	Loss 0.1975 (0.8485)	Epoch 0	Acc@1 100.000 (81.520)	Acc@5 100.000 (95.415)	Mem 269MB
[2024-11-10 17:50:36 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [13550/50000]	Time 0.032 (0.034)	Loss 0.2262 (0.8491)	Epoch 0	Acc@1 100.000 (81.507)	Acc@5 100.000 (95.417)	Mem 269MB
[2024-11-10 17:50:38 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [13600/50000]	Time 0.032 (0.034)	Loss 0.0755 (0.8485)	Epoch 0	Acc@1 100.000 (81.516)	Acc@5 100.000 (95.427)	Mem 269MB
[2024-11-10 17:50:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [13650/50000]	Time 0.032 (0.034)	Loss 0.2209 (0.8491)	Epoch 0	Acc@1 100.000 (81.518)	Acc@5 100.000 (95.414)	Mem 269MB
[2024-11-10 17:50:41 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [13700/50000]	Time 0.032 (0.034)	Loss 0.9888 (0.8477)	Epoch 0	Acc@1 100.000 (81.556)	Acc@5 100.000 (95.424)	Mem 269MB
[2024-11-10 17:50:42 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [13750/50000]	Time 0.032 (0.034)	Loss 2.4219 (0.8477)	Epoch 0	Acc@1 0.000 (81.565)	Acc@5 100.000 (95.426)	Mem 269MB
[2024-11-10 17:50:44 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [13800/50000]	Time 0.032 (0.034)	Loss 1.6924 (0.8479)	Epoch 0	Acc@1 0.000 (81.538)	Acc@5 100.000 (95.435)	Mem 269MB
[2024-11-10 17:50:46 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [13850/50000]	Time 0.032 (0.034)	Loss 3.4590 (0.8488)	Epoch 0	Acc@1 0.000 (81.525)	Acc@5 100.000 (95.430)	Mem 269MB
[2024-11-10 17:50:47 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [13900/50000]	Time 0.032 (0.034)	Loss 0.7700 (0.8487)	Epoch 0	Acc@1 100.000 (81.527)	Acc@5 100.000 (95.432)	Mem 269MB
[2024-11-10 17:50:49 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [13950/50000]	Time 0.032 (0.034)	Loss 1.9375 (0.8476)	Epoch 0	Acc@1 0.000 (81.535)	Acc@5 100.000 (95.448)	Mem 269MB
[2024-11-10 17:50:50 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [14000/50000]	Time 0.033 (0.034)	Loss 0.1779 (0.8489)	Epoch 0	Acc@1 100.000 (81.516)	Acc@5 100.000 (95.429)	Mem 269MB
[2024-11-10 17:50:52 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [14050/50000]	Time 0.033 (0.034)	Loss 0.1479 (0.8475)	Epoch 0	Acc@1 100.000 (81.553)	Acc@5 100.000 (95.431)	Mem 269MB
[2024-11-10 17:50:54 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [14100/50000]	Time 0.033 (0.034)	Loss 0.2051 (0.8469)	Epoch 0	Acc@1 100.000 (81.562)	Acc@5 100.000 (95.433)	Mem 269MB
[2024-11-10 17:50:55 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [14150/50000]	Time 0.032 (0.034)	Loss 0.1052 (0.8463)	Epoch 0	Acc@1 100.000 (81.577)	Acc@5 100.000 (95.421)	Mem 269MB
[2024-11-10 17:50:57 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [14200/50000]	Time 0.032 (0.034)	Loss 0.1093 (0.8477)	Epoch 0	Acc@1 100.000 (81.551)	Acc@5 100.000 (95.402)	Mem 269MB
[2024-11-10 17:50:59 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [14250/50000]	Time 0.032 (0.034)	Loss 0.1678 (0.8481)	Epoch 0	Acc@1 100.000 (81.559)	Acc@5 100.000 (95.390)	Mem 269MB
[2024-11-10 17:51:00 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [14300/50000]	Time 0.032 (0.034)	Loss 0.2605 (0.8474)	Epoch 0	Acc@1 100.000 (81.561)	Acc@5 100.000 (95.392)	Mem 269MB
[2024-11-10 17:51:02 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [14350/50000]	Time 0.032 (0.034)	Loss 0.1382 (0.8464)	Epoch 0	Acc@1 100.000 (81.576)	Acc@5 100.000 (95.394)	Mem 269MB
[2024-11-10 17:51:03 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [14400/50000]	Time 0.031 (0.034)	Loss 2.8125 (0.8466)	Epoch 0	Acc@1 0.000 (81.578)	Acc@5 100.000 (95.389)	Mem 269MB
[2024-11-10 17:51:05 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [14450/50000]	Time 0.033 (0.034)	Loss 0.1332 (0.8481)	Epoch 0	Acc@1 100.000 (81.545)	Acc@5 100.000 (95.377)	Mem 269MB
[2024-11-10 17:51:07 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [14500/50000]	Time 0.032 (0.034)	Loss 0.8721 (0.8481)	Epoch 0	Acc@1 100.000 (81.560)	Acc@5 100.000 (95.387)	Mem 269MB
[2024-11-10 17:51:08 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [14550/50000]	Time 0.032 (0.034)	Loss 0.1554 (0.8474)	Epoch 0	Acc@1 100.000 (81.568)	Acc@5 100.000 (95.382)	Mem 269MB
[2024-11-10 17:51:10 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [14600/50000]	Time 0.033 (0.034)	Loss 4.2148 (0.8473)	Epoch 0	Acc@1 0.000 (81.577)	Acc@5 100.000 (95.384)	Mem 269MB
[2024-11-10 17:51:11 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [14650/50000]	Time 0.035 (0.034)	Loss 0.1097 (0.8481)	Epoch 0	Acc@1 100.000 (81.578)	Acc@5 100.000 (95.386)	Mem 269MB
[2024-11-10 17:51:13 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [14700/50000]	Time 0.032 (0.034)	Loss 0.3428 (0.8473)	Epoch 0	Acc@1 100.000 (81.579)	Acc@5 100.000 (95.402)	Mem 269MB
[2024-11-10 17:51:15 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [14750/50000]	Time 0.032 (0.034)	Loss 0.1350 (0.8481)	Epoch 0	Acc@1 100.000 (81.574)	Acc@5 100.000 (95.397)	Mem 269MB
[2024-11-10 17:51:16 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [14800/50000]	Time 0.032 (0.034)	Loss 0.0735 (0.8466)	Epoch 0	Acc@1 100.000 (81.603)	Acc@5 100.000 (95.412)	Mem 269MB
[2024-11-10 17:51:18 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [14850/50000]	Time 0.032 (0.034)	Loss 2.3633 (0.8460)	Epoch 0	Acc@1 0.000 (81.611)	Acc@5 100.000 (95.421)	Mem 269MB
[2024-11-10 17:51:20 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [14900/50000]	Time 0.034 (0.034)	Loss 0.1603 (0.8464)	Epoch 0	Acc@1 100.000 (81.612)	Acc@5 100.000 (95.396)	Mem 269MB
[2024-11-10 17:51:21 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [14950/50000]	Time 0.032 (0.034)	Loss 3.4082 (0.8457)	Epoch 0	Acc@1 0.000 (81.627)	Acc@5 100.000 (95.405)	Mem 269MB
[2024-11-10 17:51:23 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [15000/50000]	Time 0.033 (0.034)	Loss 0.0786 (0.8458)	Epoch 0	Acc@1 100.000 (81.628)	Acc@5 100.000 (95.407)	Mem 269MB
[2024-11-10 17:51:24 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [15050/50000]	Time 0.032 (0.034)	Loss 0.6777 (0.8467)	Epoch 0	Acc@1 100.000 (81.609)	Acc@5 100.000 (95.389)	Mem 269MB
[2024-11-10 17:51:26 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [15100/50000]	Time 0.032 (0.034)	Loss 1.0078 (0.8465)	Epoch 0	Acc@1 0.000 (81.624)	Acc@5 100.000 (95.384)	Mem 269MB
[2024-11-10 17:51:28 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [15150/50000]	Time 0.032 (0.034)	Loss 0.6929 (0.8471)	Epoch 0	Acc@1 100.000 (81.612)	Acc@5 100.000 (95.386)	Mem 269MB
[2024-11-10 17:51:29 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [15200/50000]	Time 0.032 (0.034)	Loss 0.1227 (0.8474)	Epoch 0	Acc@1 100.000 (81.606)	Acc@5 100.000 (95.388)	Mem 269MB
[2024-11-10 17:51:31 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [15250/50000]	Time 0.032 (0.034)	Loss 0.0380 (0.8466)	Epoch 0	Acc@1 100.000 (81.641)	Acc@5 100.000 (95.384)	Mem 269MB
[2024-11-10 17:51:32 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [15300/50000]	Time 0.034 (0.034)	Loss 0.1954 (0.8469)	Epoch 0	Acc@1 100.000 (81.609)	Acc@5 100.000 (95.386)	Mem 269MB
[2024-11-10 17:51:34 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [15350/50000]	Time 0.033 (0.034)	Loss 0.6074 (0.8466)	Epoch 0	Acc@1 100.000 (81.610)	Acc@5 100.000 (95.381)	Mem 269MB
[2024-11-10 17:51:36 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [15400/50000]	Time 0.032 (0.034)	Loss 0.1326 (0.8460)	Epoch 0	Acc@1 100.000 (81.644)	Acc@5 100.000 (95.390)	Mem 269MB
[2024-11-10 17:51:37 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [15450/50000]	Time 0.032 (0.034)	Loss 0.1512 (0.8467)	Epoch 0	Acc@1 100.000 (81.639)	Acc@5 100.000 (95.379)	Mem 269MB
[2024-11-10 17:51:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [15500/50000]	Time 0.033 (0.034)	Loss 0.6533 (0.8465)	Epoch 0	Acc@1 100.000 (81.646)	Acc@5 100.000 (95.381)	Mem 269MB
[2024-11-10 17:51:41 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [15550/50000]	Time 0.032 (0.034)	Loss 0.1414 (0.8482)	Epoch 0	Acc@1 100.000 (81.609)	Acc@5 100.000 (95.364)	Mem 269MB
[2024-11-10 17:51:42 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [15600/50000]	Time 0.032 (0.034)	Loss 0.1089 (0.8476)	Epoch 0	Acc@1 100.000 (81.617)	Acc@5 100.000 (95.366)	Mem 269MB
[2024-11-10 17:51:44 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [15650/50000]	Time 0.032 (0.034)	Loss 0.2688 (0.8473)	Epoch 0	Acc@1 100.000 (81.618)	Acc@5 100.000 (95.368)	Mem 269MB
[2024-11-10 17:51:45 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [15700/50000]	Time 0.032 (0.034)	Loss 0.2219 (0.8468)	Epoch 0	Acc@1 100.000 (81.625)	Acc@5 100.000 (95.363)	Mem 269MB
[2024-11-10 17:51:47 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [15750/50000]	Time 0.032 (0.034)	Loss 0.4658 (0.8469)	Epoch 0	Acc@1 100.000 (81.633)	Acc@5 100.000 (95.353)	Mem 269MB
[2024-11-10 17:51:49 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [15800/50000]	Time 0.032 (0.034)	Loss 0.1642 (0.8475)	Epoch 0	Acc@1 100.000 (81.609)	Acc@5 100.000 (95.336)	Mem 269MB
[2024-11-10 17:51:50 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [15850/50000]	Time 0.032 (0.034)	Loss 0.1946 (0.8480)	Epoch 0	Acc@1 100.000 (81.597)	Acc@5 100.000 (95.338)	Mem 269MB
[2024-11-10 17:51:52 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [15900/50000]	Time 0.032 (0.034)	Loss 2.9980 (0.8480)	Epoch 0	Acc@1 0.000 (81.599)	Acc@5 100.000 (95.346)	Mem 269MB
[2024-11-10 17:51:53 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [15950/50000]	Time 0.032 (0.034)	Loss 0.1039 (0.8474)	Epoch 0	Acc@1 100.000 (81.625)	Acc@5 100.000 (95.355)	Mem 269MB
[2024-11-10 17:51:55 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [16000/50000]	Time 0.033 (0.034)	Loss 0.5259 (0.8475)	Epoch 0	Acc@1 100.000 (81.632)	Acc@5 100.000 (95.344)	Mem 269MB
[2024-11-10 17:51:57 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [16050/50000]	Time 0.032 (0.034)	Loss 0.1154 (0.8465)	Epoch 0	Acc@1 100.000 (81.671)	Acc@5 100.000 (95.352)	Mem 269MB
[2024-11-10 17:51:58 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [16100/50000]	Time 0.033 (0.034)	Loss 3.6250 (0.8468)	Epoch 0	Acc@1 0.000 (81.666)	Acc@5 100.000 (95.348)	Mem 269MB
[2024-11-10 17:52:00 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [16150/50000]	Time 0.032 (0.034)	Loss 3.5508 (0.8460)	Epoch 0	Acc@1 0.000 (81.692)	Acc@5 100.000 (95.356)	Mem 269MB
[2024-11-10 17:52:02 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [16200/50000]	Time 0.032 (0.034)	Loss 0.1931 (0.8452)	Epoch 0	Acc@1 100.000 (81.711)	Acc@5 100.000 (95.364)	Mem 269MB
[2024-11-10 17:52:03 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [16250/50000]	Time 0.032 (0.034)	Loss 2.0645 (0.8445)	Epoch 0	Acc@1 0.000 (81.724)	Acc@5 100.000 (95.366)	Mem 269MB
[2024-11-10 17:52:05 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [16300/50000]	Time 0.032 (0.034)	Loss 2.3496 (0.8440)	Epoch 0	Acc@1 0.000 (81.731)	Acc@5 100.000 (95.375)	Mem 269MB
[2024-11-10 17:52:06 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [16350/50000]	Time 0.034 (0.034)	Loss 3.1035 (0.8429)	Epoch 0	Acc@1 0.000 (81.756)	Acc@5 100.000 (95.383)	Mem 269MB
[2024-11-10 17:52:08 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [16400/50000]	Time 0.032 (0.034)	Loss 0.1326 (0.8419)	Epoch 0	Acc@1 100.000 (81.782)	Acc@5 100.000 (95.391)	Mem 269MB
[2024-11-10 17:52:10 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [16450/50000]	Time 0.032 (0.034)	Loss 1.6133 (0.8418)	Epoch 0	Acc@1 0.000 (81.794)	Acc@5 100.000 (95.405)	Mem 269MB
[2024-11-10 17:52:11 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [16500/50000]	Time 0.032 (0.034)	Loss 0.1152 (0.8403)	Epoch 0	Acc@1 100.000 (81.831)	Acc@5 100.000 (95.418)	Mem 269MB
[2024-11-10 17:52:13 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [16550/50000]	Time 0.032 (0.034)	Loss 0.1920 (0.8397)	Epoch 0	Acc@1 100.000 (81.850)	Acc@5 100.000 (95.426)	Mem 269MB
[2024-11-10 17:52:15 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [16600/50000]	Time 0.032 (0.034)	Loss 0.7134 (0.8397)	Epoch 0	Acc@1 100.000 (81.863)	Acc@5 100.000 (95.428)	Mem 269MB
[2024-11-10 17:52:16 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [16650/50000]	Time 0.031 (0.034)	Loss 2.0391 (0.8394)	Epoch 0	Acc@1 0.000 (81.863)	Acc@5 100.000 (95.436)	Mem 269MB
[2024-11-10 17:52:18 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [16700/50000]	Time 0.033 (0.034)	Loss 0.2279 (0.8388)	Epoch 0	Acc@1 100.000 (81.863)	Acc@5 100.000 (95.437)	Mem 269MB
[2024-11-10 17:52:19 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [16750/50000]	Time 0.032 (0.034)	Loss 0.1150 (0.8392)	Epoch 0	Acc@1 100.000 (81.840)	Acc@5 100.000 (95.439)	Mem 269MB
[2024-11-10 17:52:21 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [16800/50000]	Time 0.032 (0.034)	Loss 0.1592 (0.8403)	Epoch 0	Acc@1 100.000 (81.805)	Acc@5 100.000 (95.435)	Mem 269MB
[2024-11-10 17:52:23 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [16850/50000]	Time 0.032 (0.034)	Loss 0.2166 (0.8400)	Epoch 0	Acc@1 100.000 (81.817)	Acc@5 100.000 (95.442)	Mem 269MB
[2024-11-10 17:52:24 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [16900/50000]	Time 0.032 (0.034)	Loss 3.6719 (0.8414)	Epoch 0	Acc@1 0.000 (81.776)	Acc@5 100.000 (95.426)	Mem 269MB
[2024-11-10 17:52:26 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [16950/50000]	Time 0.032 (0.034)	Loss 0.1064 (0.8412)	Epoch 0	Acc@1 100.000 (81.795)	Acc@5 100.000 (95.428)	Mem 269MB
[2024-11-10 17:52:27 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [17000/50000]	Time 0.032 (0.034)	Loss 0.1664 (0.8406)	Epoch 0	Acc@1 100.000 (81.813)	Acc@5 100.000 (95.436)	Mem 269MB
[2024-11-10 17:52:29 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [17050/50000]	Time 0.032 (0.034)	Loss 0.2952 (0.8405)	Epoch 0	Acc@1 100.000 (81.813)	Acc@5 100.000 (95.431)	Mem 269MB
[2024-11-10 17:52:31 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [17100/50000]	Time 0.032 (0.034)	Loss 0.0710 (0.8399)	Epoch 0	Acc@1 100.000 (81.820)	Acc@5 100.000 (95.439)	Mem 269MB
[2024-11-10 17:52:32 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [17150/50000]	Time 0.032 (0.034)	Loss 0.0956 (0.8390)	Epoch 0	Acc@1 100.000 (81.826)	Acc@5 100.000 (95.446)	Mem 269MB
[2024-11-10 17:52:34 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [17200/50000]	Time 0.032 (0.034)	Loss 0.0903 (0.8391)	Epoch 0	Acc@1 100.000 (81.821)	Acc@5 100.000 (95.436)	Mem 269MB
[2024-11-10 17:52:35 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [17250/50000]	Time 0.033 (0.034)	Loss 0.1306 (0.8393)	Epoch 0	Acc@1 100.000 (81.827)	Acc@5 100.000 (95.432)	Mem 269MB
[2024-11-10 17:52:37 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [17300/50000]	Time 0.032 (0.034)	Loss 1.4902 (0.8393)	Epoch 0	Acc@1 0.000 (81.828)	Acc@5 100.000 (95.440)	Mem 269MB
[2024-11-10 17:52:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [17350/50000]	Time 0.034 (0.034)	Loss 0.2380 (0.8394)	Epoch 0	Acc@1 100.000 (81.822)	Acc@5 100.000 (95.441)	Mem 269MB
[2024-11-10 17:52:40 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [17400/50000]	Time 0.032 (0.034)	Loss 0.2103 (0.8392)	Epoch 0	Acc@1 100.000 (81.823)	Acc@5 100.000 (95.449)	Mem 269MB
[2024-11-10 17:52:42 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [17450/50000]	Time 0.032 (0.034)	Loss 0.2007 (0.8388)	Epoch 0	Acc@1 100.000 (81.812)	Acc@5 100.000 (95.462)	Mem 269MB
[2024-11-10 17:52:43 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [17500/50000]	Time 0.032 (0.034)	Loss 0.1209 (0.8387)	Epoch 0	Acc@1 100.000 (81.824)	Acc@5 100.000 (95.457)	Mem 269MB
[2024-11-10 17:52:45 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [17550/50000]	Time 0.032 (0.034)	Loss 1.3730 (0.8386)	Epoch 0	Acc@1 0.000 (81.824)	Acc@5 100.000 (95.465)	Mem 269MB
[2024-11-10 17:52:47 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [17600/50000]	Time 0.032 (0.034)	Loss 0.7949 (0.8382)	Epoch 0	Acc@1 100.000 (81.842)	Acc@5 100.000 (95.460)	Mem 269MB
[2024-11-10 17:52:48 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [17650/50000]	Time 0.032 (0.034)	Loss 0.1102 (0.8379)	Epoch 0	Acc@1 100.000 (81.848)	Acc@5 100.000 (95.456)	Mem 269MB
[2024-11-10 17:52:50 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [17700/50000]	Time 0.032 (0.034)	Loss 0.1890 (0.8383)	Epoch 0	Acc@1 100.000 (81.837)	Acc@5 100.000 (95.458)	Mem 269MB
[2024-11-10 17:52:51 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [17750/50000]	Time 0.032 (0.034)	Loss 0.0981 (0.8382)	Epoch 0	Acc@1 100.000 (81.843)	Acc@5 100.000 (95.465)	Mem 269MB
[2024-11-10 17:52:53 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [17800/50000]	Time 0.032 (0.034)	Loss 1.0391 (0.8388)	Epoch 0	Acc@1 100.000 (81.827)	Acc@5 100.000 (95.467)	Mem 269MB
[2024-11-10 17:52:55 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [17850/50000]	Time 0.032 (0.034)	Loss 1.5918 (0.8403)	Epoch 0	Acc@1 0.000 (81.799)	Acc@5 100.000 (95.457)	Mem 269MB
[2024-11-10 17:52:56 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [17900/50000]	Time 0.032 (0.034)	Loss 0.1870 (0.8399)	Epoch 0	Acc@1 100.000 (81.794)	Acc@5 100.000 (95.470)	Mem 269MB
[2024-11-10 17:52:58 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [17950/50000]	Time 0.032 (0.034)	Loss 2.3066 (0.8401)	Epoch 0	Acc@1 0.000 (81.778)	Acc@5 100.000 (95.482)	Mem 269MB
[2024-11-10 17:52:59 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [18000/50000]	Time 0.032 (0.034)	Loss 0.0419 (0.8400)	Epoch 0	Acc@1 100.000 (81.784)	Acc@5 100.000 (95.484)	Mem 269MB
[2024-11-10 17:53:01 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [18050/50000]	Time 0.032 (0.034)	Loss 0.2576 (0.8391)	Epoch 0	Acc@1 100.000 (81.785)	Acc@5 100.000 (95.491)	Mem 269MB
[2024-11-10 17:53:03 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [18100/50000]	Time 0.032 (0.034)	Loss 0.1177 (0.8392)	Epoch 0	Acc@1 100.000 (81.791)	Acc@5 100.000 (95.475)	Mem 269MB
[2024-11-10 17:53:04 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [18150/50000]	Time 0.031 (0.034)	Loss 1.5156 (0.8400)	Epoch 0	Acc@1 0.000 (81.770)	Acc@5 100.000 (95.471)	Mem 269MB
[2024-11-10 17:53:06 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [18200/50000]	Time 0.032 (0.034)	Loss 0.1907 (0.8404)	Epoch 0	Acc@1 100.000 (81.754)	Acc@5 100.000 (95.473)	Mem 269MB
[2024-11-10 17:53:07 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [18250/50000]	Time 0.032 (0.034)	Loss 0.1109 (0.8409)	Epoch 0	Acc@1 100.000 (81.749)	Acc@5 100.000 (95.463)	Mem 269MB
[2024-11-10 17:53:09 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [18300/50000]	Time 0.033 (0.034)	Loss 3.0469 (0.8417)	Epoch 0	Acc@1 0.000 (81.744)	Acc@5 100.000 (95.454)	Mem 269MB
[2024-11-10 17:53:11 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [18350/50000]	Time 0.033 (0.034)	Loss 0.1891 (0.8416)	Epoch 0	Acc@1 100.000 (81.739)	Acc@5 100.000 (95.461)	Mem 269MB
[2024-11-10 17:53:12 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [18400/50000]	Time 0.032 (0.034)	Loss 0.1422 (0.8420)	Epoch 0	Acc@1 100.000 (81.735)	Acc@5 100.000 (95.457)	Mem 269MB
[2024-11-10 17:53:14 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [18450/50000]	Time 0.032 (0.034)	Loss 0.3843 (0.8418)	Epoch 0	Acc@1 100.000 (81.746)	Acc@5 100.000 (95.458)	Mem 269MB
[2024-11-10 17:53:15 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [18500/50000]	Time 0.032 (0.034)	Loss 0.1571 (0.8414)	Epoch 0	Acc@1 100.000 (81.769)	Acc@5 100.000 (95.460)	Mem 269MB
[2024-11-10 17:53:17 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [18550/50000]	Time 0.032 (0.034)	Loss 3.0703 (0.8410)	Epoch 0	Acc@1 0.000 (81.769)	Acc@5 100.000 (95.467)	Mem 269MB
[2024-11-10 17:53:19 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [18600/50000]	Time 0.032 (0.034)	Loss 0.1231 (0.8409)	Epoch 0	Acc@1 100.000 (81.775)	Acc@5 100.000 (95.468)	Mem 269MB
[2024-11-10 17:53:20 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [18650/50000]	Time 0.032 (0.034)	Loss 0.1377 (0.8414)	Epoch 0	Acc@1 100.000 (81.776)	Acc@5 100.000 (95.459)	Mem 269MB
[2024-11-10 17:53:22 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [18700/50000]	Time 0.032 (0.034)	Loss 0.2174 (0.8406)	Epoch 0	Acc@1 100.000 (81.787)	Acc@5 100.000 (95.471)	Mem 269MB
[2024-11-10 17:53:23 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [18750/50000]	Time 0.032 (0.034)	Loss 1.5977 (0.8421)	Epoch 0	Acc@1 100.000 (81.766)	Acc@5 100.000 (95.446)	Mem 269MB
[2024-11-10 17:53:25 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [18800/50000]	Time 0.032 (0.034)	Loss 0.2102 (0.8415)	Epoch 0	Acc@1 100.000 (81.788)	Acc@5 100.000 (95.452)	Mem 269MB
[2024-11-10 17:53:27 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [18850/50000]	Time 0.033 (0.034)	Loss 0.5986 (0.8411)	Epoch 0	Acc@1 100.000 (81.805)	Acc@5 100.000 (95.454)	Mem 269MB
[2024-11-10 17:53:28 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [18900/50000]	Time 0.032 (0.034)	Loss 2.0195 (0.8405)	Epoch 0	Acc@1 0.000 (81.816)	Acc@5 100.000 (95.466)	Mem 269MB
[2024-11-10 17:53:30 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [18950/50000]	Time 0.031 (0.034)	Loss 3.6445 (0.8407)	Epoch 0	Acc@1 0.000 (81.795)	Acc@5 100.000 (95.462)	Mem 269MB
[2024-11-10 17:53:31 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [19000/50000]	Time 0.032 (0.034)	Loss 0.1021 (0.8412)	Epoch 0	Acc@1 100.000 (81.785)	Acc@5 100.000 (95.469)	Mem 269MB
[2024-11-10 17:53:33 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [19050/50000]	Time 0.039 (0.034)	Loss 0.6616 (0.8411)	Epoch 0	Acc@1 100.000 (81.780)	Acc@5 100.000 (95.470)	Mem 269MB
[2024-11-10 17:53:35 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [19100/50000]	Time 0.039 (0.034)	Loss 0.3679 (0.8405)	Epoch 0	Acc@1 100.000 (81.792)	Acc@5 100.000 (95.477)	Mem 269MB
[2024-11-10 17:53:37 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [19150/50000]	Time 0.038 (0.034)	Loss 0.2969 (0.8400)	Epoch 0	Acc@1 100.000 (81.818)	Acc@5 100.000 (95.483)	Mem 269MB
[2024-11-10 17:53:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [19200/50000]	Time 0.039 (0.034)	Loss 0.0642 (0.8399)	Epoch 0	Acc@1 100.000 (81.813)	Acc@5 100.000 (95.479)	Mem 269MB
[2024-11-10 17:53:41 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [19250/50000]	Time 0.032 (0.034)	Loss 0.1099 (0.8399)	Epoch 0	Acc@1 100.000 (81.809)	Acc@5 100.000 (95.481)	Mem 269MB
[2024-11-10 17:53:43 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [19300/50000]	Time 0.032 (0.034)	Loss 0.0571 (0.8394)	Epoch 0	Acc@1 100.000 (81.830)	Acc@5 100.000 (95.482)	Mem 269MB
[2024-11-10 17:53:44 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [19350/50000]	Time 0.032 (0.034)	Loss 4.4766 (0.8397)	Epoch 0	Acc@1 0.000 (81.810)	Acc@5 100.000 (95.489)	Mem 269MB
[2024-11-10 17:53:46 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [19400/50000]	Time 0.032 (0.034)	Loss 0.5308 (0.8389)	Epoch 0	Acc@1 100.000 (81.836)	Acc@5 100.000 (95.495)	Mem 269MB
[2024-11-10 17:53:47 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [19450/50000]	Time 0.032 (0.034)	Loss 4.5742 (0.8399)	Epoch 0	Acc@1 0.000 (81.811)	Acc@5 0.000 (95.486)	Mem 269MB
[2024-11-10 17:53:49 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [19500/50000]	Time 0.031 (0.034)	Loss 0.1774 (0.8398)	Epoch 0	Acc@1 100.000 (81.801)	Acc@5 100.000 (95.487)	Mem 269MB
[2024-11-10 17:53:51 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [19550/50000]	Time 0.032 (0.034)	Loss 0.9277 (0.8405)	Epoch 0	Acc@1 100.000 (81.781)	Acc@5 100.000 (95.478)	Mem 269MB
[2024-11-10 17:53:52 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [19600/50000]	Time 0.032 (0.034)	Loss 0.2333 (0.8396)	Epoch 0	Acc@1 100.000 (81.797)	Acc@5 100.000 (95.490)	Mem 269MB
[2024-11-10 17:53:54 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [19650/50000]	Time 0.031 (0.034)	Loss 0.2717 (0.8396)	Epoch 0	Acc@1 100.000 (81.787)	Acc@5 100.000 (95.491)	Mem 269MB
[2024-11-10 17:53:55 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [19700/50000]	Time 0.032 (0.034)	Loss 0.1159 (0.8394)	Epoch 0	Acc@1 100.000 (81.788)	Acc@5 100.000 (95.498)	Mem 269MB
[2024-11-10 17:53:57 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [19750/50000]	Time 0.031 (0.034)	Loss 0.1141 (0.8391)	Epoch 0	Acc@1 100.000 (81.783)	Acc@5 100.000 (95.509)	Mem 269MB
[2024-11-10 17:53:59 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [19800/50000]	Time 0.032 (0.034)	Loss 0.6855 (0.8396)	Epoch 0	Acc@1 100.000 (81.769)	Acc@5 100.000 (95.510)	Mem 269MB
[2024-11-10 17:54:00 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [19850/50000]	Time 0.032 (0.034)	Loss 3.9668 (0.8402)	Epoch 0	Acc@1 0.000 (81.754)	Acc@5 100.000 (95.517)	Mem 269MB
[2024-11-10 17:54:02 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [19900/50000]	Time 0.032 (0.034)	Loss 0.1290 (0.8406)	Epoch 0	Acc@1 100.000 (81.755)	Acc@5 100.000 (95.513)	Mem 269MB
[2024-11-10 17:54:03 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [19950/50000]	Time 0.031 (0.034)	Loss 0.5293 (0.8402)	Epoch 0	Acc@1 100.000 (81.760)	Acc@5 100.000 (95.509)	Mem 269MB
[2024-11-10 17:54:05 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [20000/50000]	Time 0.032 (0.034)	Loss 3.3574 (0.8402)	Epoch 0	Acc@1 0.000 (81.766)	Acc@5 100.000 (95.505)	Mem 269MB
[2024-11-10 17:54:06 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [20050/50000]	Time 0.032 (0.034)	Loss 0.3066 (0.8397)	Epoch 0	Acc@1 100.000 (81.766)	Acc@5 100.000 (95.511)	Mem 269MB
[2024-11-10 17:54:08 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [20100/50000]	Time 0.032 (0.034)	Loss 0.0822 (0.8393)	Epoch 0	Acc@1 100.000 (81.762)	Acc@5 100.000 (95.513)	Mem 269MB
[2024-11-10 17:54:10 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [20150/50000]	Time 0.032 (0.034)	Loss 0.4792 (0.8394)	Epoch 0	Acc@1 100.000 (81.773)	Acc@5 100.000 (95.499)	Mem 269MB
[2024-11-10 17:54:11 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [20200/50000]	Time 0.032 (0.034)	Loss 0.3076 (0.8387)	Epoch 0	Acc@1 100.000 (81.788)	Acc@5 100.000 (95.505)	Mem 269MB
[2024-11-10 17:54:13 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [20250/50000]	Time 0.032 (0.034)	Loss 0.1553 (0.8388)	Epoch 0	Acc@1 100.000 (81.803)	Acc@5 100.000 (95.497)	Mem 269MB
[2024-11-10 17:54:15 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [20300/50000]	Time 0.032 (0.034)	Loss 0.2363 (0.8384)	Epoch 0	Acc@1 100.000 (81.804)	Acc@5 100.000 (95.498)	Mem 269MB
[2024-11-10 17:54:16 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [20350/50000]	Time 0.035 (0.034)	Loss 0.2228 (0.8374)	Epoch 0	Acc@1 100.000 (81.824)	Acc@5 100.000 (95.504)	Mem 269MB
[2024-11-10 17:54:18 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [20400/50000]	Time 0.032 (0.034)	Loss 0.1489 (0.8368)	Epoch 0	Acc@1 100.000 (81.834)	Acc@5 100.000 (95.510)	Mem 269MB
[2024-11-10 17:54:19 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [20450/50000]	Time 0.032 (0.034)	Loss 0.0880 (0.8375)	Epoch 0	Acc@1 100.000 (81.810)	Acc@5 100.000 (95.506)	Mem 269MB
[2024-11-10 17:54:21 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [20500/50000]	Time 0.032 (0.034)	Loss 0.1989 (0.8379)	Epoch 0	Acc@1 100.000 (81.806)	Acc@5 100.000 (95.508)	Mem 269MB
[2024-11-10 17:54:23 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [20550/50000]	Time 0.035 (0.034)	Loss 1.9990 (0.8376)	Epoch 0	Acc@1 0.000 (81.801)	Acc@5 100.000 (95.514)	Mem 269MB
[2024-11-10 17:54:24 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [20600/50000]	Time 0.032 (0.034)	Loss 0.1642 (0.8375)	Epoch 0	Acc@1 100.000 (81.802)	Acc@5 100.000 (95.515)	Mem 269MB
[2024-11-10 17:54:26 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [20650/50000]	Time 0.033 (0.034)	Loss 0.1912 (0.8381)	Epoch 0	Acc@1 100.000 (81.797)	Acc@5 100.000 (95.516)	Mem 269MB
[2024-11-10 17:54:28 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [20700/50000]	Time 0.032 (0.034)	Loss 0.1547 (0.8386)	Epoch 0	Acc@1 100.000 (81.769)	Acc@5 100.000 (95.503)	Mem 269MB
[2024-11-10 17:54:29 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [20750/50000]	Time 0.035 (0.034)	Loss 0.2834 (0.8391)	Epoch 0	Acc@1 100.000 (81.760)	Acc@5 100.000 (95.499)	Mem 269MB
[2024-11-10 17:54:31 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [20800/50000]	Time 0.032 (0.034)	Loss 8.1562 (0.8394)	Epoch 0	Acc@1 0.000 (81.756)	Acc@5 0.000 (95.495)	Mem 269MB
[2024-11-10 17:54:32 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [20850/50000]	Time 0.032 (0.034)	Loss 0.0279 (0.8392)	Epoch 0	Acc@1 100.000 (81.771)	Acc@5 100.000 (95.497)	Mem 269MB
[2024-11-10 17:54:34 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [20900/50000]	Time 0.032 (0.034)	Loss 0.4507 (0.8389)	Epoch 0	Acc@1 100.000 (81.771)	Acc@5 100.000 (95.503)	Mem 269MB
[2024-11-10 17:54:36 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [20950/50000]	Time 0.032 (0.034)	Loss 0.0524 (0.8383)	Epoch 0	Acc@1 100.000 (81.791)	Acc@5 100.000 (95.509)	Mem 269MB
[2024-11-10 17:54:37 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [21000/50000]	Time 0.032 (0.034)	Loss 0.1385 (0.8380)	Epoch 0	Acc@1 100.000 (81.806)	Acc@5 100.000 (95.505)	Mem 269MB
[2024-11-10 17:54:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [21050/50000]	Time 0.032 (0.034)	Loss 0.1361 (0.8377)	Epoch 0	Acc@1 100.000 (81.811)	Acc@5 100.000 (95.506)	Mem 269MB
[2024-11-10 17:54:40 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [21100/50000]	Time 0.033 (0.034)	Loss 0.1011 (0.8380)	Epoch 0	Acc@1 100.000 (81.807)	Acc@5 100.000 (95.507)	Mem 269MB
[2024-11-10 17:54:42 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [21150/50000]	Time 0.032 (0.034)	Loss 0.1903 (0.8382)	Epoch 0	Acc@1 100.000 (81.802)	Acc@5 100.000 (95.504)	Mem 269MB
[2024-11-10 17:54:44 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [21200/50000]	Time 0.032 (0.034)	Loss 0.4973 (0.8386)	Epoch 0	Acc@1 100.000 (81.798)	Acc@5 100.000 (95.505)	Mem 269MB
[2024-11-10 17:54:45 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [21250/50000]	Time 0.032 (0.034)	Loss 0.1788 (0.8385)	Epoch 0	Acc@1 100.000 (81.794)	Acc@5 100.000 (95.511)	Mem 269MB
[2024-11-10 17:54:47 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [21300/50000]	Time 0.032 (0.034)	Loss 0.2778 (0.8387)	Epoch 0	Acc@1 100.000 (81.780)	Acc@5 100.000 (95.517)	Mem 269MB
[2024-11-10 17:54:48 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [21350/50000]	Time 0.031 (0.034)	Loss 0.2079 (0.8384)	Epoch 0	Acc@1 100.000 (81.776)	Acc@5 100.000 (95.522)	Mem 269MB
[2024-11-10 17:54:50 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [21400/50000]	Time 0.032 (0.034)	Loss 0.1573 (0.8378)	Epoch 0	Acc@1 100.000 (81.791)	Acc@5 100.000 (95.533)	Mem 269MB
[2024-11-10 17:54:52 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [21450/50000]	Time 0.032 (0.034)	Loss 0.1677 (0.8370)	Epoch 0	Acc@1 100.000 (81.805)	Acc@5 100.000 (95.543)	Mem 269MB
[2024-11-10 17:54:53 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [21500/50000]	Time 0.032 (0.034)	Loss 0.1140 (0.8371)	Epoch 0	Acc@1 100.000 (81.796)	Acc@5 100.000 (95.535)	Mem 269MB
[2024-11-10 17:54:55 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [21550/50000]	Time 0.032 (0.034)	Loss 0.2786 (0.8371)	Epoch 0	Acc@1 100.000 (81.806)	Acc@5 100.000 (95.536)	Mem 269MB
[2024-11-10 17:54:56 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [21600/50000]	Time 0.032 (0.034)	Loss 0.1370 (0.8368)	Epoch 0	Acc@1 100.000 (81.816)	Acc@5 100.000 (95.542)	Mem 269MB
[2024-11-10 17:54:58 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [21650/50000]	Time 0.032 (0.034)	Loss 0.1477 (0.8365)	Epoch 0	Acc@1 100.000 (81.825)	Acc@5 100.000 (95.543)	Mem 269MB
[2024-11-10 17:55:00 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [21700/50000]	Time 0.032 (0.034)	Loss 0.9941 (0.8363)	Epoch 0	Acc@1 100.000 (81.835)	Acc@5 100.000 (95.544)	Mem 269MB
[2024-11-10 17:55:01 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [21750/50000]	Time 0.032 (0.034)	Loss 0.1504 (0.8360)	Epoch 0	Acc@1 100.000 (81.840)	Acc@5 100.000 (95.545)	Mem 269MB
[2024-11-10 17:55:03 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [21800/50000]	Time 0.032 (0.034)	Loss 3.3398 (0.8362)	Epoch 0	Acc@1 0.000 (81.836)	Acc@5 100.000 (95.546)	Mem 269MB
[2024-11-10 17:55:04 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [21850/50000]	Time 0.032 (0.034)	Loss 6.6523 (0.8356)	Epoch 0	Acc@1 0.000 (81.850)	Acc@5 100.000 (95.556)	Mem 269MB
[2024-11-10 17:55:06 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [21900/50000]	Time 0.032 (0.034)	Loss 0.0729 (0.8353)	Epoch 0	Acc@1 100.000 (81.859)	Acc@5 100.000 (95.566)	Mem 269MB
[2024-11-10 17:55:08 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [21950/50000]	Time 0.032 (0.034)	Loss 5.6328 (0.8348)	Epoch 0	Acc@1 0.000 (81.869)	Acc@5 0.000 (95.567)	Mem 269MB
[2024-11-10 17:55:09 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [22000/50000]	Time 0.033 (0.034)	Loss 0.1096 (0.8347)	Epoch 0	Acc@1 100.000 (81.860)	Acc@5 100.000 (95.573)	Mem 269MB
[2024-11-10 17:55:11 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [22050/50000]	Time 0.032 (0.034)	Loss 0.0606 (0.8346)	Epoch 0	Acc@1 100.000 (81.869)	Acc@5 100.000 (95.569)	Mem 269MB
[2024-11-10 17:55:12 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [22100/50000]	Time 0.032 (0.034)	Loss 0.2057 (0.8341)	Epoch 0	Acc@1 100.000 (81.874)	Acc@5 100.000 (95.579)	Mem 269MB
[2024-11-10 17:55:14 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [22150/50000]	Time 0.032 (0.034)	Loss 0.1531 (0.8347)	Epoch 0	Acc@1 100.000 (81.861)	Acc@5 100.000 (95.580)	Mem 269MB
[2024-11-10 17:55:16 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [22200/50000]	Time 0.032 (0.034)	Loss 1.1865 (0.8347)	Epoch 0	Acc@1 100.000 (81.848)	Acc@5 100.000 (95.581)	Mem 269MB
[2024-11-10 17:55:17 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [22250/50000]	Time 0.032 (0.034)	Loss 0.0993 (0.8343)	Epoch 0	Acc@1 100.000 (81.861)	Acc@5 100.000 (95.582)	Mem 269MB
[2024-11-10 17:55:19 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [22300/50000]	Time 0.032 (0.034)	Loss 0.1350 (0.8342)	Epoch 0	Acc@1 100.000 (81.871)	Acc@5 100.000 (95.579)	Mem 269MB
[2024-11-10 17:55:20 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [22350/50000]	Time 0.032 (0.034)	Loss 0.1438 (0.8331)	Epoch 0	Acc@1 100.000 (81.893)	Acc@5 100.000 (95.589)	Mem 269MB
[2024-11-10 17:55:22 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [22400/50000]	Time 0.034 (0.034)	Loss 6.1602 (0.8341)	Epoch 0	Acc@1 0.000 (81.880)	Acc@5 0.000 (95.576)	Mem 269MB
[2024-11-10 17:55:24 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [22450/50000]	Time 0.031 (0.034)	Loss 4.5000 (0.8343)	Epoch 0	Acc@1 0.000 (81.863)	Acc@5 100.000 (95.577)	Mem 269MB
[2024-11-10 17:55:25 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [22500/50000]	Time 0.032 (0.034)	Loss 0.1324 (0.8345)	Epoch 0	Acc@1 100.000 (81.850)	Acc@5 100.000 (95.578)	Mem 269MB
[2024-11-10 17:55:27 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [22550/50000]	Time 0.032 (0.034)	Loss 0.1022 (0.8337)	Epoch 0	Acc@1 100.000 (81.868)	Acc@5 100.000 (95.588)	Mem 269MB
[2024-11-10 17:55:28 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [22600/50000]	Time 0.032 (0.034)	Loss 0.0891 (0.8342)	Epoch 0	Acc@1 100.000 (81.864)	Acc@5 100.000 (95.580)	Mem 269MB
[2024-11-10 17:55:30 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [22650/50000]	Time 0.032 (0.034)	Loss 0.1608 (0.8344)	Epoch 0	Acc@1 100.000 (81.864)	Acc@5 100.000 (95.581)	Mem 269MB
[2024-11-10 17:55:32 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [22700/50000]	Time 0.032 (0.034)	Loss 0.1017 (0.8356)	Epoch 0	Acc@1 100.000 (81.829)	Acc@5 100.000 (95.573)	Mem 269MB
[2024-11-10 17:55:33 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [22750/50000]	Time 0.031 (0.034)	Loss 0.1131 (0.8361)	Epoch 0	Acc@1 100.000 (81.821)	Acc@5 100.000 (95.565)	Mem 269MB
[2024-11-10 17:55:35 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [22800/50000]	Time 0.033 (0.034)	Loss 1.1309 (0.8363)	Epoch 0	Acc@1 100.000 (81.817)	Acc@5 100.000 (95.553)	Mem 269MB
[2024-11-10 17:55:36 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [22850/50000]	Time 0.032 (0.034)	Loss 0.2761 (0.8362)	Epoch 0	Acc@1 100.000 (81.826)	Acc@5 100.000 (95.554)	Mem 269MB
[2024-11-10 17:55:38 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [22900/50000]	Time 0.032 (0.034)	Loss 0.1160 (0.8351)	Epoch 0	Acc@1 100.000 (81.857)	Acc@5 100.000 (95.564)	Mem 269MB
[2024-11-10 17:55:40 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [22950/50000]	Time 0.032 (0.034)	Loss 0.1517 (0.8349)	Epoch 0	Acc@1 100.000 (81.870)	Acc@5 100.000 (95.560)	Mem 269MB
[2024-11-10 17:55:41 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [23000/50000]	Time 0.033 (0.034)	Loss 0.1080 (0.8343)	Epoch 0	Acc@1 100.000 (81.883)	Acc@5 100.000 (95.570)	Mem 269MB
[2024-11-10 17:55:43 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [23050/50000]	Time 0.032 (0.034)	Loss 0.1423 (0.8356)	Epoch 0	Acc@1 100.000 (81.862)	Acc@5 100.000 (95.562)	Mem 269MB
[2024-11-10 17:55:44 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [23100/50000]	Time 0.032 (0.034)	Loss 0.1168 (0.8361)	Epoch 0	Acc@1 100.000 (81.858)	Acc@5 100.000 (95.559)	Mem 269MB
[2024-11-10 17:55:46 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [23150/50000]	Time 0.032 (0.034)	Loss 4.6250 (0.8354)	Epoch 0	Acc@1 0.000 (81.871)	Acc@5 100.000 (95.568)	Mem 269MB
[2024-11-10 17:55:48 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [23200/50000]	Time 0.032 (0.034)	Loss 2.0742 (0.8356)	Epoch 0	Acc@1 0.000 (81.859)	Acc@5 100.000 (95.565)	Mem 269MB
[2024-11-10 17:55:49 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [23250/50000]	Time 0.032 (0.034)	Loss 1.7207 (0.8352)	Epoch 0	Acc@1 0.000 (81.850)	Acc@5 100.000 (95.570)	Mem 269MB
[2024-11-10 17:55:51 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [23300/50000]	Time 0.032 (0.034)	Loss 0.7319 (0.8352)	Epoch 0	Acc@1 100.000 (81.855)	Acc@5 100.000 (95.575)	Mem 269MB
[2024-11-10 17:55:52 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [23350/50000]	Time 0.032 (0.034)	Loss 0.1853 (0.8362)	Epoch 0	Acc@1 100.000 (81.825)	Acc@5 100.000 (95.572)	Mem 269MB
[2024-11-10 17:55:54 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [23400/50000]	Time 0.032 (0.034)	Loss 0.1278 (0.8358)	Epoch 0	Acc@1 100.000 (81.838)	Acc@5 100.000 (95.573)	Mem 269MB
[2024-11-10 17:55:56 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [23450/50000]	Time 0.032 (0.033)	Loss 0.1277 (0.8362)	Epoch 0	Acc@1 100.000 (81.834)	Acc@5 100.000 (95.569)	Mem 269MB
[2024-11-10 17:55:57 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [23500/50000]	Time 0.032 (0.033)	Loss 2.0547 (0.8359)	Epoch 0	Acc@1 0.000 (81.831)	Acc@5 100.000 (95.575)	Mem 269MB
[2024-11-10 17:55:59 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [23550/50000]	Time 0.032 (0.033)	Loss 6.9648 (0.8362)	Epoch 0	Acc@1 0.000 (81.822)	Acc@5 0.000 (95.571)	Mem 269MB
[2024-11-10 17:56:00 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [23600/50000]	Time 0.031 (0.033)	Loss 0.1519 (0.8362)	Epoch 0	Acc@1 100.000 (81.823)	Acc@5 100.000 (95.572)	Mem 269MB
[2024-11-10 17:56:02 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [23650/50000]	Time 0.032 (0.033)	Loss 2.8828 (0.8357)	Epoch 0	Acc@1 0.000 (81.832)	Acc@5 100.000 (95.577)	Mem 269MB
[2024-11-10 17:56:04 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [23700/50000]	Time 0.032 (0.033)	Loss 0.1277 (0.8355)	Epoch 0	Acc@1 100.000 (81.819)	Acc@5 100.000 (95.578)	Mem 269MB
[2024-11-10 17:56:05 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [23750/50000]	Time 0.032 (0.033)	Loss 0.2250 (0.8360)	Epoch 0	Acc@1 100.000 (81.811)	Acc@5 100.000 (95.571)	Mem 269MB
[2024-11-10 17:56:07 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [23800/50000]	Time 0.032 (0.033)	Loss 0.0809 (0.8361)	Epoch 0	Acc@1 100.000 (81.812)	Acc@5 100.000 (95.567)	Mem 269MB
[2024-11-10 17:56:08 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [23850/50000]	Time 0.032 (0.033)	Loss 0.2111 (0.8360)	Epoch 0	Acc@1 100.000 (81.804)	Acc@5 100.000 (95.577)	Mem 269MB
[2024-11-10 17:56:10 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [23900/50000]	Time 0.032 (0.033)	Loss 0.7256 (0.8351)	Epoch 0	Acc@1 100.000 (81.829)	Acc@5 100.000 (95.582)	Mem 269MB
[2024-11-10 17:56:12 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [23950/50000]	Time 0.038 (0.033)	Loss 0.1195 (0.8356)	Epoch 0	Acc@1 100.000 (81.805)	Acc@5 100.000 (95.587)	Mem 269MB
[2024-11-10 17:56:13 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [24000/50000]	Time 0.032 (0.033)	Loss 0.2086 (0.8358)	Epoch 0	Acc@1 100.000 (81.809)	Acc@5 100.000 (95.584)	Mem 269MB
[2024-11-10 17:56:15 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [24050/50000]	Time 0.032 (0.033)	Loss 0.2263 (0.8362)	Epoch 0	Acc@1 100.000 (81.801)	Acc@5 100.000 (95.580)	Mem 269MB
[2024-11-10 17:56:17 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [24100/50000]	Time 0.032 (0.033)	Loss 0.1627 (0.8366)	Epoch 0	Acc@1 100.000 (81.797)	Acc@5 100.000 (95.573)	Mem 269MB
[2024-11-10 17:56:18 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [24150/50000]	Time 0.033 (0.033)	Loss 6.4648 (0.8370)	Epoch 0	Acc@1 0.000 (81.790)	Acc@5 0.000 (95.570)	Mem 269MB
[2024-11-10 17:56:20 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [24200/50000]	Time 0.034 (0.033)	Loss 0.5762 (0.8373)	Epoch 0	Acc@1 100.000 (81.769)	Acc@5 100.000 (95.570)	Mem 269MB
[2024-11-10 17:56:21 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [24250/50000]	Time 0.032 (0.033)	Loss 4.0703 (0.8375)	Epoch 0	Acc@1 0.000 (81.762)	Acc@5 100.000 (95.571)	Mem 269MB
[2024-11-10 17:56:23 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [24300/50000]	Time 0.032 (0.033)	Loss 5.1250 (0.8376)	Epoch 0	Acc@1 0.000 (81.741)	Acc@5 0.000 (95.572)	Mem 269MB
[2024-11-10 17:56:25 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [24350/50000]	Time 0.032 (0.033)	Loss 1.0557 (0.8367)	Epoch 0	Acc@1 100.000 (81.771)	Acc@5 100.000 (95.581)	Mem 269MB
[2024-11-10 17:56:26 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [24400/50000]	Time 0.032 (0.033)	Loss 1.6465 (0.8364)	Epoch 0	Acc@1 0.000 (81.779)	Acc@5 100.000 (95.582)	Mem 269MB
[2024-11-10 17:56:28 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [24450/50000]	Time 0.032 (0.033)	Loss 0.0837 (0.8364)	Epoch 0	Acc@1 100.000 (81.784)	Acc@5 100.000 (95.575)	Mem 269MB
[2024-11-10 17:56:29 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [24500/50000]	Time 0.032 (0.033)	Loss 0.4324 (0.8375)	Epoch 0	Acc@1 100.000 (81.752)	Acc@5 100.000 (95.572)	Mem 269MB
[2024-11-10 17:56:31 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [24550/50000]	Time 0.032 (0.033)	Loss 0.1578 (0.8373)	Epoch 0	Acc@1 100.000 (81.752)	Acc@5 100.000 (95.577)	Mem 269MB
[2024-11-10 17:56:33 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [24600/50000]	Time 0.032 (0.033)	Loss 0.2173 (0.8377)	Epoch 0	Acc@1 100.000 (81.737)	Acc@5 100.000 (95.577)	Mem 269MB
[2024-11-10 17:56:34 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [24650/50000]	Time 0.034 (0.033)	Loss 0.1111 (0.8372)	Epoch 0	Acc@1 100.000 (81.733)	Acc@5 100.000 (95.586)	Mem 269MB
[2024-11-10 17:56:36 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [24700/50000]	Time 0.032 (0.033)	Loss 1.2266 (0.8368)	Epoch 0	Acc@1 0.000 (81.746)	Acc@5 100.000 (95.591)	Mem 269MB
[2024-11-10 17:56:37 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [24750/50000]	Time 0.032 (0.033)	Loss 0.1295 (0.8364)	Epoch 0	Acc@1 100.000 (81.750)	Acc@5 100.000 (95.600)	Mem 269MB
[2024-11-10 17:56:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [24800/50000]	Time 0.039 (0.033)	Loss 1.1455 (0.8365)	Epoch 0	Acc@1 100.000 (81.747)	Acc@5 100.000 (95.601)	Mem 269MB
[2024-11-10 17:56:41 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [24850/50000]	Time 0.039 (0.033)	Loss 1.0381 (0.8372)	Epoch 0	Acc@1 100.000 (81.735)	Acc@5 100.000 (95.602)	Mem 269MB
[2024-11-10 17:56:43 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [24900/50000]	Time 0.039 (0.033)	Loss 0.7305 (0.8369)	Epoch 0	Acc@1 100.000 (81.744)	Acc@5 100.000 (95.603)	Mem 269MB
[2024-11-10 17:56:45 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [24950/50000]	Time 0.039 (0.033)	Loss 5.4102 (0.8371)	Epoch 0	Acc@1 0.000 (81.732)	Acc@5 0.000 (95.607)	Mem 269MB
[2024-11-10 17:56:47 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [25000/50000]	Time 0.039 (0.033)	Loss 0.1996 (0.8371)	Epoch 0	Acc@1 100.000 (81.721)	Acc@5 100.000 (95.612)	Mem 269MB
[2024-11-10 17:56:49 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [25050/50000]	Time 0.039 (0.033)	Loss 0.1036 (0.8374)	Epoch 0	Acc@1 100.000 (81.713)	Acc@5 100.000 (95.605)	Mem 269MB
[2024-11-10 17:56:51 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [25100/50000]	Time 0.039 (0.034)	Loss 0.1085 (0.8374)	Epoch 0	Acc@1 100.000 (81.706)	Acc@5 100.000 (95.602)	Mem 269MB
[2024-11-10 17:56:53 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [25150/50000]	Time 0.039 (0.034)	Loss 1.1104 (0.8380)	Epoch 0	Acc@1 100.000 (81.691)	Acc@5 100.000 (95.591)	Mem 269MB
[2024-11-10 17:56:55 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [25200/50000]	Time 0.039 (0.034)	Loss 0.1630 (0.8389)	Epoch 0	Acc@1 100.000 (81.667)	Acc@5 100.000 (95.572)	Mem 269MB
[2024-11-10 17:56:57 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [25250/50000]	Time 0.039 (0.034)	Loss 0.1127 (0.8388)	Epoch 0	Acc@1 100.000 (81.660)	Acc@5 100.000 (95.572)	Mem 269MB
[2024-11-10 17:56:59 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [25300/50000]	Time 0.032 (0.034)	Loss 0.1247 (0.8393)	Epoch 0	Acc@1 100.000 (81.661)	Acc@5 100.000 (95.561)	Mem 269MB
[2024-11-10 17:57:00 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [25350/50000]	Time 0.032 (0.034)	Loss 0.1436 (0.8394)	Epoch 0	Acc@1 100.000 (81.654)	Acc@5 100.000 (95.566)	Mem 269MB
[2024-11-10 17:57:02 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [25400/50000]	Time 0.032 (0.034)	Loss 0.2184 (0.8393)	Epoch 0	Acc@1 100.000 (81.654)	Acc@5 100.000 (95.571)	Mem 269MB
[2024-11-10 17:57:03 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [25450/50000]	Time 0.032 (0.034)	Loss 2.0918 (0.8386)	Epoch 0	Acc@1 0.000 (81.671)	Acc@5 100.000 (95.580)	Mem 269MB
[2024-11-10 17:57:05 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [25500/50000]	Time 0.032 (0.034)	Loss 0.1151 (0.8379)	Epoch 0	Acc@1 100.000 (81.687)	Acc@5 100.000 (95.588)	Mem 269MB
[2024-11-10 17:57:07 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [25550/50000]	Time 0.032 (0.034)	Loss 0.1704 (0.8375)	Epoch 0	Acc@1 100.000 (81.695)	Acc@5 100.000 (95.589)	Mem 269MB
[2024-11-10 17:57:08 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [25600/50000]	Time 0.032 (0.034)	Loss 0.1316 (0.8368)	Epoch 0	Acc@1 100.000 (81.704)	Acc@5 100.000 (95.598)	Mem 269MB
[2024-11-10 17:57:10 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [25650/50000]	Time 0.032 (0.034)	Loss 0.1570 (0.8360)	Epoch 0	Acc@1 100.000 (81.716)	Acc@5 100.000 (95.606)	Mem 269MB
[2024-11-10 17:57:11 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [25700/50000]	Time 0.032 (0.034)	Loss 0.0696 (0.8367)	Epoch 0	Acc@1 100.000 (81.697)	Acc@5 100.000 (95.603)	Mem 269MB
[2024-11-10 17:57:13 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [25750/50000]	Time 0.032 (0.034)	Loss 0.1265 (0.8372)	Epoch 0	Acc@1 100.000 (81.678)	Acc@5 100.000 (95.604)	Mem 269MB
[2024-11-10 17:57:15 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [25800/50000]	Time 0.032 (0.034)	Loss 0.2020 (0.8370)	Epoch 0	Acc@1 100.000 (81.687)	Acc@5 100.000 (95.605)	Mem 269MB
[2024-11-10 17:57:16 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [25850/50000]	Time 0.032 (0.034)	Loss 0.2139 (0.8377)	Epoch 0	Acc@1 100.000 (81.683)	Acc@5 100.000 (95.594)	Mem 269MB
[2024-11-10 17:57:18 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [25900/50000]	Time 0.032 (0.033)	Loss 0.0947 (0.8381)	Epoch 0	Acc@1 100.000 (81.661)	Acc@5 100.000 (95.595)	Mem 269MB
[2024-11-10 17:57:19 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [25950/50000]	Time 0.032 (0.033)	Loss 0.8906 (0.8381)	Epoch 0	Acc@1 100.000 (81.665)	Acc@5 100.000 (95.596)	Mem 269MB
[2024-11-10 17:57:21 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [26000/50000]	Time 0.032 (0.033)	Loss 0.1116 (0.8373)	Epoch 0	Acc@1 100.000 (81.685)	Acc@5 100.000 (95.600)	Mem 269MB
[2024-11-10 17:57:23 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [26050/50000]	Time 0.032 (0.033)	Loss 0.1989 (0.8378)	Epoch 0	Acc@1 100.000 (81.686)	Acc@5 100.000 (95.593)	Mem 269MB
[2024-11-10 17:57:24 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [26100/50000]	Time 0.032 (0.033)	Loss 2.7598 (0.8372)	Epoch 0	Acc@1 0.000 (81.694)	Acc@5 100.000 (95.602)	Mem 269MB
[2024-11-10 17:57:26 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [26150/50000]	Time 0.032 (0.033)	Loss 0.1826 (0.8364)	Epoch 0	Acc@1 100.000 (81.706)	Acc@5 100.000 (95.606)	Mem 269MB
[2024-11-10 17:57:27 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [26200/50000]	Time 0.033 (0.033)	Loss 0.1372 (0.8359)	Epoch 0	Acc@1 100.000 (81.718)	Acc@5 100.000 (95.615)	Mem 269MB
[2024-11-10 17:57:29 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [26250/50000]	Time 0.032 (0.033)	Loss 2.5430 (0.8366)	Epoch 0	Acc@1 0.000 (81.707)	Acc@5 100.000 (95.604)	Mem 269MB
[2024-11-10 17:57:30 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [26300/50000]	Time 0.032 (0.033)	Loss 2.5117 (0.8369)	Epoch 0	Acc@1 0.000 (81.708)	Acc@5 100.000 (95.601)	Mem 269MB
[2024-11-10 17:57:32 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [26350/50000]	Time 0.031 (0.033)	Loss 4.4102 (0.8371)	Epoch 0	Acc@1 0.000 (81.712)	Acc@5 0.000 (95.590)	Mem 269MB
[2024-11-10 17:57:34 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [26400/50000]	Time 0.032 (0.033)	Loss 0.1254 (0.8364)	Epoch 0	Acc@1 100.000 (81.724)	Acc@5 100.000 (95.599)	Mem 269MB
[2024-11-10 17:57:35 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [26450/50000]	Time 0.032 (0.033)	Loss 0.0862 (0.8357)	Epoch 0	Acc@1 100.000 (81.740)	Acc@5 100.000 (95.603)	Mem 269MB
[2024-11-10 17:57:37 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [26500/50000]	Time 0.032 (0.033)	Loss 0.3054 (0.8352)	Epoch 0	Acc@1 100.000 (81.748)	Acc@5 100.000 (95.611)	Mem 269MB
[2024-11-10 17:57:38 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [26550/50000]	Time 0.032 (0.033)	Loss 4.1992 (0.8354)	Epoch 0	Acc@1 0.000 (81.745)	Acc@5 100.000 (95.616)	Mem 269MB
[2024-11-10 17:57:40 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [26600/50000]	Time 0.032 (0.033)	Loss 0.1660 (0.8362)	Epoch 0	Acc@1 100.000 (81.730)	Acc@5 100.000 (95.613)	Mem 269MB
[2024-11-10 17:57:42 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [26650/50000]	Time 0.032 (0.033)	Loss 0.0928 (0.8359)	Epoch 0	Acc@1 100.000 (81.738)	Acc@5 100.000 (95.614)	Mem 269MB
[2024-11-10 17:57:43 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [26700/50000]	Time 0.032 (0.033)	Loss 0.3379 (0.8360)	Epoch 0	Acc@1 100.000 (81.735)	Acc@5 100.000 (95.614)	Mem 269MB
[2024-11-10 17:57:45 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [26750/50000]	Time 0.032 (0.033)	Loss 0.0850 (0.8361)	Epoch 0	Acc@1 100.000 (81.728)	Acc@5 100.000 (95.615)	Mem 269MB
[2024-11-10 17:57:46 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [26800/50000]	Time 0.032 (0.033)	Loss 0.5396 (0.8360)	Epoch 0	Acc@1 100.000 (81.725)	Acc@5 100.000 (95.623)	Mem 269MB
[2024-11-10 17:57:48 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [26850/50000]	Time 0.032 (0.033)	Loss 2.7559 (0.8363)	Epoch 0	Acc@1 0.000 (81.721)	Acc@5 100.000 (95.617)	Mem 269MB
[2024-11-10 17:57:50 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [26900/50000]	Time 0.032 (0.033)	Loss 4.8633 (0.8367)	Epoch 0	Acc@1 0.000 (81.703)	Acc@5 0.000 (95.617)	Mem 269MB
[2024-11-10 17:57:51 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [26950/50000]	Time 0.032 (0.033)	Loss 0.0742 (0.8369)	Epoch 0	Acc@1 100.000 (81.696)	Acc@5 100.000 (95.611)	Mem 269MB
[2024-11-10 17:57:53 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [27000/50000]	Time 0.033 (0.033)	Loss 0.1205 (0.8367)	Epoch 0	Acc@1 100.000 (81.701)	Acc@5 100.000 (95.611)	Mem 269MB
[2024-11-10 17:57:54 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [27050/50000]	Time 0.032 (0.033)	Loss 0.1523 (0.8360)	Epoch 0	Acc@1 100.000 (81.716)	Acc@5 100.000 (95.612)	Mem 269MB
[2024-11-10 17:57:56 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [27100/50000]	Time 0.032 (0.033)	Loss 0.1511 (0.8361)	Epoch 0	Acc@1 100.000 (81.709)	Acc@5 100.000 (95.609)	Mem 269MB
[2024-11-10 17:57:58 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [27150/50000]	Time 0.030 (0.033)	Loss 1.0840 (0.8364)	Epoch 0	Acc@1 0.000 (81.702)	Acc@5 100.000 (95.606)	Mem 269MB
[2024-11-10 17:57:59 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [27200/50000]	Time 0.032 (0.033)	Loss 0.1240 (0.8362)	Epoch 0	Acc@1 100.000 (81.696)	Acc@5 100.000 (95.610)	Mem 269MB
[2024-11-10 17:58:01 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [27250/50000]	Time 0.032 (0.033)	Loss 0.1908 (0.8361)	Epoch 0	Acc@1 100.000 (81.700)	Acc@5 100.000 (95.608)	Mem 269MB
[2024-11-10 17:58:02 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [27300/50000]	Time 0.032 (0.033)	Loss 0.1263 (0.8356)	Epoch 0	Acc@1 100.000 (81.704)	Acc@5 100.000 (95.616)	Mem 269MB
[2024-11-10 17:58:04 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [27350/50000]	Time 0.032 (0.033)	Loss 0.1451 (0.8353)	Epoch 0	Acc@1 100.000 (81.712)	Acc@5 100.000 (95.620)	Mem 269MB
[2024-11-10 17:58:06 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [27400/50000]	Time 0.032 (0.033)	Loss 0.0773 (0.8354)	Epoch 0	Acc@1 100.000 (81.698)	Acc@5 100.000 (95.621)	Mem 269MB
[2024-11-10 17:58:07 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [27450/50000]	Time 0.032 (0.033)	Loss 3.6621 (0.8356)	Epoch 0	Acc@1 0.000 (81.691)	Acc@5 0.000 (95.618)	Mem 269MB
[2024-11-10 17:58:09 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [27500/50000]	Time 0.032 (0.033)	Loss 0.0917 (0.8353)	Epoch 0	Acc@1 100.000 (81.695)	Acc@5 100.000 (95.622)	Mem 269MB
[2024-11-10 17:58:10 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [27550/50000]	Time 0.031 (0.033)	Loss 1.6934 (0.8351)	Epoch 0	Acc@1 0.000 (81.689)	Acc@5 100.000 (95.630)	Mem 269MB
[2024-11-10 17:58:12 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [27600/50000]	Time 0.032 (0.033)	Loss 0.1105 (0.8355)	Epoch 0	Acc@1 100.000 (81.682)	Acc@5 100.000 (95.634)	Mem 269MB
[2024-11-10 17:58:13 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [27650/50000]	Time 0.034 (0.033)	Loss 5.3203 (0.8360)	Epoch 0	Acc@1 0.000 (81.675)	Acc@5 0.000 (95.631)	Mem 269MB
[2024-11-10 17:58:15 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [27700/50000]	Time 0.032 (0.033)	Loss 0.3171 (0.8358)	Epoch 0	Acc@1 100.000 (81.687)	Acc@5 100.000 (95.632)	Mem 269MB
[2024-11-10 17:58:17 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [27750/50000]	Time 0.031 (0.033)	Loss 0.0984 (0.8354)	Epoch 0	Acc@1 100.000 (81.694)	Acc@5 100.000 (95.636)	Mem 269MB
[2024-11-10 17:58:18 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [27800/50000]	Time 0.032 (0.033)	Loss 0.1155 (0.8362)	Epoch 0	Acc@1 100.000 (81.681)	Acc@5 100.000 (95.622)	Mem 269MB
[2024-11-10 17:58:20 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [27850/50000]	Time 0.032 (0.033)	Loss 0.1780 (0.8364)	Epoch 0	Acc@1 100.000 (81.674)	Acc@5 100.000 (95.623)	Mem 269MB
[2024-11-10 17:58:21 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [27900/50000]	Time 0.032 (0.033)	Loss 0.1729 (0.8365)	Epoch 0	Acc@1 100.000 (81.678)	Acc@5 100.000 (95.617)	Mem 269MB
[2024-11-10 17:58:23 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [27950/50000]	Time 0.036 (0.033)	Loss 2.3418 (0.8361)	Epoch 0	Acc@1 0.000 (81.679)	Acc@5 100.000 (95.624)	Mem 269MB
[2024-11-10 17:58:25 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [28000/50000]	Time 0.032 (0.033)	Loss 0.1448 (0.8365)	Epoch 0	Acc@1 100.000 (81.669)	Acc@5 100.000 (95.618)	Mem 269MB
[2024-11-10 17:58:26 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [28050/50000]	Time 0.032 (0.033)	Loss 0.1215 (0.8362)	Epoch 0	Acc@1 100.000 (81.676)	Acc@5 100.000 (95.619)	Mem 269MB
[2024-11-10 17:58:28 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [28100/50000]	Time 0.031 (0.033)	Loss 0.0165 (0.8360)	Epoch 0	Acc@1 100.000 (81.687)	Acc@5 100.000 (95.619)	Mem 269MB
[2024-11-10 17:58:29 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [28150/50000]	Time 0.037 (0.033)	Loss 2.9023 (0.8357)	Epoch 0	Acc@1 0.000 (81.688)	Acc@5 100.000 (95.627)	Mem 269MB
[2024-11-10 17:58:31 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [28200/50000]	Time 0.032 (0.033)	Loss 2.4297 (0.8355)	Epoch 0	Acc@1 0.000 (81.689)	Acc@5 100.000 (95.635)	Mem 269MB
[2024-11-10 17:58:33 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [28250/50000]	Time 0.032 (0.033)	Loss 0.5791 (0.8356)	Epoch 0	Acc@1 100.000 (81.689)	Acc@5 100.000 (95.636)	Mem 269MB
[2024-11-10 17:58:34 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [28300/50000]	Time 0.032 (0.033)	Loss 0.2075 (0.8358)	Epoch 0	Acc@1 100.000 (81.672)	Acc@5 100.000 (95.640)	Mem 269MB
[2024-11-10 17:58:36 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [28350/50000]	Time 0.032 (0.033)	Loss 0.1738 (0.8356)	Epoch 0	Acc@1 100.000 (81.676)	Acc@5 100.000 (95.640)	Mem 269MB
[2024-11-10 17:58:38 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [28400/50000]	Time 0.032 (0.033)	Loss 0.1138 (0.8354)	Epoch 0	Acc@1 100.000 (81.680)	Acc@5 100.000 (95.641)	Mem 269MB
[2024-11-10 17:58:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [28450/50000]	Time 0.032 (0.033)	Loss 5.7344 (0.8361)	Epoch 0	Acc@1 0.000 (81.667)	Acc@5 100.000 (95.638)	Mem 269MB
[2024-11-10 17:58:41 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [28500/50000]	Time 0.031 (0.033)	Loss 0.1428 (0.8362)	Epoch 0	Acc@1 100.000 (81.660)	Acc@5 100.000 (95.635)	Mem 269MB
[2024-11-10 17:58:42 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [28550/50000]	Time 0.032 (0.033)	Loss 0.5122 (0.8362)	Epoch 0	Acc@1 100.000 (81.668)	Acc@5 100.000 (95.636)	Mem 269MB
[2024-11-10 17:58:44 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [28600/50000]	Time 0.032 (0.033)	Loss 2.1719 (0.8357)	Epoch 0	Acc@1 0.000 (81.675)	Acc@5 100.000 (95.640)	Mem 269MB
[2024-11-10 17:58:46 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [28650/50000]	Time 0.031 (0.033)	Loss 0.8462 (0.8366)	Epoch 0	Acc@1 100.000 (81.652)	Acc@5 100.000 (95.630)	Mem 269MB
[2024-11-10 17:58:47 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [28700/50000]	Time 0.031 (0.033)	Loss 0.1492 (0.8363)	Epoch 0	Acc@1 100.000 (81.652)	Acc@5 100.000 (95.631)	Mem 269MB
[2024-11-10 17:58:49 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [28750/50000]	Time 0.032 (0.033)	Loss 0.1665 (0.8359)	Epoch 0	Acc@1 100.000 (81.667)	Acc@5 100.000 (95.631)	Mem 269MB
[2024-11-10 17:58:50 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [28800/50000]	Time 0.032 (0.033)	Loss 2.4766 (0.8367)	Epoch 0	Acc@1 0.000 (81.650)	Acc@5 100.000 (95.625)	Mem 269MB
[2024-11-10 17:58:52 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [28850/50000]	Time 0.031 (0.033)	Loss 0.3958 (0.8367)	Epoch 0	Acc@1 100.000 (81.654)	Acc@5 100.000 (95.615)	Mem 269MB
[2024-11-10 17:58:53 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [28900/50000]	Time 0.031 (0.033)	Loss 1.1504 (0.8366)	Epoch 0	Acc@1 0.000 (81.655)	Acc@5 100.000 (95.613)	Mem 269MB
[2024-11-10 17:58:55 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [28950/50000]	Time 0.032 (0.033)	Loss 0.2163 (0.8367)	Epoch 0	Acc@1 100.000 (81.652)	Acc@5 100.000 (95.613)	Mem 269MB
[2024-11-10 17:58:57 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [29000/50000]	Time 0.032 (0.033)	Loss 1.7461 (0.8364)	Epoch 0	Acc@1 0.000 (81.652)	Acc@5 100.000 (95.621)	Mem 269MB
[2024-11-10 17:58:58 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [29050/50000]	Time 0.031 (0.033)	Loss 0.2262 (0.8364)	Epoch 0	Acc@1 100.000 (81.656)	Acc@5 100.000 (95.625)	Mem 269MB
[2024-11-10 17:59:00 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [29100/50000]	Time 0.031 (0.033)	Loss 0.1199 (0.8364)	Epoch 0	Acc@1 100.000 (81.657)	Acc@5 100.000 (95.626)	Mem 269MB
[2024-11-10 17:59:01 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [29150/50000]	Time 0.031 (0.033)	Loss 0.1136 (0.8362)	Epoch 0	Acc@1 100.000 (81.668)	Acc@5 100.000 (95.630)	Mem 269MB
[2024-11-10 17:59:03 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [29200/50000]	Time 0.031 (0.033)	Loss 0.2056 (0.8365)	Epoch 0	Acc@1 100.000 (81.665)	Acc@5 100.000 (95.620)	Mem 269MB
[2024-11-10 17:59:05 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [29250/50000]	Time 0.030 (0.033)	Loss 3.5020 (0.8368)	Epoch 0	Acc@1 0.000 (81.662)	Acc@5 100.000 (95.624)	Mem 269MB
[2024-11-10 17:59:06 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [29300/50000]	Time 0.031 (0.033)	Loss 2.6621 (0.8370)	Epoch 0	Acc@1 100.000 (81.670)	Acc@5 100.000 (95.618)	Mem 269MB
[2024-11-10 17:59:08 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [29350/50000]	Time 0.031 (0.033)	Loss 0.1113 (0.8374)	Epoch 0	Acc@1 100.000 (81.657)	Acc@5 100.000 (95.608)	Mem 269MB
[2024-11-10 17:59:09 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [29400/50000]	Time 0.031 (0.033)	Loss 0.1292 (0.8371)	Epoch 0	Acc@1 100.000 (81.667)	Acc@5 100.000 (95.609)	Mem 269MB
[2024-11-10 17:59:11 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [29450/50000]	Time 0.031 (0.033)	Loss 0.1132 (0.8366)	Epoch 0	Acc@1 100.000 (81.681)	Acc@5 100.000 (95.616)	Mem 269MB
[2024-11-10 17:59:12 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [29500/50000]	Time 0.031 (0.033)	Loss 0.0883 (0.8370)	Epoch 0	Acc@1 100.000 (81.668)	Acc@5 100.000 (95.614)	Mem 269MB
[2024-11-10 17:59:14 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [29550/50000]	Time 0.031 (0.033)	Loss 0.1896 (0.8370)	Epoch 0	Acc@1 100.000 (81.676)	Acc@5 100.000 (95.611)	Mem 269MB
[2024-11-10 17:59:15 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [29600/50000]	Time 0.031 (0.033)	Loss 0.0939 (0.8366)	Epoch 0	Acc@1 100.000 (81.690)	Acc@5 100.000 (95.615)	Mem 269MB
[2024-11-10 17:59:17 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [29650/50000]	Time 0.030 (0.033)	Loss 0.1187 (0.8362)	Epoch 0	Acc@1 100.000 (81.704)	Acc@5 100.000 (95.612)	Mem 269MB
[2024-11-10 17:59:18 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [29700/50000]	Time 0.031 (0.033)	Loss 4.0078 (0.8359)	Epoch 0	Acc@1 0.000 (81.714)	Acc@5 100.000 (95.613)	Mem 269MB
[2024-11-10 17:59:20 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [29750/50000]	Time 0.031 (0.033)	Loss 0.2052 (0.8356)	Epoch 0	Acc@1 100.000 (81.718)	Acc@5 100.000 (95.614)	Mem 269MB
[2024-11-10 17:59:22 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [29800/50000]	Time 0.031 (0.033)	Loss 3.4473 (0.8357)	Epoch 0	Acc@1 0.000 (81.715)	Acc@5 100.000 (95.614)	Mem 269MB
[2024-11-10 17:59:23 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [29850/50000]	Time 0.031 (0.033)	Loss 1.2061 (0.8359)	Epoch 0	Acc@1 100.000 (81.706)	Acc@5 100.000 (95.608)	Mem 269MB
[2024-11-10 17:59:25 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [29900/50000]	Time 0.031 (0.033)	Loss 0.7524 (0.8360)	Epoch 0	Acc@1 100.000 (81.703)	Acc@5 100.000 (95.612)	Mem 269MB
[2024-11-10 17:59:26 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [29950/50000]	Time 0.031 (0.033)	Loss 2.3164 (0.8366)	Epoch 0	Acc@1 0.000 (81.693)	Acc@5 100.000 (95.609)	Mem 269MB
[2024-11-10 17:59:28 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [30000/50000]	Time 0.031 (0.033)	Loss 0.1365 (0.8360)	Epoch 0	Acc@1 100.000 (81.707)	Acc@5 100.000 (95.617)	Mem 269MB
[2024-11-10 17:59:29 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [30050/50000]	Time 0.031 (0.033)	Loss 2.2637 (0.8359)	Epoch 0	Acc@1 0.000 (81.708)	Acc@5 100.000 (95.614)	Mem 269MB
[2024-11-10 17:59:31 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [30100/50000]	Time 0.031 (0.033)	Loss 4.7695 (0.8356)	Epoch 0	Acc@1 0.000 (81.718)	Acc@5 100.000 (95.618)	Mem 269MB
[2024-11-10 17:59:32 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [30150/50000]	Time 0.031 (0.033)	Loss 0.1934 (0.8358)	Epoch 0	Acc@1 100.000 (81.719)	Acc@5 100.000 (95.622)	Mem 269MB
[2024-11-10 17:59:34 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [30200/50000]	Time 0.031 (0.033)	Loss 3.6855 (0.8356)	Epoch 0	Acc@1 0.000 (81.726)	Acc@5 0.000 (95.619)	Mem 269MB
[2024-11-10 17:59:36 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [30250/50000]	Time 0.030 (0.033)	Loss 0.4897 (0.8356)	Epoch 0	Acc@1 100.000 (81.720)	Acc@5 100.000 (95.620)	Mem 269MB
[2024-11-10 17:59:37 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [30300/50000]	Time 0.031 (0.033)	Loss 0.0979 (0.8357)	Epoch 0	Acc@1 100.000 (81.717)	Acc@5 100.000 (95.621)	Mem 269MB
[2024-11-10 17:59:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [30350/50000]	Time 0.031 (0.033)	Loss 0.2352 (0.8358)	Epoch 0	Acc@1 100.000 (81.711)	Acc@5 100.000 (95.621)	Mem 269MB
[2024-11-10 17:59:40 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [30400/50000]	Time 0.031 (0.033)	Loss 0.1492 (0.8359)	Epoch 0	Acc@1 100.000 (81.711)	Acc@5 100.000 (95.625)	Mem 269MB
[2024-11-10 17:59:42 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [30450/50000]	Time 0.032 (0.033)	Loss 0.0992 (0.8357)	Epoch 0	Acc@1 100.000 (81.715)	Acc@5 100.000 (95.629)	Mem 269MB
[2024-11-10 17:59:43 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [30500/50000]	Time 0.031 (0.033)	Loss 0.1155 (0.8363)	Epoch 0	Acc@1 100.000 (81.696)	Acc@5 100.000 (95.623)	Mem 269MB
[2024-11-10 17:59:45 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [30550/50000]	Time 0.031 (0.033)	Loss 0.1041 (0.8360)	Epoch 0	Acc@1 100.000 (81.706)	Acc@5 100.000 (95.624)	Mem 269MB
[2024-11-10 17:59:46 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [30600/50000]	Time 0.031 (0.033)	Loss 0.0779 (0.8362)	Epoch 0	Acc@1 100.000 (81.706)	Acc@5 100.000 (95.618)	Mem 269MB
[2024-11-10 17:59:48 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [30650/50000]	Time 0.031 (0.033)	Loss 2.5195 (0.8360)	Epoch 0	Acc@1 0.000 (81.707)	Acc@5 100.000 (95.622)	Mem 269MB
[2024-11-10 17:59:50 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [30700/50000]	Time 0.031 (0.033)	Loss 0.1248 (0.8360)	Epoch 0	Acc@1 100.000 (81.704)	Acc@5 100.000 (95.619)	Mem 269MB
[2024-11-10 17:59:51 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [30750/50000]	Time 0.031 (0.033)	Loss 0.1233 (0.8366)	Epoch 0	Acc@1 100.000 (81.692)	Acc@5 100.000 (95.613)	Mem 269MB
[2024-11-10 17:59:53 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [30800/50000]	Time 0.031 (0.033)	Loss 1.2705 (0.8365)	Epoch 0	Acc@1 100.000 (81.695)	Acc@5 100.000 (95.614)	Mem 269MB
[2024-11-10 17:59:54 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [30850/50000]	Time 0.031 (0.033)	Loss 0.1149 (0.8368)	Epoch 0	Acc@1 100.000 (81.693)	Acc@5 100.000 (95.611)	Mem 269MB
[2024-11-10 17:59:56 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [30900/50000]	Time 0.031 (0.033)	Loss 1.0029 (0.8368)	Epoch 0	Acc@1 0.000 (81.677)	Acc@5 100.000 (95.612)	Mem 269MB
[2024-11-10 17:59:57 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [30950/50000]	Time 0.031 (0.033)	Loss 0.1705 (0.8366)	Epoch 0	Acc@1 100.000 (81.684)	Acc@5 100.000 (95.619)	Mem 269MB
[2024-11-10 17:59:59 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [31000/50000]	Time 0.031 (0.033)	Loss 0.0657 (0.8369)	Epoch 0	Acc@1 100.000 (81.681)	Acc@5 100.000 (95.616)	Mem 269MB
[2024-11-10 18:00:00 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [31050/50000]	Time 0.031 (0.033)	Loss 1.0010 (0.8369)	Epoch 0	Acc@1 100.000 (81.685)	Acc@5 100.000 (95.617)	Mem 269MB
[2024-11-10 18:00:02 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [31100/50000]	Time 0.030 (0.033)	Loss 0.0986 (0.8375)	Epoch 0	Acc@1 100.000 (81.663)	Acc@5 100.000 (95.614)	Mem 269MB
[2024-11-10 18:00:04 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [31150/50000]	Time 0.032 (0.033)	Loss 0.2324 (0.8370)	Epoch 0	Acc@1 100.000 (81.670)	Acc@5 100.000 (95.618)	Mem 269MB
[2024-11-10 18:00:05 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [31200/50000]	Time 0.031 (0.033)	Loss 0.1228 (0.8372)	Epoch 0	Acc@1 100.000 (81.661)	Acc@5 100.000 (95.616)	Mem 269MB
[2024-11-10 18:00:07 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [31250/50000]	Time 0.034 (0.033)	Loss 0.1041 (0.8373)	Epoch 0	Acc@1 100.000 (81.649)	Acc@5 100.000 (95.619)	Mem 269MB
[2024-11-10 18:00:08 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [31300/50000]	Time 0.031 (0.033)	Loss 0.1320 (0.8374)	Epoch 0	Acc@1 100.000 (81.646)	Acc@5 100.000 (95.617)	Mem 269MB
[2024-11-10 18:00:10 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [31350/50000]	Time 0.031 (0.033)	Loss 0.1643 (0.8373)	Epoch 0	Acc@1 100.000 (81.643)	Acc@5 100.000 (95.617)	Mem 269MB
[2024-11-10 18:00:11 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [31400/50000]	Time 0.031 (0.033)	Loss 2.0391 (0.8376)	Epoch 0	Acc@1 0.000 (81.634)	Acc@5 100.000 (95.615)	Mem 269MB
[2024-11-10 18:00:13 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [31450/50000]	Time 0.032 (0.033)	Loss 0.1387 (0.8372)	Epoch 0	Acc@1 100.000 (81.638)	Acc@5 100.000 (95.619)	Mem 269MB
[2024-11-10 18:00:14 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [31500/50000]	Time 0.031 (0.033)	Loss 0.1644 (0.8366)	Epoch 0	Acc@1 100.000 (81.655)	Acc@5 100.000 (95.626)	Mem 269MB
[2024-11-10 18:00:16 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [31550/50000]	Time 0.032 (0.033)	Loss 0.0442 (0.8366)	Epoch 0	Acc@1 100.000 (81.658)	Acc@5 100.000 (95.623)	Mem 269MB
[2024-11-10 18:00:18 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [31600/50000]	Time 0.031 (0.033)	Loss 0.1071 (0.8364)	Epoch 0	Acc@1 100.000 (81.665)	Acc@5 100.000 (95.624)	Mem 269MB
[2024-11-10 18:00:19 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [31650/50000]	Time 0.031 (0.033)	Loss 0.0802 (0.8361)	Epoch 0	Acc@1 100.000 (81.672)	Acc@5 100.000 (95.624)	Mem 269MB
[2024-11-10 18:00:21 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [31700/50000]	Time 0.031 (0.033)	Loss 1.2236 (0.8362)	Epoch 0	Acc@1 0.000 (81.669)	Acc@5 100.000 (95.625)	Mem 269MB
[2024-11-10 18:00:22 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [31750/50000]	Time 0.031 (0.033)	Loss 0.0959 (0.8356)	Epoch 0	Acc@1 100.000 (81.686)	Acc@5 100.000 (95.632)	Mem 269MB
[2024-11-10 18:00:24 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [31800/50000]	Time 0.031 (0.033)	Loss 0.1010 (0.8348)	Epoch 0	Acc@1 100.000 (81.705)	Acc@5 100.000 (95.639)	Mem 269MB
[2024-11-10 18:00:25 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [31850/50000]	Time 0.031 (0.033)	Loss 0.1567 (0.8348)	Epoch 0	Acc@1 100.000 (81.699)	Acc@5 100.000 (95.642)	Mem 269MB
[2024-11-10 18:00:27 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [31900/50000]	Time 0.031 (0.033)	Loss 0.1321 (0.8353)	Epoch 0	Acc@1 100.000 (81.678)	Acc@5 100.000 (95.640)	Mem 269MB
[2024-11-10 18:00:28 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [31950/50000]	Time 0.034 (0.033)	Loss 0.1204 (0.8356)	Epoch 0	Acc@1 100.000 (81.663)	Acc@5 100.000 (95.643)	Mem 269MB
[2024-11-10 18:00:30 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [32000/50000]	Time 0.031 (0.033)	Loss 0.1780 (0.8355)	Epoch 0	Acc@1 100.000 (81.669)	Acc@5 100.000 (95.647)	Mem 269MB
[2024-11-10 18:00:31 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [32050/50000]	Time 0.031 (0.033)	Loss 2.6797 (0.8357)	Epoch 0	Acc@1 0.000 (81.654)	Acc@5 100.000 (95.648)	Mem 269MB
[2024-11-10 18:00:33 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [32100/50000]	Time 0.031 (0.033)	Loss 0.1281 (0.8358)	Epoch 0	Acc@1 100.000 (81.661)	Acc@5 100.000 (95.642)	Mem 269MB
[2024-11-10 18:00:35 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [32150/50000]	Time 0.031 (0.033)	Loss 0.1175 (0.8357)	Epoch 0	Acc@1 100.000 (81.665)	Acc@5 100.000 (95.642)	Mem 269MB
[2024-11-10 18:00:36 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [32200/50000]	Time 0.031 (0.033)	Loss 1.1104 (0.8361)	Epoch 0	Acc@1 100.000 (81.656)	Acc@5 100.000 (95.637)	Mem 269MB
[2024-11-10 18:00:38 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [32250/50000]	Time 0.031 (0.033)	Loss 0.2761 (0.8362)	Epoch 0	Acc@1 100.000 (81.650)	Acc@5 100.000 (95.644)	Mem 269MB
[2024-11-10 18:00:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [32300/50000]	Time 0.031 (0.033)	Loss 0.2118 (0.8358)	Epoch 0	Acc@1 100.000 (81.651)	Acc@5 100.000 (95.650)	Mem 269MB
[2024-11-10 18:00:41 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [32350/50000]	Time 0.031 (0.033)	Loss 0.4321 (0.8362)	Epoch 0	Acc@1 100.000 (81.636)	Acc@5 100.000 (95.648)	Mem 269MB
[2024-11-10 18:00:42 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [32400/50000]	Time 0.031 (0.033)	Loss 3.0645 (0.8361)	Epoch 0	Acc@1 0.000 (81.636)	Acc@5 100.000 (95.648)	Mem 269MB
[2024-11-10 18:00:44 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [32450/50000]	Time 0.031 (0.033)	Loss 0.1053 (0.8358)	Epoch 0	Acc@1 100.000 (81.646)	Acc@5 100.000 (95.652)	Mem 269MB
[2024-11-10 18:00:45 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [32500/50000]	Time 0.031 (0.033)	Loss 0.0788 (0.8356)	Epoch 0	Acc@1 100.000 (81.644)	Acc@5 100.000 (95.656)	Mem 269MB
[2024-11-10 18:00:47 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [32550/50000]	Time 0.031 (0.033)	Loss 0.4238 (0.8355)	Epoch 0	Acc@1 100.000 (81.641)	Acc@5 100.000 (95.653)	Mem 269MB
[2024-11-10 18:00:48 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [32600/50000]	Time 0.031 (0.033)	Loss 1.9814 (0.8352)	Epoch 0	Acc@1 0.000 (81.648)	Acc@5 100.000 (95.654)	Mem 269MB
[2024-11-10 18:00:50 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [32650/50000]	Time 0.031 (0.033)	Loss 0.2284 (0.8352)	Epoch 0	Acc@1 100.000 (81.651)	Acc@5 100.000 (95.654)	Mem 269MB
[2024-11-10 18:00:52 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [32700/50000]	Time 0.030 (0.033)	Loss 0.5376 (0.8351)	Epoch 0	Acc@1 100.000 (81.661)	Acc@5 100.000 (95.655)	Mem 269MB
[2024-11-10 18:00:53 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [32750/50000]	Time 0.031 (0.033)	Loss 0.1613 (0.8353)	Epoch 0	Acc@1 100.000 (81.649)	Acc@5 100.000 (95.661)	Mem 269MB
[2024-11-10 18:00:55 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [32800/50000]	Time 0.031 (0.033)	Loss 0.1826 (0.8355)	Epoch 0	Acc@1 100.000 (81.644)	Acc@5 100.000 (95.656)	Mem 269MB
[2024-11-10 18:00:56 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [32850/50000]	Time 0.031 (0.033)	Loss 0.1304 (0.8350)	Epoch 0	Acc@1 100.000 (81.663)	Acc@5 100.000 (95.659)	Mem 269MB
[2024-11-10 18:00:58 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [32900/50000]	Time 0.030 (0.033)	Loss 0.1709 (0.8351)	Epoch 0	Acc@1 100.000 (81.669)	Acc@5 100.000 (95.654)	Mem 269MB
[2024-11-10 18:00:59 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [32950/50000]	Time 0.033 (0.033)	Loss 0.9014 (0.8352)	Epoch 0	Acc@1 100.000 (81.673)	Acc@5 100.000 (95.651)	Mem 269MB
[2024-11-10 18:01:01 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [33000/50000]	Time 0.031 (0.033)	Loss 0.2283 (0.8354)	Epoch 0	Acc@1 100.000 (81.673)	Acc@5 100.000 (95.643)	Mem 269MB
[2024-11-10 18:01:02 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [33050/50000]	Time 0.031 (0.033)	Loss 2.4590 (0.8355)	Epoch 0	Acc@1 0.000 (81.659)	Acc@5 100.000 (95.649)	Mem 269MB
[2024-11-10 18:01:04 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [33100/50000]	Time 0.032 (0.033)	Loss 0.0971 (0.8353)	Epoch 0	Acc@1 100.000 (81.656)	Acc@5 100.000 (95.647)	Mem 269MB
[2024-11-10 18:01:06 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [33150/50000]	Time 0.032 (0.033)	Loss 0.3396 (0.8351)	Epoch 0	Acc@1 100.000 (81.654)	Acc@5 100.000 (95.650)	Mem 269MB
[2024-11-10 18:01:07 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [33200/50000]	Time 0.032 (0.033)	Loss 0.0854 (0.8348)	Epoch 0	Acc@1 100.000 (81.654)	Acc@5 100.000 (95.654)	Mem 269MB
[2024-11-10 18:01:09 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [33250/50000]	Time 0.032 (0.033)	Loss 0.6543 (0.8348)	Epoch 0	Acc@1 100.000 (81.661)	Acc@5 100.000 (95.645)	Mem 269MB
[2024-11-10 18:01:10 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [33300/50000]	Time 0.032 (0.033)	Loss 0.3318 (0.8350)	Epoch 0	Acc@1 100.000 (81.658)	Acc@5 100.000 (95.643)	Mem 269MB
[2024-11-10 18:01:12 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [33350/50000]	Time 0.033 (0.033)	Loss 1.9316 (0.8346)	Epoch 0	Acc@1 0.000 (81.665)	Acc@5 100.000 (95.646)	Mem 269MB
[2024-11-10 18:01:14 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [33400/50000]	Time 0.032 (0.033)	Loss 1.7129 (0.8347)	Epoch 0	Acc@1 0.000 (81.662)	Acc@5 100.000 (95.647)	Mem 269MB
[2024-11-10 18:01:15 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [33450/50000]	Time 0.033 (0.033)	Loss 5.1953 (0.8346)	Epoch 0	Acc@1 0.000 (81.669)	Acc@5 0.000 (95.644)	Mem 269MB
[2024-11-10 18:01:17 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [33500/50000]	Time 0.032 (0.033)	Loss 1.1445 (0.8342)	Epoch 0	Acc@1 0.000 (81.678)	Acc@5 100.000 (95.648)	Mem 269MB
[2024-11-10 18:01:18 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [33550/50000]	Time 0.032 (0.033)	Loss 0.1443 (0.8342)	Epoch 0	Acc@1 100.000 (81.676)	Acc@5 100.000 (95.645)	Mem 269MB
[2024-11-10 18:01:20 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [33600/50000]	Time 0.032 (0.033)	Loss 0.1006 (0.8350)	Epoch 0	Acc@1 100.000 (81.661)	Acc@5 100.000 (95.631)	Mem 269MB
[2024-11-10 18:01:22 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [33650/50000]	Time 0.032 (0.033)	Loss 0.1143 (0.8351)	Epoch 0	Acc@1 100.000 (81.656)	Acc@5 100.000 (95.632)	Mem 269MB
[2024-11-10 18:01:23 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [33700/50000]	Time 0.033 (0.033)	Loss 0.0982 (0.8353)	Epoch 0	Acc@1 100.000 (81.659)	Acc@5 100.000 (95.623)	Mem 269MB
[2024-11-10 18:01:25 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [33750/50000]	Time 0.032 (0.033)	Loss 0.0634 (0.8350)	Epoch 0	Acc@1 100.000 (81.672)	Acc@5 100.000 (95.624)	Mem 269MB
[2024-11-10 18:01:27 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [33800/50000]	Time 0.032 (0.033)	Loss 4.9688 (0.8350)	Epoch 0	Acc@1 0.000 (81.669)	Acc@5 0.000 (95.618)	Mem 269MB
[2024-11-10 18:01:28 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [33850/50000]	Time 0.032 (0.033)	Loss 0.1240 (0.8347)	Epoch 0	Acc@1 100.000 (81.673)	Acc@5 100.000 (95.625)	Mem 269MB
[2024-11-10 18:01:30 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [33900/50000]	Time 0.032 (0.033)	Loss 0.5684 (0.8345)	Epoch 0	Acc@1 100.000 (81.682)	Acc@5 100.000 (95.631)	Mem 269MB
[2024-11-10 18:01:31 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [33950/50000]	Time 0.033 (0.033)	Loss 0.2988 (0.8348)	Epoch 0	Acc@1 100.000 (81.677)	Acc@5 100.000 (95.623)	Mem 269MB
[2024-11-10 18:01:33 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [34000/50000]	Time 0.032 (0.033)	Loss 0.0952 (0.8346)	Epoch 0	Acc@1 100.000 (81.677)	Acc@5 100.000 (95.627)	Mem 269MB
[2024-11-10 18:01:35 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [34050/50000]	Time 0.032 (0.033)	Loss 0.2388 (0.8348)	Epoch 0	Acc@1 100.000 (81.669)	Acc@5 100.000 (95.624)	Mem 269MB
[2024-11-10 18:01:36 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [34100/50000]	Time 0.032 (0.033)	Loss 2.8359 (0.8348)	Epoch 0	Acc@1 0.000 (81.663)	Acc@5 100.000 (95.622)	Mem 269MB
[2024-11-10 18:01:38 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [34150/50000]	Time 0.032 (0.033)	Loss 0.1500 (0.8351)	Epoch 0	Acc@1 100.000 (81.664)	Acc@5 100.000 (95.617)	Mem 269MB
[2024-11-10 18:01:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [34200/50000]	Time 0.033 (0.033)	Loss 0.2216 (0.8353)	Epoch 0	Acc@1 100.000 (81.656)	Acc@5 100.000 (95.614)	Mem 269MB
[2024-11-10 18:01:41 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [34250/50000]	Time 0.032 (0.033)	Loss 0.0466 (0.8351)	Epoch 0	Acc@1 100.000 (81.659)	Acc@5 100.000 (95.618)	Mem 269MB
[2024-11-10 18:01:43 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [34300/50000]	Time 0.033 (0.033)	Loss 2.1934 (0.8353)	Epoch 0	Acc@1 0.000 (81.651)	Acc@5 100.000 (95.615)	Mem 269MB
[2024-11-10 18:01:44 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [34350/50000]	Time 0.032 (0.033)	Loss 0.2302 (0.8356)	Epoch 0	Acc@1 100.000 (81.645)	Acc@5 100.000 (95.613)	Mem 269MB
[2024-11-10 18:01:46 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [34400/50000]	Time 0.032 (0.033)	Loss 0.2321 (0.8357)	Epoch 0	Acc@1 100.000 (81.649)	Acc@5 100.000 (95.616)	Mem 269MB
[2024-11-10 18:01:47 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [34450/50000]	Time 0.033 (0.033)	Loss 0.1786 (0.8363)	Epoch 0	Acc@1 100.000 (81.649)	Acc@5 100.000 (95.611)	Mem 269MB
[2024-11-10 18:01:49 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [34500/50000]	Time 0.032 (0.033)	Loss 0.3135 (0.8361)	Epoch 0	Acc@1 100.000 (81.647)	Acc@5 100.000 (95.612)	Mem 269MB
[2024-11-10 18:01:51 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [34550/50000]	Time 0.032 (0.033)	Loss 0.1572 (0.8361)	Epoch 0	Acc@1 100.000 (81.645)	Acc@5 100.000 (95.612)	Mem 269MB
[2024-11-10 18:01:52 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [34600/50000]	Time 0.032 (0.033)	Loss 0.7432 (0.8355)	Epoch 0	Acc@1 100.000 (81.659)	Acc@5 100.000 (95.616)	Mem 269MB
[2024-11-10 18:01:54 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [34650/50000]	Time 0.032 (0.033)	Loss 0.0787 (0.8353)	Epoch 0	Acc@1 100.000 (81.669)	Acc@5 100.000 (95.616)	Mem 269MB
[2024-11-10 18:01:55 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [34700/50000]	Time 0.032 (0.033)	Loss 6.2500 (0.8354)	Epoch 0	Acc@1 0.000 (81.660)	Acc@5 100.000 (95.614)	Mem 269MB
[2024-11-10 18:01:57 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [34750/50000]	Time 0.032 (0.033)	Loss 0.1875 (0.8358)	Epoch 0	Acc@1 100.000 (81.652)	Acc@5 100.000 (95.612)	Mem 269MB
[2024-11-10 18:01:59 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [34800/50000]	Time 0.033 (0.033)	Loss 3.5059 (0.8360)	Epoch 0	Acc@1 0.000 (81.650)	Acc@5 100.000 (95.615)	Mem 269MB
[2024-11-10 18:02:00 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [34850/50000]	Time 0.033 (0.033)	Loss 0.1643 (0.8356)	Epoch 0	Acc@1 100.000 (81.659)	Acc@5 100.000 (95.618)	Mem 269MB
[2024-11-10 18:02:02 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [34900/50000]	Time 0.031 (0.033)	Loss 0.1018 (0.8361)	Epoch 0	Acc@1 100.000 (81.648)	Acc@5 100.000 (95.613)	Mem 269MB
[2024-11-10 18:02:03 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [34950/50000]	Time 0.031 (0.033)	Loss 0.2235 (0.8361)	Epoch 0	Acc@1 100.000 (81.649)	Acc@5 100.000 (95.614)	Mem 269MB
[2024-11-10 18:02:05 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [35000/50000]	Time 0.031 (0.033)	Loss 0.1693 (0.8368)	Epoch 0	Acc@1 100.000 (81.641)	Acc@5 100.000 (95.612)	Mem 269MB
[2024-11-10 18:02:06 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [35050/50000]	Time 0.031 (0.033)	Loss 2.6523 (0.8370)	Epoch 0	Acc@1 0.000 (81.627)	Acc@5 100.000 (95.615)	Mem 269MB
[2024-11-10 18:02:08 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [35100/50000]	Time 0.031 (0.033)	Loss 0.2979 (0.8369)	Epoch 0	Acc@1 100.000 (81.622)	Acc@5 100.000 (95.621)	Mem 269MB
[2024-11-10 18:02:10 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [35150/50000]	Time 0.031 (0.033)	Loss 0.1043 (0.8367)	Epoch 0	Acc@1 100.000 (81.622)	Acc@5 100.000 (95.627)	Mem 269MB
[2024-11-10 18:02:11 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [35200/50000]	Time 0.031 (0.033)	Loss 0.1512 (0.8367)	Epoch 0	Acc@1 100.000 (81.620)	Acc@5 100.000 (95.628)	Mem 269MB
[2024-11-10 18:02:13 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [35250/50000]	Time 0.031 (0.033)	Loss 0.4209 (0.8367)	Epoch 0	Acc@1 100.000 (81.615)	Acc@5 100.000 (95.628)	Mem 269MB
[2024-11-10 18:02:14 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [35300/50000]	Time 0.031 (0.033)	Loss 0.1317 (0.8371)	Epoch 0	Acc@1 100.000 (81.615)	Acc@5 100.000 (95.621)	Mem 269MB
[2024-11-10 18:02:16 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [35350/50000]	Time 0.031 (0.033)	Loss 0.1481 (0.8372)	Epoch 0	Acc@1 100.000 (81.616)	Acc@5 100.000 (95.618)	Mem 269MB
[2024-11-10 18:02:17 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [35400/50000]	Time 0.031 (0.033)	Loss 0.5728 (0.8377)	Epoch 0	Acc@1 100.000 (81.605)	Acc@5 100.000 (95.619)	Mem 269MB
[2024-11-10 18:02:19 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [35450/50000]	Time 0.031 (0.033)	Loss 2.3359 (0.8380)	Epoch 0	Acc@1 0.000 (81.591)	Acc@5 100.000 (95.622)	Mem 269MB
[2024-11-10 18:02:20 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [35500/50000]	Time 0.031 (0.033)	Loss 0.3157 (0.8380)	Epoch 0	Acc@1 100.000 (81.595)	Acc@5 100.000 (95.625)	Mem 269MB
[2024-11-10 18:02:22 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [35550/50000]	Time 0.031 (0.033)	Loss 0.0725 (0.8382)	Epoch 0	Acc@1 100.000 (81.593)	Acc@5 100.000 (95.626)	Mem 269MB
[2024-11-10 18:02:24 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [35600/50000]	Time 0.031 (0.033)	Loss 0.1641 (0.8379)	Epoch 0	Acc@1 100.000 (81.604)	Acc@5 100.000 (95.627)	Mem 269MB
[2024-11-10 18:02:25 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [35650/50000]	Time 0.031 (0.033)	Loss 0.2842 (0.8387)	Epoch 0	Acc@1 100.000 (81.597)	Acc@5 100.000 (95.624)	Mem 269MB
[2024-11-10 18:02:27 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [35700/50000]	Time 0.031 (0.033)	Loss 0.1044 (0.8388)	Epoch 0	Acc@1 100.000 (81.594)	Acc@5 100.000 (95.625)	Mem 269MB
[2024-11-10 18:02:28 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [35750/50000]	Time 0.031 (0.033)	Loss 0.2468 (0.8386)	Epoch 0	Acc@1 100.000 (81.601)	Acc@5 100.000 (95.623)	Mem 269MB
[2024-11-10 18:02:30 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [35800/50000]	Time 0.031 (0.033)	Loss 0.1322 (0.8387)	Epoch 0	Acc@1 100.000 (81.598)	Acc@5 100.000 (95.617)	Mem 269MB
[2024-11-10 18:02:31 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [35850/50000]	Time 0.031 (0.033)	Loss 0.2068 (0.8387)	Epoch 0	Acc@1 100.000 (81.596)	Acc@5 100.000 (95.618)	Mem 269MB
[2024-11-10 18:02:33 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [35900/50000]	Time 0.031 (0.033)	Loss 0.0731 (0.8389)	Epoch 0	Acc@1 100.000 (81.588)	Acc@5 100.000 (95.616)	Mem 269MB
[2024-11-10 18:02:34 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [35950/50000]	Time 0.031 (0.033)	Loss 0.0828 (0.8391)	Epoch 0	Acc@1 100.000 (81.580)	Acc@5 100.000 (95.616)	Mem 269MB
[2024-11-10 18:02:36 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [36000/50000]	Time 0.031 (0.033)	Loss 2.0117 (0.8392)	Epoch 0	Acc@1 0.000 (81.581)	Acc@5 100.000 (95.611)	Mem 269MB
[2024-11-10 18:02:37 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [36050/50000]	Time 0.031 (0.033)	Loss 0.5386 (0.8390)	Epoch 0	Acc@1 100.000 (81.584)	Acc@5 100.000 (95.617)	Mem 269MB
[2024-11-10 18:02:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [36100/50000]	Time 0.031 (0.033)	Loss 0.3052 (0.8391)	Epoch 0	Acc@1 100.000 (81.577)	Acc@5 100.000 (95.618)	Mem 269MB
[2024-11-10 18:02:41 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [36150/50000]	Time 0.031 (0.033)	Loss 3.7637 (0.8392)	Epoch 0	Acc@1 0.000 (81.569)	Acc@5 100.000 (95.621)	Mem 269MB
[2024-11-10 18:02:42 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [36200/50000]	Time 0.031 (0.033)	Loss 0.2302 (0.8392)	Epoch 0	Acc@1 100.000 (81.570)	Acc@5 100.000 (95.622)	Mem 269MB
[2024-11-10 18:02:44 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [36250/50000]	Time 0.031 (0.033)	Loss 3.6465 (0.8398)	Epoch 0	Acc@1 0.000 (81.556)	Acc@5 100.000 (95.617)	Mem 269MB
[2024-11-10 18:02:45 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [36300/50000]	Time 0.031 (0.033)	Loss 0.1694 (0.8401)	Epoch 0	Acc@1 100.000 (81.538)	Acc@5 100.000 (95.617)	Mem 269MB
[2024-11-10 18:02:47 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [36350/50000]	Time 0.031 (0.033)	Loss 0.1616 (0.8398)	Epoch 0	Acc@1 100.000 (81.547)	Acc@5 100.000 (95.618)	Mem 269MB
[2024-11-10 18:02:48 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [36400/50000]	Time 0.031 (0.033)	Loss 2.0254 (0.8395)	Epoch 0	Acc@1 0.000 (81.547)	Acc@5 100.000 (95.624)	Mem 269MB
[2024-11-10 18:02:50 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [36450/50000]	Time 0.031 (0.033)	Loss 0.3135 (0.8390)	Epoch 0	Acc@1 100.000 (81.556)	Acc@5 100.000 (95.624)	Mem 269MB
[2024-11-10 18:02:51 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [36500/50000]	Time 0.031 (0.033)	Loss 0.0863 (0.8389)	Epoch 0	Acc@1 100.000 (81.548)	Acc@5 100.000 (95.628)	Mem 269MB
[2024-11-10 18:02:53 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [36550/50000]	Time 0.033 (0.033)	Loss 0.7900 (0.8394)	Epoch 0	Acc@1 100.000 (81.544)	Acc@5 100.000 (95.625)	Mem 269MB
[2024-11-10 18:02:55 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [36600/50000]	Time 0.031 (0.033)	Loss 0.1853 (0.8390)	Epoch 0	Acc@1 100.000 (81.547)	Acc@5 100.000 (95.629)	Mem 269MB
[2024-11-10 18:02:56 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [36650/50000]	Time 0.031 (0.033)	Loss 0.6416 (0.8388)	Epoch 0	Acc@1 100.000 (81.558)	Acc@5 100.000 (95.626)	Mem 269MB
[2024-11-10 18:02:58 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [36700/50000]	Time 0.031 (0.033)	Loss 1.4951 (0.8390)	Epoch 0	Acc@1 0.000 (81.551)	Acc@5 100.000 (95.624)	Mem 269MB
[2024-11-10 18:02:59 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [36750/50000]	Time 0.031 (0.033)	Loss 0.1154 (0.8389)	Epoch 0	Acc@1 100.000 (81.557)	Acc@5 100.000 (95.627)	Mem 269MB
[2024-11-10 18:03:01 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [36800/50000]	Time 0.031 (0.033)	Loss 0.8110 (0.8385)	Epoch 0	Acc@1 100.000 (81.563)	Acc@5 100.000 (95.631)	Mem 269MB
[2024-11-10 18:03:02 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [36850/50000]	Time 0.031 (0.033)	Loss 4.7734 (0.8384)	Epoch 0	Acc@1 0.000 (81.569)	Acc@5 100.000 (95.631)	Mem 269MB
[2024-11-10 18:03:04 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [36900/50000]	Time 0.031 (0.033)	Loss 0.1836 (0.8382)	Epoch 0	Acc@1 100.000 (81.572)	Acc@5 100.000 (95.634)	Mem 269MB
[2024-11-10 18:03:05 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [36950/50000]	Time 0.031 (0.033)	Loss 0.1443 (0.8387)	Epoch 0	Acc@1 100.000 (81.562)	Acc@5 100.000 (95.632)	Mem 269MB
[2024-11-10 18:03:07 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [37000/50000]	Time 0.031 (0.033)	Loss 0.3730 (0.8386)	Epoch 0	Acc@1 100.000 (81.552)	Acc@5 100.000 (95.635)	Mem 269MB
[2024-11-10 18:03:08 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [37050/50000]	Time 0.031 (0.033)	Loss 5.3320 (0.8387)	Epoch 0	Acc@1 0.000 (81.555)	Acc@5 0.000 (95.630)	Mem 269MB
[2024-11-10 18:03:10 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [37100/50000]	Time 0.031 (0.033)	Loss 0.1183 (0.8383)	Epoch 0	Acc@1 100.000 (81.556)	Acc@5 100.000 (95.636)	Mem 269MB
[2024-11-10 18:03:12 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [37150/50000]	Time 0.031 (0.033)	Loss 0.0937 (0.8387)	Epoch 0	Acc@1 100.000 (81.554)	Acc@5 100.000 (95.626)	Mem 269MB
[2024-11-10 18:03:13 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [37200/50000]	Time 0.029 (0.033)	Loss 0.2961 (0.8384)	Epoch 0	Acc@1 100.000 (81.560)	Acc@5 100.000 (95.632)	Mem 269MB
[2024-11-10 18:03:15 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [37250/50000]	Time 0.031 (0.033)	Loss 0.0831 (0.8383)	Epoch 0	Acc@1 100.000 (81.558)	Acc@5 100.000 (95.630)	Mem 269MB
[2024-11-10 18:03:16 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [37300/50000]	Time 0.034 (0.033)	Loss 2.4082 (0.8386)	Epoch 0	Acc@1 0.000 (81.550)	Acc@5 100.000 (95.630)	Mem 269MB
[2024-11-10 18:03:18 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [37350/50000]	Time 0.032 (0.033)	Loss 0.0682 (0.8380)	Epoch 0	Acc@1 100.000 (81.564)	Acc@5 100.000 (95.636)	Mem 269MB
[2024-11-10 18:03:19 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [37400/50000]	Time 0.032 (0.033)	Loss 0.1726 (0.8381)	Epoch 0	Acc@1 100.000 (81.562)	Acc@5 100.000 (95.631)	Mem 269MB
[2024-11-10 18:03:21 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [37450/50000]	Time 0.032 (0.033)	Loss 0.2078 (0.8379)	Epoch 0	Acc@1 100.000 (81.568)	Acc@5 100.000 (95.634)	Mem 269MB
[2024-11-10 18:03:23 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [37500/50000]	Time 0.032 (0.033)	Loss 0.1030 (0.8382)	Epoch 0	Acc@1 100.000 (81.558)	Acc@5 100.000 (95.637)	Mem 269MB
[2024-11-10 18:03:24 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [37550/50000]	Time 0.032 (0.033)	Loss 2.9785 (0.8378)	Epoch 0	Acc@1 0.000 (81.572)	Acc@5 100.000 (95.643)	Mem 269MB
[2024-11-10 18:03:26 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [37600/50000]	Time 0.032 (0.033)	Loss 0.0908 (0.8380)	Epoch 0	Acc@1 100.000 (81.567)	Acc@5 100.000 (95.646)	Mem 269MB
[2024-11-10 18:03:27 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [37650/50000]	Time 0.032 (0.033)	Loss 0.1196 (0.8377)	Epoch 0	Acc@1 100.000 (81.568)	Acc@5 100.000 (95.647)	Mem 269MB
[2024-11-10 18:03:29 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [37700/50000]	Time 0.032 (0.033)	Loss 0.2253 (0.8381)	Epoch 0	Acc@1 100.000 (81.558)	Acc@5 100.000 (95.645)	Mem 269MB
[2024-11-10 18:03:31 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [37750/50000]	Time 0.032 (0.033)	Loss 0.0734 (0.8383)	Epoch 0	Acc@1 100.000 (81.558)	Acc@5 100.000 (95.645)	Mem 269MB
[2024-11-10 18:03:32 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [37800/50000]	Time 0.032 (0.033)	Loss 0.1541 (0.8382)	Epoch 0	Acc@1 100.000 (81.561)	Acc@5 100.000 (95.648)	Mem 269MB
[2024-11-10 18:03:34 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [37850/50000]	Time 0.032 (0.033)	Loss 2.2090 (0.8377)	Epoch 0	Acc@1 0.000 (81.575)	Acc@5 100.000 (95.651)	Mem 269MB
[2024-11-10 18:03:35 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [37900/50000]	Time 0.032 (0.033)	Loss 6.1445 (0.8377)	Epoch 0	Acc@1 0.000 (81.578)	Acc@5 0.000 (95.649)	Mem 269MB
[2024-11-10 18:03:37 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [37950/50000]	Time 0.033 (0.033)	Loss 0.1355 (0.8379)	Epoch 0	Acc@1 100.000 (81.574)	Acc@5 100.000 (95.647)	Mem 269MB
[2024-11-10 18:03:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [38000/50000]	Time 0.032 (0.033)	Loss 0.1517 (0.8382)	Epoch 0	Acc@1 100.000 (81.572)	Acc@5 100.000 (95.640)	Mem 269MB
[2024-11-10 18:03:40 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [38050/50000]	Time 0.032 (0.033)	Loss 0.2169 (0.8386)	Epoch 0	Acc@1 100.000 (81.567)	Acc@5 100.000 (95.635)	Mem 269MB
[2024-11-10 18:03:42 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [38100/50000]	Time 0.032 (0.033)	Loss 0.2800 (0.8382)	Epoch 0	Acc@1 100.000 (81.575)	Acc@5 100.000 (95.638)	Mem 269MB
[2024-11-10 18:03:43 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [38150/50000]	Time 0.032 (0.033)	Loss 0.1278 (0.8381)	Epoch 0	Acc@1 100.000 (81.576)	Acc@5 100.000 (95.638)	Mem 269MB
[2024-11-10 18:03:45 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [38200/50000]	Time 0.032 (0.033)	Loss 0.1240 (0.8384)	Epoch 0	Acc@1 100.000 (81.576)	Acc@5 100.000 (95.639)	Mem 269MB
[2024-11-10 18:03:47 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [38250/50000]	Time 0.031 (0.033)	Loss 0.2588 (0.8382)	Epoch 0	Acc@1 100.000 (81.580)	Acc@5 100.000 (95.639)	Mem 269MB
[2024-11-10 18:03:48 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [38300/50000]	Time 0.033 (0.033)	Loss 0.1698 (0.8379)	Epoch 0	Acc@1 100.000 (81.583)	Acc@5 100.000 (95.642)	Mem 269MB
[2024-11-10 18:03:50 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [38350/50000]	Time 0.032 (0.033)	Loss 0.1193 (0.8380)	Epoch 0	Acc@1 100.000 (81.583)	Acc@5 100.000 (95.640)	Mem 269MB
[2024-11-10 18:03:51 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [38400/50000]	Time 0.032 (0.033)	Loss 0.1390 (0.8382)	Epoch 0	Acc@1 100.000 (81.581)	Acc@5 100.000 (95.638)	Mem 269MB
[2024-11-10 18:03:53 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [38450/50000]	Time 0.032 (0.033)	Loss 1.9980 (0.8379)	Epoch 0	Acc@1 0.000 (81.590)	Acc@5 100.000 (95.639)	Mem 269MB
[2024-11-10 18:03:55 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [38500/50000]	Time 0.032 (0.033)	Loss 0.1892 (0.8377)	Epoch 0	Acc@1 100.000 (81.593)	Acc@5 100.000 (95.642)	Mem 269MB
[2024-11-10 18:03:56 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [38550/50000]	Time 0.032 (0.033)	Loss 0.1018 (0.8379)	Epoch 0	Acc@1 100.000 (81.588)	Acc@5 100.000 (95.634)	Mem 269MB
[2024-11-10 18:03:58 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [38600/50000]	Time 0.032 (0.033)	Loss 0.1174 (0.8380)	Epoch 0	Acc@1 100.000 (81.583)	Acc@5 100.000 (95.635)	Mem 269MB
[2024-11-10 18:04:00 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [38650/50000]	Time 0.032 (0.033)	Loss 0.1195 (0.8381)	Epoch 0	Acc@1 100.000 (81.571)	Acc@5 100.000 (95.635)	Mem 269MB
[2024-11-10 18:04:01 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [38700/50000]	Time 0.032 (0.033)	Loss 0.5454 (0.8386)	Epoch 0	Acc@1 100.000 (81.564)	Acc@5 100.000 (95.631)	Mem 269MB
[2024-11-10 18:04:03 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [38750/50000]	Time 0.032 (0.033)	Loss 0.1722 (0.8386)	Epoch 0	Acc@1 100.000 (81.564)	Acc@5 100.000 (95.626)	Mem 269MB
[2024-11-10 18:04:04 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [38800/50000]	Time 0.033 (0.033)	Loss 0.8931 (0.8387)	Epoch 0	Acc@1 0.000 (81.560)	Acc@5 100.000 (95.621)	Mem 269MB
[2024-11-10 18:04:06 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [38850/50000]	Time 0.032 (0.033)	Loss 0.6084 (0.8388)	Epoch 0	Acc@1 100.000 (81.558)	Acc@5 100.000 (95.619)	Mem 269MB
[2024-11-10 18:04:08 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [38900/50000]	Time 0.032 (0.033)	Loss 0.1387 (0.8385)	Epoch 0	Acc@1 100.000 (81.561)	Acc@5 100.000 (95.625)	Mem 269MB
[2024-11-10 18:04:09 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [38950/50000]	Time 0.032 (0.033)	Loss 1.1602 (0.8384)	Epoch 0	Acc@1 100.000 (81.561)	Acc@5 100.000 (95.625)	Mem 269MB
[2024-11-10 18:04:11 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [39000/50000]	Time 0.032 (0.033)	Loss 2.6211 (0.8382)	Epoch 0	Acc@1 0.000 (81.567)	Acc@5 100.000 (95.628)	Mem 269MB
[2024-11-10 18:04:12 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [39050/50000]	Time 0.032 (0.033)	Loss 0.1619 (0.8384)	Epoch 0	Acc@1 100.000 (81.555)	Acc@5 100.000 (95.629)	Mem 269MB
[2024-11-10 18:04:14 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [39100/50000]	Time 0.032 (0.033)	Loss 2.8008 (0.8382)	Epoch 0	Acc@1 0.000 (81.550)	Acc@5 100.000 (95.634)	Mem 269MB
[2024-11-10 18:04:16 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [39150/50000]	Time 0.033 (0.033)	Loss 0.1334 (0.8380)	Epoch 0	Acc@1 100.000 (81.551)	Acc@5 100.000 (95.637)	Mem 269MB
[2024-11-10 18:04:17 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [39200/50000]	Time 0.032 (0.033)	Loss 0.2006 (0.8380)	Epoch 0	Acc@1 100.000 (81.551)	Acc@5 100.000 (95.640)	Mem 269MB
[2024-11-10 18:04:19 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [39250/50000]	Time 0.032 (0.033)	Loss 4.5039 (0.8379)	Epoch 0	Acc@1 0.000 (81.560)	Acc@5 100.000 (95.638)	Mem 269MB
[2024-11-10 18:04:20 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [39300/50000]	Time 0.032 (0.033)	Loss 1.5625 (0.8379)	Epoch 0	Acc@1 0.000 (81.563)	Acc@5 100.000 (95.636)	Mem 269MB
[2024-11-10 18:04:22 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [39350/50000]	Time 0.032 (0.033)	Loss 0.1814 (0.8380)	Epoch 0	Acc@1 100.000 (81.563)	Acc@5 100.000 (95.634)	Mem 269MB
[2024-11-10 18:04:24 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [39400/50000]	Time 0.034 (0.033)	Loss 0.1387 (0.8381)	Epoch 0	Acc@1 100.000 (81.559)	Acc@5 100.000 (95.632)	Mem 269MB
[2024-11-10 18:04:25 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [39450/50000]	Time 0.032 (0.033)	Loss 0.2108 (0.8385)	Epoch 0	Acc@1 100.000 (81.547)	Acc@5 100.000 (95.627)	Mem 269MB
[2024-11-10 18:04:27 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [39500/50000]	Time 0.035 (0.033)	Loss 0.1157 (0.8391)	Epoch 0	Acc@1 100.000 (81.535)	Acc@5 100.000 (95.618)	Mem 269MB
[2024-11-10 18:04:28 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [39550/50000]	Time 0.032 (0.033)	Loss 0.0976 (0.8389)	Epoch 0	Acc@1 100.000 (81.533)	Acc@5 100.000 (95.621)	Mem 269MB
[2024-11-10 18:04:30 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [39600/50000]	Time 0.032 (0.033)	Loss 0.6982 (0.8392)	Epoch 0	Acc@1 100.000 (81.528)	Acc@5 100.000 (95.621)	Mem 269MB
[2024-11-10 18:04:32 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [39650/50000]	Time 0.032 (0.033)	Loss 0.1650 (0.8390)	Epoch 0	Acc@1 100.000 (81.534)	Acc@5 100.000 (95.619)	Mem 269MB
[2024-11-10 18:04:33 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [39700/50000]	Time 0.033 (0.033)	Loss 0.0872 (0.8395)	Epoch 0	Acc@1 100.000 (81.524)	Acc@5 100.000 (95.620)	Mem 269MB
[2024-11-10 18:04:35 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [39750/50000]	Time 0.032 (0.033)	Loss 0.0931 (0.8396)	Epoch 0	Acc@1 100.000 (81.522)	Acc@5 100.000 (95.618)	Mem 269MB
[2024-11-10 18:04:36 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [39800/50000]	Time 0.032 (0.033)	Loss 0.5801 (0.8394)	Epoch 0	Acc@1 100.000 (81.528)	Acc@5 100.000 (95.618)	Mem 269MB
[2024-11-10 18:04:38 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [39850/50000]	Time 0.031 (0.033)	Loss 0.2959 (0.8400)	Epoch 0	Acc@1 100.000 (81.511)	Acc@5 100.000 (95.616)	Mem 269MB
[2024-11-10 18:04:40 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [39900/50000]	Time 0.033 (0.033)	Loss 0.1006 (0.8406)	Epoch 0	Acc@1 100.000 (81.502)	Acc@5 100.000 (95.612)	Mem 269MB
[2024-11-10 18:04:41 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [39950/50000]	Time 0.032 (0.033)	Loss 0.2739 (0.8405)	Epoch 0	Acc@1 100.000 (81.502)	Acc@5 100.000 (95.610)	Mem 269MB
[2024-11-10 18:04:43 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [40000/50000]	Time 0.032 (0.033)	Loss 2.4238 (0.8406)	Epoch 0	Acc@1 0.000 (81.495)	Acc@5 100.000 (95.608)	Mem 269MB
[2024-11-10 18:04:44 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [40050/50000]	Time 0.032 (0.033)	Loss 1.6924 (0.8412)	Epoch 0	Acc@1 0.000 (81.479)	Acc@5 100.000 (95.593)	Mem 269MB
[2024-11-10 18:04:46 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [40100/50000]	Time 0.032 (0.033)	Loss 0.1713 (0.8420)	Epoch 0	Acc@1 100.000 (81.464)	Acc@5 100.000 (95.579)	Mem 269MB
[2024-11-10 18:04:48 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [40150/50000]	Time 0.032 (0.033)	Loss 0.8306 (0.8420)	Epoch 0	Acc@1 100.000 (81.467)	Acc@5 100.000 (95.579)	Mem 269MB
[2024-11-10 18:04:49 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [40200/50000]	Time 0.032 (0.033)	Loss 0.1775 (0.8418)	Epoch 0	Acc@1 100.000 (81.478)	Acc@5 100.000 (95.582)	Mem 269MB
[2024-11-10 18:04:51 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [40250/50000]	Time 0.034 (0.033)	Loss 0.1440 (0.8419)	Epoch 0	Acc@1 100.000 (81.479)	Acc@5 100.000 (95.580)	Mem 269MB
[2024-11-10 18:04:52 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [40300/50000]	Time 0.032 (0.033)	Loss 0.0778 (0.8419)	Epoch 0	Acc@1 100.000 (81.472)	Acc@5 100.000 (95.578)	Mem 269MB
[2024-11-10 18:04:54 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [40350/50000]	Time 0.032 (0.033)	Loss 0.2178 (0.8419)	Epoch 0	Acc@1 100.000 (81.468)	Acc@5 100.000 (95.579)	Mem 269MB
[2024-11-10 18:04:56 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [40400/50000]	Time 0.032 (0.033)	Loss 0.1251 (0.8419)	Epoch 0	Acc@1 100.000 (81.468)	Acc@5 100.000 (95.582)	Mem 269MB
[2024-11-10 18:04:57 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [40450/50000]	Time 0.032 (0.033)	Loss 0.1334 (0.8420)	Epoch 0	Acc@1 100.000 (81.459)	Acc@5 100.000 (95.577)	Mem 269MB
[2024-11-10 18:04:59 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [40500/50000]	Time 0.032 (0.033)	Loss 0.1356 (0.8419)	Epoch 0	Acc@1 100.000 (81.460)	Acc@5 100.000 (95.578)	Mem 269MB
[2024-11-10 18:05:00 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [40550/50000]	Time 0.032 (0.033)	Loss 0.2568 (0.8418)	Epoch 0	Acc@1 100.000 (81.468)	Acc@5 100.000 (95.573)	Mem 269MB
[2024-11-10 18:05:02 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [40600/50000]	Time 0.032 (0.033)	Loss 0.1089 (0.8412)	Epoch 0	Acc@1 100.000 (81.478)	Acc@5 100.000 (95.579)	Mem 269MB
[2024-11-10 18:05:04 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [40650/50000]	Time 0.033 (0.033)	Loss 0.0739 (0.8412)	Epoch 0	Acc@1 100.000 (81.474)	Acc@5 100.000 (95.582)	Mem 269MB
[2024-11-10 18:05:05 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [40700/50000]	Time 0.032 (0.033)	Loss 0.2449 (0.8411)	Epoch 0	Acc@1 100.000 (81.475)	Acc@5 100.000 (95.585)	Mem 269MB
[2024-11-10 18:05:07 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [40750/50000]	Time 0.032 (0.033)	Loss 0.5620 (0.8413)	Epoch 0	Acc@1 100.000 (81.473)	Acc@5 100.000 (95.583)	Mem 269MB
[2024-11-10 18:05:08 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [40800/50000]	Time 0.032 (0.033)	Loss 1.4141 (0.8414)	Epoch 0	Acc@1 0.000 (81.469)	Acc@5 100.000 (95.579)	Mem 269MB
[2024-11-10 18:05:10 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [40850/50000]	Time 0.031 (0.033)	Loss 0.1470 (0.8411)	Epoch 0	Acc@1 100.000 (81.477)	Acc@5 100.000 (95.582)	Mem 269MB
[2024-11-10 18:05:11 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [40900/50000]	Time 0.032 (0.033)	Loss 0.8257 (0.8418)	Epoch 0	Acc@1 100.000 (81.472)	Acc@5 100.000 (95.572)	Mem 269MB
[2024-11-10 18:05:13 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [40950/50000]	Time 0.033 (0.033)	Loss 0.3662 (0.8418)	Epoch 0	Acc@1 100.000 (81.466)	Acc@5 100.000 (95.568)	Mem 269MB
[2024-11-10 18:05:15 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [41000/50000]	Time 0.032 (0.033)	Loss 2.0215 (0.8421)	Epoch 0	Acc@1 0.000 (81.459)	Acc@5 100.000 (95.566)	Mem 269MB
[2024-11-10 18:05:16 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [41050/50000]	Time 0.039 (0.033)	Loss 0.2115 (0.8418)	Epoch 0	Acc@1 100.000 (81.467)	Acc@5 100.000 (95.571)	Mem 269MB
[2024-11-10 18:05:18 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [41100/50000]	Time 0.032 (0.033)	Loss 0.0594 (0.8416)	Epoch 0	Acc@1 100.000 (81.475)	Acc@5 100.000 (95.567)	Mem 269MB
[2024-11-10 18:05:20 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [41150/50000]	Time 0.032 (0.033)	Loss 0.1234 (0.8420)	Epoch 0	Acc@1 100.000 (81.468)	Acc@5 100.000 (95.560)	Mem 269MB
[2024-11-10 18:05:21 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [41200/50000]	Time 0.032 (0.033)	Loss 0.0696 (0.8417)	Epoch 0	Acc@1 100.000 (81.474)	Acc@5 100.000 (95.558)	Mem 269MB
[2024-11-10 18:05:23 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [41250/50000]	Time 0.033 (0.033)	Loss 0.1873 (0.8417)	Epoch 0	Acc@1 100.000 (81.470)	Acc@5 100.000 (95.559)	Mem 269MB
[2024-11-10 18:05:24 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [41300/50000]	Time 0.032 (0.033)	Loss 0.1855 (0.8417)	Epoch 0	Acc@1 100.000 (81.465)	Acc@5 100.000 (95.559)	Mem 269MB
[2024-11-10 18:05:26 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [41350/50000]	Time 0.032 (0.033)	Loss 4.0977 (0.8419)	Epoch 0	Acc@1 0.000 (81.461)	Acc@5 0.000 (95.555)	Mem 269MB
[2024-11-10 18:05:28 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [41400/50000]	Time 0.032 (0.033)	Loss 0.1337 (0.8419)	Epoch 0	Acc@1 100.000 (81.467)	Acc@5 100.000 (95.553)	Mem 269MB
[2024-11-10 18:05:29 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [41450/50000]	Time 0.032 (0.033)	Loss 0.1534 (0.8417)	Epoch 0	Acc@1 100.000 (81.462)	Acc@5 100.000 (95.556)	Mem 269MB
[2024-11-10 18:05:31 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [41500/50000]	Time 0.032 (0.033)	Loss 0.0940 (0.8420)	Epoch 0	Acc@1 100.000 (81.463)	Acc@5 100.000 (95.554)	Mem 269MB
[2024-11-10 18:05:32 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [41550/50000]	Time 0.032 (0.033)	Loss 1.6045 (0.8422)	Epoch 0	Acc@1 0.000 (81.464)	Acc@5 100.000 (95.550)	Mem 269MB
[2024-11-10 18:05:34 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [41600/50000]	Time 0.031 (0.033)	Loss 0.0724 (0.8423)	Epoch 0	Acc@1 100.000 (81.457)	Acc@5 100.000 (95.548)	Mem 269MB
[2024-11-10 18:05:35 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [41650/50000]	Time 0.031 (0.033)	Loss 2.9297 (0.8421)	Epoch 0	Acc@1 0.000 (81.465)	Acc@5 100.000 (95.551)	Mem 269MB
[2024-11-10 18:05:37 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [41700/50000]	Time 0.032 (0.033)	Loss 0.2014 (0.8422)	Epoch 0	Acc@1 100.000 (81.461)	Acc@5 100.000 (95.552)	Mem 269MB
[2024-11-10 18:05:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [41750/50000]	Time 0.032 (0.033)	Loss 0.5005 (0.8422)	Epoch 0	Acc@1 100.000 (81.459)	Acc@5 100.000 (95.550)	Mem 269MB
[2024-11-10 18:05:40 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [41800/50000]	Time 0.032 (0.033)	Loss 0.3115 (0.8418)	Epoch 0	Acc@1 100.000 (81.467)	Acc@5 100.000 (95.553)	Mem 269MB
[2024-11-10 18:05:42 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [41850/50000]	Time 0.031 (0.033)	Loss 0.3213 (0.8416)	Epoch 0	Acc@1 100.000 (81.470)	Acc@5 100.000 (95.556)	Mem 269MB
[2024-11-10 18:05:43 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [41900/50000]	Time 0.032 (0.033)	Loss 0.0827 (0.8418)	Epoch 0	Acc@1 100.000 (81.471)	Acc@5 100.000 (95.559)	Mem 269MB
[2024-11-10 18:05:45 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [41950/50000]	Time 0.032 (0.033)	Loss 0.1378 (0.8413)	Epoch 0	Acc@1 100.000 (81.478)	Acc@5 100.000 (95.564)	Mem 269MB
[2024-11-10 18:05:47 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [42000/50000]	Time 0.032 (0.033)	Loss 2.0176 (0.8414)	Epoch 0	Acc@1 100.000 (81.481)	Acc@5 100.000 (95.562)	Mem 269MB
[2024-11-10 18:05:48 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [42050/50000]	Time 0.031 (0.033)	Loss 0.1266 (0.8409)	Epoch 0	Acc@1 100.000 (81.482)	Acc@5 100.000 (95.567)	Mem 269MB
[2024-11-10 18:05:50 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [42100/50000]	Time 0.031 (0.033)	Loss 0.2703 (0.8408)	Epoch 0	Acc@1 100.000 (81.478)	Acc@5 100.000 (95.568)	Mem 269MB
[2024-11-10 18:05:51 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [42150/50000]	Time 0.031 (0.033)	Loss 0.1138 (0.8407)	Epoch 0	Acc@1 100.000 (81.483)	Acc@5 100.000 (95.566)	Mem 269MB
[2024-11-10 18:05:53 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [42200/50000]	Time 0.034 (0.033)	Loss 0.1265 (0.8406)	Epoch 0	Acc@1 100.000 (81.484)	Acc@5 100.000 (95.569)	Mem 269MB
[2024-11-10 18:05:54 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [42250/50000]	Time 0.031 (0.033)	Loss 0.1653 (0.8411)	Epoch 0	Acc@1 100.000 (81.470)	Acc@5 100.000 (95.567)	Mem 269MB
[2024-11-10 18:05:56 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [42300/50000]	Time 0.031 (0.033)	Loss 0.0980 (0.8412)	Epoch 0	Acc@1 100.000 (81.466)	Acc@5 100.000 (95.567)	Mem 269MB
[2024-11-10 18:05:58 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [42350/50000]	Time 0.031 (0.033)	Loss 0.0651 (0.8411)	Epoch 0	Acc@1 100.000 (81.467)	Acc@5 100.000 (95.568)	Mem 269MB
[2024-11-10 18:05:59 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [42400/50000]	Time 0.031 (0.033)	Loss 0.1357 (0.8411)	Epoch 0	Acc@1 100.000 (81.463)	Acc@5 100.000 (95.566)	Mem 269MB
[2024-11-10 18:06:01 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [42450/50000]	Time 0.031 (0.033)	Loss 0.1334 (0.8412)	Epoch 0	Acc@1 100.000 (81.463)	Acc@5 100.000 (95.564)	Mem 269MB
[2024-11-10 18:06:02 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [42500/50000]	Time 0.032 (0.033)	Loss 0.1586 (0.8409)	Epoch 0	Acc@1 100.000 (81.473)	Acc@5 100.000 (95.562)	Mem 269MB
[2024-11-10 18:06:04 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [42550/50000]	Time 0.031 (0.033)	Loss 0.0760 (0.8413)	Epoch 0	Acc@1 100.000 (81.472)	Acc@5 100.000 (95.556)	Mem 269MB
[2024-11-10 18:06:05 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [42600/50000]	Time 0.031 (0.033)	Loss 0.1688 (0.8412)	Epoch 0	Acc@1 100.000 (81.470)	Acc@5 100.000 (95.559)	Mem 269MB
[2024-11-10 18:06:07 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [42650/50000]	Time 0.031 (0.033)	Loss 0.1757 (0.8414)	Epoch 0	Acc@1 100.000 (81.466)	Acc@5 100.000 (95.557)	Mem 269MB
[2024-11-10 18:06:08 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [42700/50000]	Time 0.031 (0.033)	Loss 1.4033 (0.8411)	Epoch 0	Acc@1 100.000 (81.469)	Acc@5 100.000 (95.557)	Mem 269MB
[2024-11-10 18:06:10 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [42750/50000]	Time 0.031 (0.033)	Loss 0.2954 (0.8413)	Epoch 0	Acc@1 100.000 (81.460)	Acc@5 100.000 (95.553)	Mem 269MB
[2024-11-10 18:06:12 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [42800/50000]	Time 0.031 (0.033)	Loss 0.0858 (0.8410)	Epoch 0	Acc@1 100.000 (81.465)	Acc@5 100.000 (95.554)	Mem 269MB
[2024-11-10 18:06:13 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [42850/50000]	Time 0.031 (0.033)	Loss 0.0852 (0.8410)	Epoch 0	Acc@1 100.000 (81.464)	Acc@5 100.000 (95.557)	Mem 269MB
[2024-11-10 18:06:15 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [42900/50000]	Time 0.031 (0.033)	Loss 0.1792 (0.8411)	Epoch 0	Acc@1 100.000 (81.460)	Acc@5 100.000 (95.560)	Mem 269MB
[2024-11-10 18:06:16 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [42950/50000]	Time 0.031 (0.033)	Loss 0.1812 (0.8410)	Epoch 0	Acc@1 100.000 (81.456)	Acc@5 100.000 (95.562)	Mem 269MB
[2024-11-10 18:06:18 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [43000/50000]	Time 0.031 (0.033)	Loss 0.1168 (0.8410)	Epoch 0	Acc@1 100.000 (81.459)	Acc@5 100.000 (95.561)	Mem 269MB
[2024-11-10 18:06:19 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [43050/50000]	Time 0.030 (0.033)	Loss 0.1417 (0.8412)	Epoch 0	Acc@1 100.000 (81.452)	Acc@5 100.000 (95.556)	Mem 269MB
[2024-11-10 18:06:21 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [43100/50000]	Time 0.031 (0.033)	Loss 0.1021 (0.8412)	Epoch 0	Acc@1 100.000 (81.458)	Acc@5 100.000 (95.555)	Mem 269MB
[2024-11-10 18:06:22 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [43150/50000]	Time 0.031 (0.033)	Loss 0.1180 (0.8413)	Epoch 0	Acc@1 100.000 (81.456)	Acc@5 100.000 (95.555)	Mem 269MB
[2024-11-10 18:06:24 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [43200/50000]	Time 0.031 (0.033)	Loss 1.6094 (0.8412)	Epoch 0	Acc@1 0.000 (81.456)	Acc@5 100.000 (95.556)	Mem 269MB
[2024-11-10 18:06:25 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [43250/50000]	Time 0.030 (0.033)	Loss 0.1143 (0.8410)	Epoch 0	Acc@1 100.000 (81.457)	Acc@5 100.000 (95.558)	Mem 269MB
[2024-11-10 18:06:27 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [43300/50000]	Time 0.031 (0.033)	Loss 8.7578 (0.8413)	Epoch 0	Acc@1 0.000 (81.453)	Acc@5 0.000 (95.552)	Mem 269MB
[2024-11-10 18:06:29 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [43350/50000]	Time 0.032 (0.033)	Loss 0.4460 (0.8415)	Epoch 0	Acc@1 100.000 (81.451)	Acc@5 100.000 (95.548)	Mem 269MB
[2024-11-10 18:06:30 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [43400/50000]	Time 0.030 (0.033)	Loss 0.5947 (0.8415)	Epoch 0	Acc@1 100.000 (81.457)	Acc@5 100.000 (95.548)	Mem 269MB
[2024-11-10 18:06:32 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [43450/50000]	Time 0.031 (0.033)	Loss 0.1819 (0.8414)	Epoch 0	Acc@1 100.000 (81.460)	Acc@5 100.000 (95.549)	Mem 269MB
[2024-11-10 18:06:33 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [43500/50000]	Time 0.031 (0.033)	Loss 0.7583 (0.8420)	Epoch 0	Acc@1 100.000 (81.451)	Acc@5 100.000 (95.545)	Mem 269MB
[2024-11-10 18:06:35 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [43550/50000]	Time 0.031 (0.033)	Loss 5.2695 (0.8416)	Epoch 0	Acc@1 0.000 (81.461)	Acc@5 0.000 (95.545)	Mem 269MB
[2024-11-10 18:06:36 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [43600/50000]	Time 0.031 (0.033)	Loss 0.3513 (0.8414)	Epoch 0	Acc@1 100.000 (81.464)	Acc@5 100.000 (95.551)	Mem 269MB
[2024-11-10 18:06:38 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [43650/50000]	Time 0.031 (0.033)	Loss 0.1689 (0.8415)	Epoch 0	Acc@1 100.000 (81.464)	Acc@5 100.000 (95.544)	Mem 269MB
[2024-11-10 18:06:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [43700/50000]	Time 0.030 (0.033)	Loss 0.1444 (0.8416)	Epoch 0	Acc@1 100.000 (81.465)	Acc@5 100.000 (95.540)	Mem 269MB
[2024-11-10 18:06:41 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [43750/50000]	Time 0.031 (0.033)	Loss 0.0386 (0.8411)	Epoch 0	Acc@1 100.000 (81.475)	Acc@5 100.000 (95.543)	Mem 269MB
[2024-11-10 18:06:42 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [43800/50000]	Time 0.031 (0.033)	Loss 0.1733 (0.8407)	Epoch 0	Acc@1 100.000 (81.484)	Acc@5 100.000 (95.548)	Mem 269MB
[2024-11-10 18:06:44 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [43850/50000]	Time 0.030 (0.033)	Loss 0.3501 (0.8411)	Epoch 0	Acc@1 100.000 (81.480)	Acc@5 100.000 (95.546)	Mem 269MB
[2024-11-10 18:06:45 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [43900/50000]	Time 0.031 (0.033)	Loss 0.2046 (0.8405)	Epoch 0	Acc@1 100.000 (81.492)	Acc@5 100.000 (95.551)	Mem 269MB
[2024-11-10 18:06:47 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [43950/50000]	Time 0.031 (0.033)	Loss 0.2352 (0.8404)	Epoch 0	Acc@1 100.000 (81.498)	Acc@5 100.000 (95.552)	Mem 269MB
[2024-11-10 18:06:49 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [44000/50000]	Time 0.038 (0.033)	Loss 0.2737 (0.8408)	Epoch 0	Acc@1 100.000 (81.491)	Acc@5 100.000 (95.546)	Mem 269MB
[2024-11-10 18:06:50 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [44050/50000]	Time 0.031 (0.033)	Loss 0.0778 (0.8407)	Epoch 0	Acc@1 100.000 (81.490)	Acc@5 100.000 (95.548)	Mem 269MB
[2024-11-10 18:06:52 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [44100/50000]	Time 0.032 (0.033)	Loss 0.0944 (0.8407)	Epoch 0	Acc@1 100.000 (81.495)	Acc@5 100.000 (95.544)	Mem 269MB
[2024-11-10 18:06:53 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [44150/50000]	Time 0.032 (0.033)	Loss 0.0925 (0.8405)	Epoch 0	Acc@1 100.000 (81.504)	Acc@5 100.000 (95.547)	Mem 269MB
[2024-11-10 18:06:55 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [44200/50000]	Time 0.032 (0.033)	Loss 1.1406 (0.8406)	Epoch 0	Acc@1 100.000 (81.500)	Acc@5 100.000 (95.545)	Mem 269MB
[2024-11-10 18:06:57 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [44250/50000]	Time 0.031 (0.033)	Loss 0.2369 (0.8407)	Epoch 0	Acc@1 100.000 (81.501)	Acc@5 100.000 (95.541)	Mem 269MB
[2024-11-10 18:06:58 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [44300/50000]	Time 0.031 (0.033)	Loss 2.1719 (0.8409)	Epoch 0	Acc@1 0.000 (81.499)	Acc@5 100.000 (95.540)	Mem 269MB
[2024-11-10 18:07:00 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [44350/50000]	Time 0.039 (0.033)	Loss 0.1241 (0.8410)	Epoch 0	Acc@1 100.000 (81.500)	Acc@5 100.000 (95.538)	Mem 269MB
[2024-11-10 18:07:02 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [44400/50000]	Time 0.039 (0.033)	Loss 0.0954 (0.8405)	Epoch 0	Acc@1 100.000 (81.512)	Acc@5 100.000 (95.543)	Mem 269MB
[2024-11-10 18:07:04 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [44450/50000]	Time 0.038 (0.033)	Loss 0.1204 (0.8408)	Epoch 0	Acc@1 100.000 (81.508)	Acc@5 100.000 (95.537)	Mem 269MB
[2024-11-10 18:07:06 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [44500/50000]	Time 0.039 (0.033)	Loss 0.1055 (0.8410)	Epoch 0	Acc@1 100.000 (81.504)	Acc@5 100.000 (95.530)	Mem 269MB
[2024-11-10 18:07:08 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [44550/50000]	Time 0.039 (0.033)	Loss 9.1250 (0.8412)	Epoch 0	Acc@1 0.000 (81.500)	Acc@5 0.000 (95.531)	Mem 269MB
[2024-11-10 18:07:10 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [44600/50000]	Time 0.039 (0.033)	Loss 0.0940 (0.8410)	Epoch 0	Acc@1 100.000 (81.509)	Acc@5 100.000 (95.531)	Mem 269MB
[2024-11-10 18:07:12 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [44650/50000]	Time 0.039 (0.033)	Loss 0.3621 (0.8410)	Epoch 0	Acc@1 100.000 (81.508)	Acc@5 100.000 (95.532)	Mem 269MB
[2024-11-10 18:21:59 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [44700/50000]	Time 0.039 (0.053)	Loss 0.0883 (0.8411)	Epoch 0	Acc@1 100.000 (81.508)	Acc@5 100.000 (95.535)	Mem 269MB
[2024-11-10 18:22:00 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [44750/50000]	Time 0.038 (0.053)	Loss 0.4749 (0.8412)	Epoch 0	Acc@1 100.000 (81.504)	Acc@5 100.000 (95.533)	Mem 269MB
[2024-11-10 18:22:02 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [44800/50000]	Time 0.039 (0.053)	Loss 0.1464 (0.8410)	Epoch 0	Acc@1 100.000 (81.509)	Acc@5 100.000 (95.534)	Mem 269MB
[2024-11-10 18:22:04 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [44850/50000]	Time 0.039 (0.052)	Loss 0.1064 (0.8411)	Epoch 0	Acc@1 100.000 (81.514)	Acc@5 100.000 (95.530)	Mem 269MB
[2024-11-10 18:22:06 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [44900/50000]	Time 0.039 (0.052)	Loss 0.1396 (0.8413)	Epoch 0	Acc@1 100.000 (81.513)	Acc@5 100.000 (95.528)	Mem 269MB
[2024-11-10 18:22:08 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [44950/50000]	Time 0.039 (0.052)	Loss 0.4995 (0.8409)	Epoch 0	Acc@1 100.000 (81.522)	Acc@5 100.000 (95.533)	Mem 269MB
[2024-11-10 18:22:10 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [45000/50000]	Time 0.039 (0.052)	Loss 0.6372 (0.8408)	Epoch 0	Acc@1 100.000 (81.518)	Acc@5 100.000 (95.533)	Mem 269MB
[2024-11-10 18:22:12 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [45050/50000]	Time 0.039 (0.052)	Loss 0.1509 (0.8405)	Epoch 0	Acc@1 100.000 (81.521)	Acc@5 100.000 (95.536)	Mem 269MB
[2024-11-10 18:22:14 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [45100/50000]	Time 0.039 (0.052)	Loss 0.1370 (0.8403)	Epoch 0	Acc@1 100.000 (81.519)	Acc@5 100.000 (95.539)	Mem 269MB
[2024-11-10 18:22:16 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [45150/50000]	Time 0.039 (0.052)	Loss 0.1422 (0.8401)	Epoch 0	Acc@1 100.000 (81.526)	Acc@5 100.000 (95.537)	Mem 269MB
[2024-11-10 18:22:18 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [45200/50000]	Time 0.039 (0.052)	Loss 4.2266 (0.8402)	Epoch 0	Acc@1 0.000 (81.520)	Acc@5 100.000 (95.540)	Mem 269MB
[2024-11-10 18:22:20 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [45250/50000]	Time 0.038 (0.052)	Loss 0.1149 (0.8404)	Epoch 0	Acc@1 100.000 (81.519)	Acc@5 100.000 (95.536)	Mem 269MB
[2024-11-10 18:22:22 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [45300/50000]	Time 0.039 (0.052)	Loss 0.1307 (0.8404)	Epoch 0	Acc@1 100.000 (81.515)	Acc@5 100.000 (95.539)	Mem 269MB
[2024-11-10 18:22:24 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [45350/50000]	Time 0.032 (0.052)	Loss 0.6387 (0.8405)	Epoch 0	Acc@1 100.000 (81.515)	Acc@5 100.000 (95.539)	Mem 269MB
[2024-11-10 18:22:25 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [45400/50000]	Time 0.032 (0.052)	Loss 0.1930 (0.8403)	Epoch 0	Acc@1 100.000 (81.516)	Acc@5 100.000 (95.542)	Mem 269MB
[2024-11-10 18:22:27 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [45450/50000]	Time 0.031 (0.052)	Loss 0.2908 (0.8409)	Epoch 0	Acc@1 100.000 (81.499)	Acc@5 100.000 (95.534)	Mem 269MB
[2024-11-10 18:22:28 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [45500/50000]	Time 0.032 (0.052)	Loss 0.9121 (0.8410)	Epoch 0	Acc@1 100.000 (81.497)	Acc@5 100.000 (95.534)	Mem 269MB
[2024-11-10 18:22:30 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [45550/50000]	Time 0.032 (0.052)	Loss 0.0957 (0.8416)	Epoch 0	Acc@1 100.000 (81.482)	Acc@5 100.000 (95.526)	Mem 269MB
[2024-11-10 18:22:31 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [45600/50000]	Time 0.032 (0.052)	Loss 0.6758 (0.8415)	Epoch 0	Acc@1 100.000 (81.483)	Acc@5 100.000 (95.531)	Mem 269MB
[2024-11-10 18:22:33 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [45650/50000]	Time 0.032 (0.052)	Loss 0.1072 (0.8410)	Epoch 0	Acc@1 100.000 (81.492)	Acc@5 100.000 (95.534)	Mem 269MB
[2024-11-10 18:22:35 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [45700/50000]	Time 0.032 (0.052)	Loss 0.5894 (0.8408)	Epoch 0	Acc@1 100.000 (81.497)	Acc@5 100.000 (95.536)	Mem 269MB
[2024-11-10 18:22:36 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [45750/50000]	Time 0.032 (0.052)	Loss 0.1227 (0.8408)	Epoch 0	Acc@1 100.000 (81.500)	Acc@5 100.000 (95.535)	Mem 269MB
[2024-11-10 18:22:38 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [45800/50000]	Time 0.034 (0.052)	Loss 0.1061 (0.8406)	Epoch 0	Acc@1 100.000 (81.503)	Acc@5 100.000 (95.537)	Mem 269MB
[2024-11-10 18:22:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [45850/50000]	Time 0.032 (0.052)	Loss 0.1350 (0.8407)	Epoch 0	Acc@1 100.000 (81.510)	Acc@5 100.000 (95.536)	Mem 269MB
[2024-11-10 18:22:41 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [45900/50000]	Time 0.032 (0.052)	Loss 0.1118 (0.8407)	Epoch 0	Acc@1 100.000 (81.512)	Acc@5 100.000 (95.532)	Mem 269MB
[2024-11-10 18:22:43 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [45950/50000]	Time 0.032 (0.052)	Loss 4.8750 (0.8406)	Epoch 0	Acc@1 0.000 (81.517)	Acc@5 100.000 (95.534)	Mem 269MB
[2024-11-10 18:22:44 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [46000/50000]	Time 0.032 (0.052)	Loss 0.1155 (0.8403)	Epoch 0	Acc@1 100.000 (81.526)	Acc@5 100.000 (95.537)	Mem 269MB
[2024-11-10 18:22:46 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [46050/50000]	Time 0.031 (0.052)	Loss 0.1865 (0.8405)	Epoch 0	Acc@1 100.000 (81.525)	Acc@5 100.000 (95.533)	Mem 269MB
[2024-11-10 18:22:47 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [46100/50000]	Time 0.032 (0.052)	Loss 7.2188 (0.8404)	Epoch 0	Acc@1 0.000 (81.530)	Acc@5 0.000 (95.534)	Mem 269MB
[2024-11-10 18:22:49 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [46150/50000]	Time 0.031 (0.052)	Loss 0.1534 (0.8402)	Epoch 0	Acc@1 100.000 (81.530)	Acc@5 100.000 (95.534)	Mem 269MB
[2024-11-10 18:22:50 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [46200/50000]	Time 0.032 (0.052)	Loss 5.4844 (0.8404)	Epoch 0	Acc@1 0.000 (81.526)	Acc@5 100.000 (95.533)	Mem 269MB
[2024-11-10 18:22:52 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [46250/50000]	Time 0.031 (0.052)	Loss 2.4746 (0.8402)	Epoch 0	Acc@1 0.000 (81.527)	Acc@5 100.000 (95.537)	Mem 269MB
[2024-11-10 18:22:54 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [46300/50000]	Time 0.032 (0.052)	Loss 0.1429 (0.8406)	Epoch 0	Acc@1 100.000 (81.523)	Acc@5 100.000 (95.534)	Mem 269MB
[2024-11-10 18:22:55 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [46350/50000]	Time 0.032 (0.052)	Loss 0.7148 (0.8402)	Epoch 0	Acc@1 100.000 (81.528)	Acc@5 100.000 (95.538)	Mem 269MB
[2024-11-10 18:22:57 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [46400/50000]	Time 0.032 (0.052)	Loss 0.1819 (0.8405)	Epoch 0	Acc@1 100.000 (81.526)	Acc@5 100.000 (95.532)	Mem 269MB
[2024-11-10 18:22:58 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [46450/50000]	Time 0.032 (0.052)	Loss 0.0810 (0.8405)	Epoch 0	Acc@1 100.000 (81.525)	Acc@5 100.000 (95.531)	Mem 269MB
[2024-11-10 18:23:00 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [46500/50000]	Time 0.032 (0.052)	Loss 0.1464 (0.8406)	Epoch 0	Acc@1 100.000 (81.521)	Acc@5 100.000 (95.529)	Mem 269MB
[2024-11-10 18:23:02 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [46550/50000]	Time 0.033 (0.052)	Loss 0.1088 (0.8407)	Epoch 0	Acc@1 100.000 (81.521)	Acc@5 100.000 (95.525)	Mem 269MB
[2024-11-10 18:23:03 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [46600/50000]	Time 0.032 (0.052)	Loss 0.1703 (0.8406)	Epoch 0	Acc@1 100.000 (81.524)	Acc@5 100.000 (95.528)	Mem 269MB
[2024-11-10 18:23:05 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [46650/50000]	Time 0.032 (0.052)	Loss 0.1432 (0.8403)	Epoch 0	Acc@1 100.000 (81.531)	Acc@5 100.000 (95.528)	Mem 269MB
[2024-11-10 18:23:06 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [46700/50000]	Time 0.032 (0.052)	Loss 0.1410 (0.8407)	Epoch 0	Acc@1 100.000 (81.525)	Acc@5 100.000 (95.523)	Mem 269MB
[2024-11-10 18:23:08 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [46750/50000]	Time 0.032 (0.052)	Loss 5.2891 (0.8409)	Epoch 0	Acc@1 0.000 (81.517)	Acc@5 0.000 (95.521)	Mem 269MB
[2024-11-10 18:23:10 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [46800/50000]	Time 0.032 (0.052)	Loss 0.1448 (0.8412)	Epoch 0	Acc@1 100.000 (81.513)	Acc@5 100.000 (95.515)	Mem 269MB
[2024-11-10 18:23:11 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [46850/50000]	Time 0.031 (0.052)	Loss 3.2969 (0.8412)	Epoch 0	Acc@1 0.000 (81.514)	Acc@5 100.000 (95.516)	Mem 269MB
[2024-11-10 18:23:13 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [46900/50000]	Time 0.032 (0.052)	Loss 0.0845 (0.8413)	Epoch 0	Acc@1 100.000 (81.514)	Acc@5 100.000 (95.514)	Mem 269MB
[2024-11-10 18:23:14 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [46950/50000]	Time 0.032 (0.052)	Loss 0.1476 (0.8411)	Epoch 0	Acc@1 100.000 (81.517)	Acc@5 100.000 (95.517)	Mem 269MB
[2024-11-10 18:23:16 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [47000/50000]	Time 0.032 (0.052)	Loss 0.1526 (0.8414)	Epoch 0	Acc@1 100.000 (81.513)	Acc@5 100.000 (95.511)	Mem 269MB
[2024-11-10 18:23:18 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [47050/50000]	Time 0.032 (0.052)	Loss 0.2094 (0.8413)	Epoch 0	Acc@1 100.000 (81.514)	Acc@5 100.000 (95.516)	Mem 269MB
[2024-11-10 18:23:19 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [47100/50000]	Time 0.031 (0.052)	Loss 0.7671 (0.8410)	Epoch 0	Acc@1 100.000 (81.518)	Acc@5 100.000 (95.516)	Mem 269MB
[2024-11-10 18:23:21 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [47150/50000]	Time 0.032 (0.052)	Loss 0.1045 (0.8408)	Epoch 0	Acc@1 100.000 (81.521)	Acc@5 100.000 (95.519)	Mem 269MB
[2024-11-10 18:23:22 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [47200/50000]	Time 0.032 (0.052)	Loss 0.2499 (0.8408)	Epoch 0	Acc@1 100.000 (81.526)	Acc@5 100.000 (95.517)	Mem 269MB
[2024-11-10 18:23:24 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [47250/50000]	Time 0.032 (0.052)	Loss 0.0973 (0.8410)	Epoch 0	Acc@1 100.000 (81.522)	Acc@5 100.000 (95.518)	Mem 269MB
[2024-11-10 18:23:25 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [47300/50000]	Time 0.032 (0.051)	Loss 0.0883 (0.8411)	Epoch 0	Acc@1 100.000 (81.525)	Acc@5 100.000 (95.516)	Mem 269MB
[2024-11-10 18:23:27 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [47350/50000]	Time 0.032 (0.051)	Loss 0.0698 (0.8408)	Epoch 0	Acc@1 100.000 (81.532)	Acc@5 100.000 (95.516)	Mem 269MB
[2024-11-10 18:23:29 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [47400/50000]	Time 0.033 (0.051)	Loss 0.1754 (0.8410)	Epoch 0	Acc@1 100.000 (81.530)	Acc@5 100.000 (95.515)	Mem 269MB
[2024-11-10 18:23:30 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [47450/50000]	Time 0.032 (0.051)	Loss 0.1166 (0.8407)	Epoch 0	Acc@1 100.000 (81.535)	Acc@5 100.000 (95.520)	Mem 269MB
[2024-11-10 18:23:32 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [47500/50000]	Time 0.032 (0.051)	Loss 2.7109 (0.8408)	Epoch 0	Acc@1 0.000 (81.531)	Acc@5 100.000 (95.518)	Mem 269MB
[2024-11-10 18:23:33 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [47550/50000]	Time 0.032 (0.051)	Loss 0.2986 (0.8412)	Epoch 0	Acc@1 100.000 (81.525)	Acc@5 100.000 (95.516)	Mem 269MB
[2024-11-10 18:23:35 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [47600/50000]	Time 0.032 (0.051)	Loss 0.0427 (0.8411)	Epoch 0	Acc@1 100.000 (81.528)	Acc@5 100.000 (95.515)	Mem 269MB
[2024-11-10 18:23:37 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [47650/50000]	Time 0.032 (0.051)	Loss 0.1093 (0.8410)	Epoch 0	Acc@1 100.000 (81.528)	Acc@5 100.000 (95.520)	Mem 269MB
[2024-11-10 18:23:38 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [47700/50000]	Time 0.033 (0.051)	Loss 3.0391 (0.8410)	Epoch 0	Acc@1 0.000 (81.527)	Acc@5 100.000 (95.518)	Mem 269MB
[2024-11-10 18:23:40 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [47750/50000]	Time 0.031 (0.051)	Loss 0.0839 (0.8408)	Epoch 0	Acc@1 100.000 (81.533)	Acc@5 100.000 (95.523)	Mem 269MB
[2024-11-10 18:23:42 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [47800/50000]	Time 0.033 (0.051)	Loss 0.1473 (0.8412)	Epoch 0	Acc@1 100.000 (81.525)	Acc@5 100.000 (95.519)	Mem 269MB
[2024-11-10 18:23:43 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [47850/50000]	Time 0.032 (0.051)	Loss 0.5044 (0.8411)	Epoch 0	Acc@1 100.000 (81.530)	Acc@5 100.000 (95.519)	Mem 269MB
[2024-11-10 18:23:45 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [47900/50000]	Time 0.032 (0.051)	Loss 0.1660 (0.8407)	Epoch 0	Acc@1 100.000 (81.541)	Acc@5 100.000 (95.522)	Mem 269MB
[2024-11-10 18:23:46 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [47950/50000]	Time 0.032 (0.051)	Loss 4.0273 (0.8406)	Epoch 0	Acc@1 0.000 (81.548)	Acc@5 100.000 (95.520)	Mem 269MB
[2024-11-10 18:23:48 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [48000/50000]	Time 0.035 (0.051)	Loss 1.6562 (0.8405)	Epoch 0	Acc@1 0.000 (81.542)	Acc@5 100.000 (95.521)	Mem 269MB
[2024-11-10 18:23:49 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [48050/50000]	Time 0.032 (0.051)	Loss 0.1295 (0.8405)	Epoch 0	Acc@1 100.000 (81.540)	Acc@5 100.000 (95.519)	Mem 269MB
[2024-11-10 18:23:51 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [48100/50000]	Time 0.032 (0.051)	Loss 0.0932 (0.8402)	Epoch 0	Acc@1 100.000 (81.543)	Acc@5 100.000 (95.522)	Mem 269MB
[2024-11-10 18:23:53 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [48150/50000]	Time 0.032 (0.051)	Loss 1.4609 (0.8403)	Epoch 0	Acc@1 100.000 (81.537)	Acc@5 100.000 (95.520)	Mem 269MB
[2024-11-10 18:23:54 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [48200/50000]	Time 0.031 (0.051)	Loss 0.0827 (0.8403)	Epoch 0	Acc@1 100.000 (81.536)	Acc@5 100.000 (95.519)	Mem 269MB
[2024-11-10 18:23:56 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [48250/50000]	Time 0.032 (0.051)	Loss 0.0829 (0.8404)	Epoch 0	Acc@1 100.000 (81.536)	Acc@5 100.000 (95.517)	Mem 269MB
[2024-11-10 18:23:57 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [48300/50000]	Time 0.032 (0.051)	Loss 0.0898 (0.8406)	Epoch 0	Acc@1 100.000 (81.535)	Acc@5 100.000 (95.518)	Mem 269MB
[2024-11-10 18:23:59 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [48350/50000]	Time 0.031 (0.051)	Loss 0.1410 (0.8406)	Epoch 0	Acc@1 100.000 (81.533)	Acc@5 100.000 (95.518)	Mem 269MB
[2024-11-10 18:24:01 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [48400/50000]	Time 0.031 (0.051)	Loss 1.3691 (0.8405)	Epoch 0	Acc@1 100.000 (81.533)	Acc@5 100.000 (95.517)	Mem 269MB
[2024-11-10 18:24:02 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [48450/50000]	Time 0.032 (0.051)	Loss 0.3262 (0.8407)	Epoch 0	Acc@1 100.000 (81.528)	Acc@5 100.000 (95.511)	Mem 269MB
[2024-11-10 18:24:04 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [48500/50000]	Time 0.032 (0.051)	Loss 0.1310 (0.8405)	Epoch 0	Acc@1 100.000 (81.532)	Acc@5 100.000 (95.513)	Mem 269MB
[2024-11-10 18:24:06 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [48550/50000]	Time 0.031 (0.051)	Loss 0.8364 (0.8407)	Epoch 0	Acc@1 100.000 (81.525)	Acc@5 100.000 (95.510)	Mem 269MB
[2024-11-10 18:24:07 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [48600/50000]	Time 0.031 (0.051)	Loss 0.1052 (0.8408)	Epoch 0	Acc@1 100.000 (81.517)	Acc@5 100.000 (95.508)	Mem 269MB
[2024-11-10 18:24:09 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [48650/50000]	Time 0.031 (0.051)	Loss 0.1185 (0.8409)	Epoch 0	Acc@1 100.000 (81.515)	Acc@5 100.000 (95.507)	Mem 269MB
[2024-11-10 18:24:10 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [48700/50000]	Time 0.032 (0.051)	Loss 0.1494 (0.8408)	Epoch 0	Acc@1 100.000 (81.512)	Acc@5 100.000 (95.505)	Mem 269MB
[2024-11-10 18:24:12 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [48750/50000]	Time 0.031 (0.051)	Loss 0.4634 (0.8406)	Epoch 0	Acc@1 100.000 (81.518)	Acc@5 100.000 (95.506)	Mem 269MB
[2024-11-10 18:24:14 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [48800/50000]	Time 0.040 (0.051)	Loss 2.1543 (0.8407)	Epoch 0	Acc@1 0.000 (81.511)	Acc@5 100.000 (95.508)	Mem 269MB
[2024-11-10 18:24:15 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [48850/50000]	Time 0.032 (0.051)	Loss 0.1522 (0.8405)	Epoch 0	Acc@1 100.000 (81.517)	Acc@5 100.000 (95.509)	Mem 269MB
[2024-11-10 18:24:17 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [48900/50000]	Time 0.032 (0.051)	Loss 0.0808 (0.8404)	Epoch 0	Acc@1 100.000 (81.516)	Acc@5 100.000 (95.511)	Mem 269MB
[2024-11-10 18:24:18 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [48950/50000]	Time 0.032 (0.051)	Loss 0.1065 (0.8407)	Epoch 0	Acc@1 100.000 (81.506)	Acc@5 100.000 (95.510)	Mem 269MB
[2024-11-10 18:24:20 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [49000/50000]	Time 0.032 (0.051)	Loss 0.1622 (0.8409)	Epoch 0	Acc@1 100.000 (81.496)	Acc@5 100.000 (95.508)	Mem 269MB
[2024-11-10 18:24:22 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [49050/50000]	Time 0.032 (0.051)	Loss 0.1366 (0.8412)	Epoch 0	Acc@1 100.000 (81.487)	Acc@5 100.000 (95.507)	Mem 269MB
[2024-11-10 18:24:23 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [49100/50000]	Time 0.032 (0.051)	Loss 4.9727 (0.8411)	Epoch 0	Acc@1 0.000 (81.485)	Acc@5 0.000 (95.503)	Mem 269MB
[2024-11-10 18:24:25 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [49150/50000]	Time 0.032 (0.051)	Loss 0.1138 (0.8416)	Epoch 0	Acc@1 100.000 (81.482)	Acc@5 100.000 (95.498)	Mem 269MB
[2024-11-10 18:24:26 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [49200/50000]	Time 0.032 (0.051)	Loss 5.3633 (0.8417)	Epoch 0	Acc@1 0.000 (81.482)	Acc@5 0.000 (95.496)	Mem 269MB
[2024-11-10 18:24:28 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [49250/50000]	Time 0.032 (0.051)	Loss 2.7988 (0.8416)	Epoch 0	Acc@1 0.000 (81.485)	Acc@5 100.000 (95.499)	Mem 269MB
[2024-11-10 18:24:29 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [49300/50000]	Time 0.032 (0.051)	Loss 1.8818 (0.8414)	Epoch 0	Acc@1 0.000 (81.491)	Acc@5 100.000 (95.501)	Mem 269MB
[2024-11-10 18:24:31 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [49350/50000]	Time 0.032 (0.051)	Loss 2.4375 (0.8412)	Epoch 0	Acc@1 0.000 (81.494)	Acc@5 100.000 (95.506)	Mem 269MB
[2024-11-10 18:24:33 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [49400/50000]	Time 0.031 (0.051)	Loss 0.1272 (0.8412)	Epoch 0	Acc@1 100.000 (81.494)	Acc@5 100.000 (95.506)	Mem 269MB
[2024-11-10 18:24:34 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [49450/50000]	Time 0.032 (0.051)	Loss 0.1526 (0.8409)	Epoch 0	Acc@1 100.000 (81.499)	Acc@5 100.000 (95.509)	Mem 269MB
[2024-11-10 18:24:36 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [49500/50000]	Time 0.032 (0.051)	Loss 2.1719 (0.8409)	Epoch 0	Acc@1 0.000 (81.497)	Acc@5 100.000 (95.509)	Mem 269MB
[2024-11-10 18:24:37 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [49550/50000]	Time 0.032 (0.051)	Loss 6.8242 (0.8407)	Epoch 0	Acc@1 0.000 (81.504)	Acc@5 0.000 (95.510)	Mem 269MB
[2024-11-10 18:24:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [49600/50000]	Time 0.031 (0.051)	Loss 0.1223 (0.8406)	Epoch 0	Acc@1 100.000 (81.504)	Acc@5 100.000 (95.512)	Mem 269MB
[2024-11-10 18:24:41 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [49650/50000]	Time 0.032 (0.051)	Loss 0.2278 (0.8404)	Epoch 0	Acc@1 100.000 (81.511)	Acc@5 100.000 (95.515)	Mem 269MB
[2024-11-10 18:24:42 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [49700/50000]	Time 0.032 (0.051)	Loss 0.1102 (0.8405)	Epoch 0	Acc@1 100.000 (81.509)	Acc@5 100.000 (95.517)	Mem 269MB
[2024-11-10 18:24:44 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [49750/50000]	Time 0.032 (0.051)	Loss 0.1565 (0.8405)	Epoch 0	Acc@1 100.000 (81.508)	Acc@5 100.000 (95.518)	Mem 269MB
[2024-11-10 18:24:45 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [49800/50000]	Time 0.033 (0.051)	Loss 0.6255 (0.8407)	Epoch 0	Acc@1 100.000 (81.502)	Acc@5 100.000 (95.516)	Mem 269MB
[2024-11-10 18:24:47 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [49850/50000]	Time 0.032 (0.050)	Loss 0.3535 (0.8405)	Epoch 0	Acc@1 100.000 (81.505)	Acc@5 100.000 (95.519)	Mem 269MB
[2024-11-10 18:24:49 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [49900/50000]	Time 0.032 (0.050)	Loss 0.0720 (0.8406)	Epoch 0	Acc@1 100.000 (81.505)	Acc@5 100.000 (95.517)	Mem 269MB
[2024-11-10 18:24:50 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [49950/50000]	Time 0.032 (0.050)	Loss 1.1699 (0.8404)	Epoch 0	Acc@1 0.000 (81.510)	Acc@5 100.000 (95.522)	Mem 269MB
[2024-11-10 18:24:52 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [49999/50000]	Time 0.030 (0.050)	Loss 1.6729 (0.8402)	Epoch 0	Acc@1 0.000 (81.516)	Acc@5 100.000 (95.526)	Mem 269MB
[2024-11-10 18:24:52 cros_tiny_patch4_group7_224] (main.py 317): INFO  * Acc@1 81.516 Acc@5 95.526
[2024-11-10 18:24:52 cros_tiny_patch4_group7_224] (main.py 127): INFO Accuracy of the network on the 50000 test images: 81.5%
[2024-11-10 18:28:20 cros_tiny_patch4_group7_224] (main.py 386): INFO Full config saved to output/log/debug/config.json
[2024-11-10 18:28:20 cros_tiny_patch4_group7_224] (main.py 389): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: false
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/crossformer-t.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 18:28:33 cros_tiny_patch4_group7_224] (main.py 94): INFO Creating model:cross-scale/cros_tiny_patch4_group7_224
[2024-11-10 18:28:44 cros_tiny_patch4_group7_224] (main.py 97): INFO CrossFormer(
  (patch_embed): PatchEmbed(
    (projs): ModuleList(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(4, 4))
      (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (2): Conv2d(3, 8, kernel_size=(16, 16), stride=(4, 4), padding=(6, 6))
      (3): Conv2d(3, 8, kernel_size=(32, 32), stride=(4, 4), padding=(14, 14))
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): Stage(
      dim=64, input_resolution=(56, 56), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=64, input_resolution=(56, 56), num_heads=2, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=8
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=64, group_size=(7, 7), num_heads=2
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=4, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=2, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=64, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=64
        (reductions): ModuleList(
          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Stage(
      dim=128, input_resolution=(28, 28), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=128, input_resolution=(28, 28), num_heads=4, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=4
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=128, group_size=(7, 7), num_heads=4
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=8, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=4, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.007)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=128
        (reductions): ModuleList(
          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): Stage(
      dim=256, input_resolution=(14, 14), depth=8
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.020)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.047)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.060)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=256
        (reductions): ModuleList(
          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): Stage(
      dim=512, input_resolution=(7, 7), depth=6
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=512, out_features=1000, bias=True)
)
[2024-11-10 18:28:44 cros_tiny_patch4_group7_224] (main.py 107): INFO number of params: 27804848
[2024-11-10 18:28:45 cros_tiny_patch4_group7_224] (main.py 110): INFO number of GFLOPs: 2.858292352
[2024-11-10 18:28:48 cros_tiny_patch4_group7_224] (utils.py 42): INFO ==============> Resuming from ./model_ckpt/crossformer-t.pth....................
[2024-11-10 18:28:48 cros_tiny_patch4_group7_224] (utils.py 49): INFO _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.rpb.relative_position_bias_table', 'layers.1.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.1.attn.rpb.relative_position_bias_table', 'layers.2.blocks.2.attn.rpb.relative_position_bias_table', 'layers.2.blocks.3.attn.rpb.relative_position_bias_table', 'layers.2.blocks.4.attn.rpb.relative_position_bias_table', 'layers.2.blocks.5.attn.rpb.relative_position_bias_table', 'layers.2.blocks.6.attn.rpb.relative_position_bias_table', 'layers.2.blocks.7.attn.rpb.relative_position_bias_table', 'layers.3.blocks.0.attn.rpb.relative_position_bias_table', 'layers.3.blocks.1.attn.rpb.relative_position_bias_table', 'layers.3.blocks.2.attn.rpb.relative_position_bias_table', 'layers.3.blocks.3.attn.rpb.relative_position_bias_table', 'layers.3.blocks.4.attn.rpb.relative_position_bias_table', 'layers.3.blocks.5.attn.rpb.relative_position_bias_table'], unexpected_keys=['layers.0.blocks.0.attn.biases', 'layers.0.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.attn.biases', 'layers.1.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.attn.biases', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.biases', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.biases', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.biases', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.biases', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.biases', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.biases', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.biases', 'layers.2.blocks.7.attn.relative_position_index', 'layers.3.blocks.0.attn.biases', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.biases', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.2.attn.biases', 'layers.3.blocks.2.attn.relative_position_index', 'layers.3.blocks.3.attn.biases', 'layers.3.blocks.3.attn.relative_position_index', 'layers.3.blocks.4.attn.biases', 'layers.3.blocks.4.attn.relative_position_index', 'layers.3.blocks.5.attn.biases', 'layers.3.blocks.5.attn.relative_position_index'])
[2024-11-10 18:29:14 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [0/391]	Time 14.597 (14.597)	Loss 2.3144 (2.3144)	Epoch 0	Acc@1 55.469 (55.469)	Acc@5 75.000 (75.000)	Mem 2725MB
[2024-11-10 18:29:32 cros_tiny_patch4_group7_224] (save_rpb_weight.py 158): INFO Full config saved to output/log/debug/config.json
[2024-11-10 18:29:32 cros_tiny_patch4_group7_224] (save_rpb_weight.py 161): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: true
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/crossformer-t.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0005
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 5.0e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 5.0e-07
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 18:29:33 cros_tiny_patch4_group7_224] (save_rpb_weight.py 92): INFO Creating model:cross-scale/cros_tiny_patch4_group7_224
[2024-11-10 18:29:36 cros_tiny_patch4_group7_224] (save_rpb_weight.py 95): INFO CrossFormer(
  (patch_embed): PatchEmbed(
    (projs): ModuleList(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(4, 4))
      (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (2): Conv2d(3, 8, kernel_size=(16, 16), stride=(4, 4), padding=(6, 6))
      (3): Conv2d(3, 8, kernel_size=(32, 32), stride=(4, 4), padding=(14, 14))
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): Stage(
      dim=64, input_resolution=(56, 56), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=64, input_resolution=(56, 56), num_heads=2, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=8
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=64, group_size=(7, 7), num_heads=2
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=4, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=2, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=64, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=64
        (reductions): ModuleList(
          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Stage(
      dim=128, input_resolution=(28, 28), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=128, input_resolution=(28, 28), num_heads=4, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=4
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=128, group_size=(7, 7), num_heads=4
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=8, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=4, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.007)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=128
        (reductions): ModuleList(
          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): Stage(
      dim=256, input_resolution=(14, 14), depth=8
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.020)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.047)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.060)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=256
        (reductions): ModuleList(
          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): Stage(
      dim=512, input_resolution=(7, 7), depth=6
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=512, out_features=1000, bias=True)
)
[2024-11-10 18:29:37 cros_tiny_patch4_group7_224] (save_rpb_weight.py 98): INFO number of params: 27804848
[2024-11-10 18:29:40 cros_tiny_patch4_group7_224] (save_rpb_weight.py 105): INFO _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.rpb.relative_position_bias_table', 'layers.1.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.1.attn.rpb.relative_position_bias_table', 'layers.2.blocks.2.attn.rpb.relative_position_bias_table', 'layers.2.blocks.3.attn.rpb.relative_position_bias_table', 'layers.2.blocks.4.attn.rpb.relative_position_bias_table', 'layers.2.blocks.5.attn.rpb.relative_position_bias_table', 'layers.2.blocks.6.attn.rpb.relative_position_bias_table', 'layers.2.blocks.7.attn.rpb.relative_position_bias_table', 'layers.3.blocks.0.attn.rpb.relative_position_bias_table', 'layers.3.blocks.1.attn.rpb.relative_position_bias_table', 'layers.3.blocks.2.attn.rpb.relative_position_bias_table', 'layers.3.blocks.3.attn.rpb.relative_position_bias_table', 'layers.3.blocks.4.attn.rpb.relative_position_bias_table', 'layers.3.blocks.5.attn.rpb.relative_position_bias_table'], unexpected_keys=['layers.0.blocks.0.attn.biases', 'layers.0.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.attn.biases', 'layers.1.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.attn.biases', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.biases', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.biases', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.biases', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.biases', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.biases', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.biases', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.biases', 'layers.2.blocks.7.attn.relative_position_index', 'layers.3.blocks.0.attn.biases', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.biases', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.2.attn.biases', 'layers.3.blocks.2.attn.relative_position_index', 'layers.3.blocks.3.attn.biases', 'layers.3.blocks.3.attn.relative_position_index', 'layers.3.blocks.4.attn.biases', 'layers.3.blocks.4.attn.relative_position_index', 'layers.3.blocks.5.attn.biases', 'layers.3.blocks.5.attn.relative_position_index'])
[2024-11-10 18:29:42 cros_tiny_patch4_group7_224] (save_rpb_weight.py 108): INFO => loaded successfully './model_ckpt/crossformer-t.pth'
[2024-11-10 18:34:15 cros_tiny_patch4_group7_224] (main_save.py 357): INFO Full config saved to output/log/debug/config.json
[2024-11-10 18:34:15 cros_tiny_patch4_group7_224] (main_save.py 360): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: true
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/crossformer-t.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 18:34:26 cros_tiny_patch4_group7_224] (main_save.py 94): INFO Creating model:cross-scale/cros_tiny_patch4_group7_224
[2024-11-10 18:34:27 cros_tiny_patch4_group7_224] (main_save.py 97): INFO CrossFormer(
  (patch_embed): PatchEmbed(
    (projs): ModuleList(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(4, 4))
      (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (2): Conv2d(3, 8, kernel_size=(16, 16), stride=(4, 4), padding=(6, 6))
      (3): Conv2d(3, 8, kernel_size=(32, 32), stride=(4, 4), padding=(14, 14))
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): Stage(
      dim=64, input_resolution=(56, 56), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=64, input_resolution=(56, 56), num_heads=2, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=8
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=64, group_size=(7, 7), num_heads=2
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=4, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=2, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=64, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=64
        (reductions): ModuleList(
          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Stage(
      dim=128, input_resolution=(28, 28), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=128, input_resolution=(28, 28), num_heads=4, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=4
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=128, group_size=(7, 7), num_heads=4
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=8, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=4, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.007)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=128
        (reductions): ModuleList(
          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): Stage(
      dim=256, input_resolution=(14, 14), depth=8
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.020)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.047)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.060)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=256
        (reductions): ModuleList(
          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): Stage(
      dim=512, input_resolution=(7, 7), depth=6
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=512, out_features=1000, bias=True)
)
[2024-11-10 18:34:27 cros_tiny_patch4_group7_224] (main_save.py 107): INFO number of params: 27804848
[2024-11-10 18:34:27 cros_tiny_patch4_group7_224] (main_save.py 110): INFO number of GFLOPs: 2.858292352
[2024-11-10 18:34:27 cros_tiny_patch4_group7_224] (utils.py 42): INFO ==============> Resuming from ./model_ckpt/crossformer-t.pth....................
[2024-11-10 18:34:27 cros_tiny_patch4_group7_224] (utils.py 49): INFO _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.rpb.relative_position_bias_table', 'layers.1.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.1.attn.rpb.relative_position_bias_table', 'layers.2.blocks.2.attn.rpb.relative_position_bias_table', 'layers.2.blocks.3.attn.rpb.relative_position_bias_table', 'layers.2.blocks.4.attn.rpb.relative_position_bias_table', 'layers.2.blocks.5.attn.rpb.relative_position_bias_table', 'layers.2.blocks.6.attn.rpb.relative_position_bias_table', 'layers.2.blocks.7.attn.rpb.relative_position_bias_table', 'layers.3.blocks.0.attn.rpb.relative_position_bias_table', 'layers.3.blocks.1.attn.rpb.relative_position_bias_table', 'layers.3.blocks.2.attn.rpb.relative_position_bias_table', 'layers.3.blocks.3.attn.rpb.relative_position_bias_table', 'layers.3.blocks.4.attn.rpb.relative_position_bias_table', 'layers.3.blocks.5.attn.rpb.relative_position_bias_table'], unexpected_keys=['layers.0.blocks.0.attn.biases', 'layers.0.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.attn.biases', 'layers.1.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.attn.biases', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.biases', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.biases', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.biases', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.biases', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.biases', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.biases', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.biases', 'layers.2.blocks.7.attn.relative_position_index', 'layers.3.blocks.0.attn.biases', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.biases', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.2.attn.biases', 'layers.3.blocks.2.attn.relative_position_index', 'layers.3.blocks.3.attn.biases', 'layers.3.blocks.3.attn.relative_position_index', 'layers.3.blocks.4.attn.biases', 'layers.3.blocks.4.attn.relative_position_index', 'layers.3.blocks.5.attn.biases', 'layers.3.blocks.5.attn.relative_position_index'])
[2024-11-10 18:35:30 cros_tiny_patch4_group7_224] (main.py 386): INFO Full config saved to output/log/debug/config.json
[2024-11-10 18:35:30 cros_tiny_patch4_group7_224] (main.py 389): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: true
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/crossformer-t.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 18:35:33 cros_tiny_patch4_group7_224] (main.py 94): INFO Creating model:cross-scale/cros_tiny_patch4_group7_224
[2024-11-10 18:35:33 cros_tiny_patch4_group7_224] (main.py 97): INFO CrossFormer(
  (patch_embed): PatchEmbed(
    (projs): ModuleList(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(4, 4))
      (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (2): Conv2d(3, 8, kernel_size=(16, 16), stride=(4, 4), padding=(6, 6))
      (3): Conv2d(3, 8, kernel_size=(32, 32), stride=(4, 4), padding=(14, 14))
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): Stage(
      dim=64, input_resolution=(56, 56), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=64, input_resolution=(56, 56), num_heads=2, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=8
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=64, group_size=(7, 7), num_heads=2
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=4, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=2, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=64, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=64
        (reductions): ModuleList(
          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Stage(
      dim=128, input_resolution=(28, 28), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=128, input_resolution=(28, 28), num_heads=4, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=4
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=128, group_size=(7, 7), num_heads=4
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=8, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=4, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.007)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=128
        (reductions): ModuleList(
          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): Stage(
      dim=256, input_resolution=(14, 14), depth=8
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.020)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.047)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.060)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=256
        (reductions): ModuleList(
          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): Stage(
      dim=512, input_resolution=(7, 7), depth=6
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=512, out_features=1000, bias=True)
)
[2024-11-10 18:35:33 cros_tiny_patch4_group7_224] (main.py 107): INFO number of params: 27804848
[2024-11-10 18:35:33 cros_tiny_patch4_group7_224] (main.py 110): INFO number of GFLOPs: 2.858292352
[2024-11-10 18:35:33 cros_tiny_patch4_group7_224] (utils.py 42): INFO ==============> Resuming from ./model_ckpt/crossformer-t.pth....................
[2024-11-10 18:35:34 cros_tiny_patch4_group7_224] (utils.py 49): INFO _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.rpb.relative_position_bias_table', 'layers.1.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.1.attn.rpb.relative_position_bias_table', 'layers.2.blocks.2.attn.rpb.relative_position_bias_table', 'layers.2.blocks.3.attn.rpb.relative_position_bias_table', 'layers.2.blocks.4.attn.rpb.relative_position_bias_table', 'layers.2.blocks.5.attn.rpb.relative_position_bias_table', 'layers.2.blocks.6.attn.rpb.relative_position_bias_table', 'layers.2.blocks.7.attn.rpb.relative_position_bias_table', 'layers.3.blocks.0.attn.rpb.relative_position_bias_table', 'layers.3.blocks.1.attn.rpb.relative_position_bias_table', 'layers.3.blocks.2.attn.rpb.relative_position_bias_table', 'layers.3.blocks.3.attn.rpb.relative_position_bias_table', 'layers.3.blocks.4.attn.rpb.relative_position_bias_table', 'layers.3.blocks.5.attn.rpb.relative_position_bias_table'], unexpected_keys=['layers.0.blocks.0.attn.biases', 'layers.0.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.attn.biases', 'layers.1.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.attn.biases', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.biases', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.biases', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.biases', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.biases', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.biases', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.biases', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.biases', 'layers.2.blocks.7.attn.relative_position_index', 'layers.3.blocks.0.attn.biases', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.biases', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.2.attn.biases', 'layers.3.blocks.2.attn.relative_position_index', 'layers.3.blocks.3.attn.biases', 'layers.3.blocks.3.attn.relative_position_index', 'layers.3.blocks.4.attn.biases', 'layers.3.blocks.4.attn.relative_position_index', 'layers.3.blocks.5.attn.biases', 'layers.3.blocks.5.attn.relative_position_index'])
[2024-11-10 18:35:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [0/391]	Time 5.001 (5.001)	Loss 0.9881 (0.9881)	Epoch 0	Acc@1 78.125 (78.125)	Acc@5 94.531 (94.531)	Mem 2725MB
[2024-11-10 18:38:01 cros_tiny_patch4_group7_224] (main.py 386): INFO Full config saved to output/log/debug/config.json
[2024-11-10 18:38:01 cros_tiny_patch4_group7_224] (main.py 389): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: false
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/cros_tiny_patch4_group7_224_rpb.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 18:38:04 cros_tiny_patch4_group7_224] (main.py 94): INFO Creating model:cross-scale/cros_tiny_patch4_group7_224
[2024-11-10 18:38:05 cros_tiny_patch4_group7_224] (main.py 97): INFO CrossFormer(
  (patch_embed): PatchEmbed(
    (projs): ModuleList(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(4, 4))
      (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (2): Conv2d(3, 8, kernel_size=(16, 16), stride=(4, 4), padding=(6, 6))
      (3): Conv2d(3, 8, kernel_size=(32, 32), stride=(4, 4), padding=(14, 14))
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): Stage(
      dim=64, input_resolution=(56, 56), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=64, input_resolution=(56, 56), num_heads=2, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=8
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=64, group_size=(7, 7), num_heads=2
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=4, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=2, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=64, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=64
        (reductions): ModuleList(
          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Stage(
      dim=128, input_resolution=(28, 28), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=128, input_resolution=(28, 28), num_heads=4, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=4
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=128, group_size=(7, 7), num_heads=4
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=8, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=4, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.007)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=128
        (reductions): ModuleList(
          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): Stage(
      dim=256, input_resolution=(14, 14), depth=8
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.020)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.047)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.060)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=256
        (reductions): ModuleList(
          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): Stage(
      dim=512, input_resolution=(7, 7), depth=6
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=512, out_features=1000, bias=True)
)
[2024-11-10 18:38:05 cros_tiny_patch4_group7_224] (main.py 107): INFO number of params: 27804848
[2024-11-10 18:38:05 cros_tiny_patch4_group7_224] (main.py 110): INFO number of GFLOPs: 2.858292352
[2024-11-10 18:38:05 cros_tiny_patch4_group7_224] (utils.py 42): INFO ==============> Resuming from ./model_ckpt/cros_tiny_patch4_group7_224_rpb.pth....................
[2024-11-10 18:38:05 cros_tiny_patch4_group7_224] (utils.py 49): INFO <All keys matched successfully>
[2024-11-10 18:38:35 cros_tiny_patch4_group7_224] (main.py 386): INFO Full config saved to output/log/debug/config.json
[2024-11-10 18:38:35 cros_tiny_patch4_group7_224] (main.py 389): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: false
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/cros_tiny_patch4_group7_224_rpb.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 18:38:38 cros_tiny_patch4_group7_224] (main.py 94): INFO Creating model:cross-scale/cros_tiny_patch4_group7_224
[2024-11-10 18:38:39 cros_tiny_patch4_group7_224] (main.py 97): INFO CrossFormer(
  (patch_embed): PatchEmbed(
    (projs): ModuleList(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(4, 4))
      (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (2): Conv2d(3, 8, kernel_size=(16, 16), stride=(4, 4), padding=(6, 6))
      (3): Conv2d(3, 8, kernel_size=(32, 32), stride=(4, 4), padding=(14, 14))
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): Stage(
      dim=64, input_resolution=(56, 56), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=64, input_resolution=(56, 56), num_heads=2, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=8
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=64, group_size=(7, 7), num_heads=2
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=4, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=2, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=64, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=64
        (reductions): ModuleList(
          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Stage(
      dim=128, input_resolution=(28, 28), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=128, input_resolution=(28, 28), num_heads=4, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=4
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=128, group_size=(7, 7), num_heads=4
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=8, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=4, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.007)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=128
        (reductions): ModuleList(
          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): Stage(
      dim=256, input_resolution=(14, 14), depth=8
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.020)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.047)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.060)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=256
        (reductions): ModuleList(
          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): Stage(
      dim=512, input_resolution=(7, 7), depth=6
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=512, out_features=1000, bias=True)
)
[2024-11-10 18:38:39 cros_tiny_patch4_group7_224] (main.py 107): INFO number of params: 27804848
[2024-11-10 18:38:39 cros_tiny_patch4_group7_224] (main.py 110): INFO number of GFLOPs: 2.858292352
[2024-11-10 18:38:39 cros_tiny_patch4_group7_224] (utils.py 42): INFO ==============> Resuming from ./model_ckpt/cros_tiny_patch4_group7_224_rpb.pth....................
[2024-11-10 18:38:39 cros_tiny_patch4_group7_224] (utils.py 49): INFO <All keys matched successfully>
[2024-11-10 18:40:02 cros_tiny_patch4_group7_224] (main.py 386): INFO Full config saved to output/log/debug/config.json
[2024-11-10 18:40:02 cros_tiny_patch4_group7_224] (main.py 389): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: false
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/cros_tiny_patch4_group7_224_rpb.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 18:40:05 cros_tiny_patch4_group7_224] (main.py 94): INFO Creating model:cross-scale/cros_tiny_patch4_group7_224
[2024-11-10 18:40:06 cros_tiny_patch4_group7_224] (main.py 97): INFO CrossFormer(
  (patch_embed): PatchEmbed(
    (projs): ModuleList(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(4, 4))
      (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (2): Conv2d(3, 8, kernel_size=(16, 16), stride=(4, 4), padding=(6, 6))
      (3): Conv2d(3, 8, kernel_size=(32, 32), stride=(4, 4), padding=(14, 14))
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): Stage(
      dim=64, input_resolution=(56, 56), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=64, input_resolution=(56, 56), num_heads=2, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=8
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=64, group_size=(7, 7), num_heads=2
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=4, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=2, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=64, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=64
        (reductions): ModuleList(
          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Stage(
      dim=128, input_resolution=(28, 28), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=128, input_resolution=(28, 28), num_heads=4, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=4
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=128, group_size=(7, 7), num_heads=4
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=8, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=4, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.007)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=128
        (reductions): ModuleList(
          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): Stage(
      dim=256, input_resolution=(14, 14), depth=8
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.020)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.047)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.060)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=256
        (reductions): ModuleList(
          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): Stage(
      dim=512, input_resolution=(7, 7), depth=6
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=512, out_features=1000, bias=True)
)
[2024-11-10 18:40:06 cros_tiny_patch4_group7_224] (main.py 107): INFO number of params: 27804848
[2024-11-10 18:40:06 cros_tiny_patch4_group7_224] (main.py 110): INFO number of GFLOPs: 2.858292352
[2024-11-10 18:40:06 cros_tiny_patch4_group7_224] (utils.py 42): INFO ==============> Resuming from ./model_ckpt/cros_tiny_patch4_group7_224_rpb.pth....................
[2024-11-10 18:40:06 cros_tiny_patch4_group7_224] (utils.py 49): INFO <All keys matched successfully>
[2024-11-10 18:42:30 cros_tiny_patch4_group7_224] (save_rpb_weight.py 158): INFO Full config saved to output/log/debug/config.json
[2024-11-10 18:42:30 cros_tiny_patch4_group7_224] (save_rpb_weight.py 161): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: true
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/crossformer-t.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0005
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 5.0e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 5.0e-07
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 18:42:32 cros_tiny_patch4_group7_224] (save_rpb_weight.py 92): INFO Creating model:cross-scale/cros_tiny_patch4_group7_224
[2024-11-10 18:42:34 cros_tiny_patch4_group7_224] (save_rpb_weight.py 95): INFO CrossFormer(
  (patch_embed): PatchEmbed(
    (projs): ModuleList(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(4, 4))
      (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (2): Conv2d(3, 8, kernel_size=(16, 16), stride=(4, 4), padding=(6, 6))
      (3): Conv2d(3, 8, kernel_size=(32, 32), stride=(4, 4), padding=(14, 14))
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): Stage(
      dim=64, input_resolution=(56, 56), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=64, input_resolution=(56, 56), num_heads=2, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=8
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=64, group_size=(7, 7), num_heads=2
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=4, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=2, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=64, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=64
        (reductions): ModuleList(
          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Stage(
      dim=128, input_resolution=(28, 28), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=128, input_resolution=(28, 28), num_heads=4, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=4
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=128, group_size=(7, 7), num_heads=4
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=8, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=4, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.007)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=128
        (reductions): ModuleList(
          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): Stage(
      dim=256, input_resolution=(14, 14), depth=8
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.020)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.047)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.060)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=256
        (reductions): ModuleList(
          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): Stage(
      dim=512, input_resolution=(7, 7), depth=6
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=512, out_features=1000, bias=True)
)
[2024-11-10 18:42:34 cros_tiny_patch4_group7_224] (save_rpb_weight.py 98): INFO number of params: 27804848
[2024-11-10 18:42:34 cros_tiny_patch4_group7_224] (save_rpb_weight.py 105): INFO _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.rpb.relative_position_bias_table', 'layers.1.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.1.attn.rpb.relative_position_bias_table', 'layers.2.blocks.2.attn.rpb.relative_position_bias_table', 'layers.2.blocks.3.attn.rpb.relative_position_bias_table', 'layers.2.blocks.4.attn.rpb.relative_position_bias_table', 'layers.2.blocks.5.attn.rpb.relative_position_bias_table', 'layers.2.blocks.6.attn.rpb.relative_position_bias_table', 'layers.2.blocks.7.attn.rpb.relative_position_bias_table', 'layers.3.blocks.0.attn.rpb.relative_position_bias_table', 'layers.3.blocks.1.attn.rpb.relative_position_bias_table', 'layers.3.blocks.2.attn.rpb.relative_position_bias_table', 'layers.3.blocks.3.attn.rpb.relative_position_bias_table', 'layers.3.blocks.4.attn.rpb.relative_position_bias_table', 'layers.3.blocks.5.attn.rpb.relative_position_bias_table'], unexpected_keys=['layers.0.blocks.0.attn.biases', 'layers.0.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.attn.biases', 'layers.1.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.attn.biases', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.biases', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.biases', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.biases', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.biases', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.biases', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.biases', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.biases', 'layers.2.blocks.7.attn.relative_position_index', 'layers.3.blocks.0.attn.biases', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.biases', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.2.attn.biases', 'layers.3.blocks.2.attn.relative_position_index', 'layers.3.blocks.3.attn.biases', 'layers.3.blocks.3.attn.relative_position_index', 'layers.3.blocks.4.attn.biases', 'layers.3.blocks.4.attn.relative_position_index', 'layers.3.blocks.5.attn.biases', 'layers.3.blocks.5.attn.relative_position_index'])
[2024-11-10 18:42:34 cros_tiny_patch4_group7_224] (save_rpb_weight.py 108): INFO => loaded successfully './model_ckpt/crossformer-t.pth'
[2024-11-10 18:45:52 cros_tiny_patch4_group7_224] (main.py 386): INFO Full config saved to output/log/debug/config.json
[2024-11-10 18:45:52 cros_tiny_patch4_group7_224] (main.py 389): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: false
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/crossformer-t.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 18:46:04 cros_tiny_patch4_group7_224] (main.py 94): INFO Creating model:cross-scale/cros_tiny_patch4_group7_224
[2024-11-10 18:46:07 cros_tiny_patch4_group7_224] (main.py 97): INFO CrossFormer(
  (patch_embed): PatchEmbed(
    (projs): ModuleList(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(4, 4))
      (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (2): Conv2d(3, 8, kernel_size=(16, 16), stride=(4, 4), padding=(6, 6))
      (3): Conv2d(3, 8, kernel_size=(32, 32), stride=(4, 4), padding=(14, 14))
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): Stage(
      dim=64, input_resolution=(56, 56), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=64, input_resolution=(56, 56), num_heads=2, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=8
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=64, group_size=(7, 7), num_heads=2
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=4, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=2, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=64, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=64
        (reductions): ModuleList(
          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Stage(
      dim=128, input_resolution=(28, 28), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=128, input_resolution=(28, 28), num_heads=4, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=4
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=128, group_size=(7, 7), num_heads=4
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=8, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=4, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.007)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=128
        (reductions): ModuleList(
          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): Stage(
      dim=256, input_resolution=(14, 14), depth=8
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.020)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.047)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.060)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=256
        (reductions): ModuleList(
          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): Stage(
      dim=512, input_resolution=(7, 7), depth=6
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=512, out_features=1000, bias=True)
)
[2024-11-10 18:46:07 cros_tiny_patch4_group7_224] (main.py 107): INFO number of params: 27804848
[2024-11-10 18:46:07 cros_tiny_patch4_group7_224] (main.py 110): INFO number of GFLOPs: 2.858292352
[2024-11-10 18:46:08 cros_tiny_patch4_group7_224] (utils.py 42): INFO ==============> Resuming from ./model_ckpt/crossformer-t.pth....................
[2024-11-10 18:46:08 cros_tiny_patch4_group7_224] (utils.py 49): INFO _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.rpb.relative_position_bias_table', 'layers.1.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.1.attn.rpb.relative_position_bias_table', 'layers.2.blocks.2.attn.rpb.relative_position_bias_table', 'layers.2.blocks.3.attn.rpb.relative_position_bias_table', 'layers.2.blocks.4.attn.rpb.relative_position_bias_table', 'layers.2.blocks.5.attn.rpb.relative_position_bias_table', 'layers.2.blocks.6.attn.rpb.relative_position_bias_table', 'layers.2.blocks.7.attn.rpb.relative_position_bias_table', 'layers.3.blocks.0.attn.rpb.relative_position_bias_table', 'layers.3.blocks.1.attn.rpb.relative_position_bias_table', 'layers.3.blocks.2.attn.rpb.relative_position_bias_table', 'layers.3.blocks.3.attn.rpb.relative_position_bias_table', 'layers.3.blocks.4.attn.rpb.relative_position_bias_table', 'layers.3.blocks.5.attn.rpb.relative_position_bias_table'], unexpected_keys=['layers.0.blocks.0.attn.biases', 'layers.0.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.attn.biases', 'layers.1.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.attn.biases', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.biases', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.biases', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.biases', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.biases', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.biases', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.biases', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.biases', 'layers.2.blocks.7.attn.relative_position_index', 'layers.3.blocks.0.attn.biases', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.biases', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.2.attn.biases', 'layers.3.blocks.2.attn.relative_position_index', 'layers.3.blocks.3.attn.biases', 'layers.3.blocks.3.attn.relative_position_index', 'layers.3.blocks.4.attn.biases', 'layers.3.blocks.4.attn.relative_position_index', 'layers.3.blocks.5.attn.biases', 'layers.3.blocks.5.attn.relative_position_index'])
[2024-11-10 18:49:06 cros_tiny_patch4_group7_224] (main.py 386): INFO Full config saved to output/log/debug/config.json
[2024-11-10 18:49:06 cros_tiny_patch4_group7_224] (main.py 389): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: false
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/crossformer-t.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 18:49:18 cros_tiny_patch4_group7_224] (main.py 94): INFO Creating model:cross-scale/cros_tiny_patch4_group7_224
[2024-11-10 18:49:22 cros_tiny_patch4_group7_224] (main.py 97): INFO CrossFormer(
  (patch_embed): PatchEmbed(
    (projs): ModuleList(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(4, 4))
      (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (2): Conv2d(3, 8, kernel_size=(16, 16), stride=(4, 4), padding=(6, 6))
      (3): Conv2d(3, 8, kernel_size=(32, 32), stride=(4, 4), padding=(14, 14))
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): Stage(
      dim=64, input_resolution=(56, 56), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=64, input_resolution=(56, 56), num_heads=2, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=8
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=64, group_size=(7, 7), num_heads=2
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=4, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=2, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=64, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=64
        (reductions): ModuleList(
          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Stage(
      dim=128, input_resolution=(28, 28), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=128, input_resolution=(28, 28), num_heads=4, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=4
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=128, group_size=(7, 7), num_heads=4
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=8, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=4, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.007)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=128
        (reductions): ModuleList(
          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): Stage(
      dim=256, input_resolution=(14, 14), depth=8
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.020)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.047)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.060)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=256
        (reductions): ModuleList(
          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): Stage(
      dim=512, input_resolution=(7, 7), depth=6
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=512, out_features=1000, bias=True)
)
[2024-11-10 18:49:22 cros_tiny_patch4_group7_224] (main.py 107): INFO number of params: 27804848
[2024-11-10 18:49:22 cros_tiny_patch4_group7_224] (main.py 110): INFO number of GFLOPs: 2.858292352
[2024-11-10 18:49:23 cros_tiny_patch4_group7_224] (utils.py 42): INFO ==============> Resuming from ./model_ckpt/crossformer-t.pth....................
[2024-11-10 18:49:24 cros_tiny_patch4_group7_224] (utils.py 49): INFO _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.rpb.relative_position_bias_table', 'layers.1.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.1.attn.rpb.relative_position_bias_table', 'layers.2.blocks.2.attn.rpb.relative_position_bias_table', 'layers.2.blocks.3.attn.rpb.relative_position_bias_table', 'layers.2.blocks.4.attn.rpb.relative_position_bias_table', 'layers.2.blocks.5.attn.rpb.relative_position_bias_table', 'layers.2.blocks.6.attn.rpb.relative_position_bias_table', 'layers.2.blocks.7.attn.rpb.relative_position_bias_table', 'layers.3.blocks.0.attn.rpb.relative_position_bias_table', 'layers.3.blocks.1.attn.rpb.relative_position_bias_table', 'layers.3.blocks.2.attn.rpb.relative_position_bias_table', 'layers.3.blocks.3.attn.rpb.relative_position_bias_table', 'layers.3.blocks.4.attn.rpb.relative_position_bias_table', 'layers.3.blocks.5.attn.rpb.relative_position_bias_table'], unexpected_keys=['layers.0.blocks.0.attn.biases', 'layers.0.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.attn.biases', 'layers.1.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.attn.biases', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.biases', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.biases', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.biases', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.biases', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.biases', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.biases', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.biases', 'layers.2.blocks.7.attn.relative_position_index', 'layers.3.blocks.0.attn.biases', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.biases', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.2.attn.biases', 'layers.3.blocks.2.attn.relative_position_index', 'layers.3.blocks.3.attn.biases', 'layers.3.blocks.3.attn.relative_position_index', 'layers.3.blocks.4.attn.biases', 'layers.3.blocks.4.attn.relative_position_index', 'layers.3.blocks.5.attn.biases', 'layers.3.blocks.5.attn.relative_position_index'])
[2024-11-10 18:49:31 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [0/391]	Time 4.614 (4.614)	Loss 2.3144 (2.3144)	Epoch 0	Acc@1 55.469 (55.469)	Acc@5 75.000 (75.000)	Mem 2725MB
[2024-11-10 18:50:55 cros_tiny_patch4_group7_224] (save_rpb_weight.py 158): INFO Full config saved to output/log/debug/config.json
[2024-11-10 18:50:55 cros_tiny_patch4_group7_224] (save_rpb_weight.py 161): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: true
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/crossformer-t.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0005
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 5.0e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 5.0e-07
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 18:50:55 cros_tiny_patch4_group7_224] (save_rpb_weight.py 92): INFO Creating model:cross-scale/cros_tiny_patch4_group7_224
[2024-11-10 18:51:37 cros_tiny_patch4_group7_224] (main_save.py 357): INFO Full config saved to output/log/debug/config.json
[2024-11-10 18:51:37 cros_tiny_patch4_group7_224] (main_save.py 360): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: true
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/crossformer-t.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 18:51:47 cros_tiny_patch4_group7_224] (main_save.py 94): INFO Creating model:cross-scale/cros_tiny_patch4_group7_224
[2024-11-10 18:51:48 cros_tiny_patch4_group7_224] (main_save.py 97): INFO CrossFormer(
  (patch_embed): PatchEmbed(
    (projs): ModuleList(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(4, 4))
      (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (2): Conv2d(3, 8, kernel_size=(16, 16), stride=(4, 4), padding=(6, 6))
      (3): Conv2d(3, 8, kernel_size=(32, 32), stride=(4, 4), padding=(14, 14))
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): Stage(
      dim=64, input_resolution=(56, 56), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=64, input_resolution=(56, 56), num_heads=2, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=8
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=64, group_size=(7, 7), num_heads=2
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=4, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=2, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=64, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=64
        (reductions): ModuleList(
          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Stage(
      dim=128, input_resolution=(28, 28), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=128, input_resolution=(28, 28), num_heads=4, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=4
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=128, group_size=(7, 7), num_heads=4
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=8, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=4, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.007)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=128
        (reductions): ModuleList(
          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): Stage(
      dim=256, input_resolution=(14, 14), depth=8
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.020)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.047)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.060)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=256
        (reductions): ModuleList(
          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): Stage(
      dim=512, input_resolution=(7, 7), depth=6
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=512, out_features=1000, bias=True)
)
[2024-11-10 18:51:48 cros_tiny_patch4_group7_224] (main_save.py 107): INFO number of params: 27804848
[2024-11-10 18:51:48 cros_tiny_patch4_group7_224] (main_save.py 110): INFO number of GFLOPs: 2.858292352
[2024-11-10 18:51:48 cros_tiny_patch4_group7_224] (utils.py 42): INFO ==============> Resuming from ./model_ckpt/crossformer-t.pth....................
[2024-11-10 18:51:48 cros_tiny_patch4_group7_224] (utils.py 49): INFO _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.rpb.relative_position_bias_table', 'layers.1.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.1.attn.rpb.relative_position_bias_table', 'layers.2.blocks.2.attn.rpb.relative_position_bias_table', 'layers.2.blocks.3.attn.rpb.relative_position_bias_table', 'layers.2.blocks.4.attn.rpb.relative_position_bias_table', 'layers.2.blocks.5.attn.rpb.relative_position_bias_table', 'layers.2.blocks.6.attn.rpb.relative_position_bias_table', 'layers.2.blocks.7.attn.rpb.relative_position_bias_table', 'layers.3.blocks.0.attn.rpb.relative_position_bias_table', 'layers.3.blocks.1.attn.rpb.relative_position_bias_table', 'layers.3.blocks.2.attn.rpb.relative_position_bias_table', 'layers.3.blocks.3.attn.rpb.relative_position_bias_table', 'layers.3.blocks.4.attn.rpb.relative_position_bias_table', 'layers.3.blocks.5.attn.rpb.relative_position_bias_table'], unexpected_keys=['layers.0.blocks.0.attn.biases', 'layers.0.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.attn.biases', 'layers.1.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.attn.biases', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.biases', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.biases', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.biases', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.biases', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.biases', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.biases', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.biases', 'layers.2.blocks.7.attn.relative_position_index', 'layers.3.blocks.0.attn.biases', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.biases', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.2.attn.biases', 'layers.3.blocks.2.attn.relative_position_index', 'layers.3.blocks.3.attn.biases', 'layers.3.blocks.3.attn.relative_position_index', 'layers.3.blocks.4.attn.biases', 'layers.3.blocks.4.attn.relative_position_index', 'layers.3.blocks.5.attn.biases', 'layers.3.blocks.5.attn.relative_position_index'])
[2024-11-10 18:54:24 cros_tiny_patch4_group7_224] (save_rpb_weight.py 158): INFO Full config saved to output/log/debug/config.json
[2024-11-10 18:54:24 cros_tiny_patch4_group7_224] (save_rpb_weight.py 161): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: true
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/crossformer-t.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.0005
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 5.0e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 5.0e-07
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 18:54:26 cros_tiny_patch4_group7_224] (save_rpb_weight.py 92): INFO Creating model:cross-scale/cros_tiny_patch4_group7_224
[2024-11-10 18:54:28 cros_tiny_patch4_group7_224] (save_rpb_weight.py 95): INFO CrossFormer(
  (patch_embed): PatchEmbed(
    (projs): ModuleList(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(4, 4))
      (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (2): Conv2d(3, 8, kernel_size=(16, 16), stride=(4, 4), padding=(6, 6))
      (3): Conv2d(3, 8, kernel_size=(32, 32), stride=(4, 4), padding=(14, 14))
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): Stage(
      dim=64, input_resolution=(56, 56), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=64, input_resolution=(56, 56), num_heads=2, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=8
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=64, group_size=(7, 7), num_heads=2
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=4, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=2, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=64, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=64
        (reductions): ModuleList(
          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Stage(
      dim=128, input_resolution=(28, 28), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=128, input_resolution=(28, 28), num_heads=4, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=4
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=128, group_size=(7, 7), num_heads=4
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=8, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=4, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.007)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=128
        (reductions): ModuleList(
          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): Stage(
      dim=256, input_resolution=(14, 14), depth=8
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.020)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.047)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.060)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=256
        (reductions): ModuleList(
          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): Stage(
      dim=512, input_resolution=(7, 7), depth=6
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=512, out_features=1000, bias=True)
)
[2024-11-10 18:54:28 cros_tiny_patch4_group7_224] (save_rpb_weight.py 98): INFO number of params: 27804848
[2024-11-10 18:54:28 cros_tiny_patch4_group7_224] (save_rpb_weight.py 105): INFO _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.rpb.relative_position_bias_table', 'layers.1.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.1.attn.rpb.relative_position_bias_table', 'layers.2.blocks.2.attn.rpb.relative_position_bias_table', 'layers.2.blocks.3.attn.rpb.relative_position_bias_table', 'layers.2.blocks.4.attn.rpb.relative_position_bias_table', 'layers.2.blocks.5.attn.rpb.relative_position_bias_table', 'layers.2.blocks.6.attn.rpb.relative_position_bias_table', 'layers.2.blocks.7.attn.rpb.relative_position_bias_table', 'layers.3.blocks.0.attn.rpb.relative_position_bias_table', 'layers.3.blocks.1.attn.rpb.relative_position_bias_table', 'layers.3.blocks.2.attn.rpb.relative_position_bias_table', 'layers.3.blocks.3.attn.rpb.relative_position_bias_table', 'layers.3.blocks.4.attn.rpb.relative_position_bias_table', 'layers.3.blocks.5.attn.rpb.relative_position_bias_table'], unexpected_keys=['layers.0.blocks.0.attn.biases', 'layers.0.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.attn.biases', 'layers.1.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.attn.biases', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.biases', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.biases', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.biases', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.biases', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.biases', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.biases', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.biases', 'layers.2.blocks.7.attn.relative_position_index', 'layers.3.blocks.0.attn.biases', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.biases', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.2.attn.biases', 'layers.3.blocks.2.attn.relative_position_index', 'layers.3.blocks.3.attn.biases', 'layers.3.blocks.3.attn.relative_position_index', 'layers.3.blocks.4.attn.biases', 'layers.3.blocks.4.attn.relative_position_index', 'layers.3.blocks.5.attn.biases', 'layers.3.blocks.5.attn.relative_position_index'])
[2024-11-10 18:54:28 cros_tiny_patch4_group7_224] (save_rpb_weight.py 108): INFO => loaded successfully './model_ckpt/crossformer-t.pth'
[2024-11-10 18:56:19 cros_tiny_patch4_group7_224] (main.py 386): INFO Full config saved to output/log/debug/config.json
[2024-11-10 18:56:19 cros_tiny_patch4_group7_224] (main.py 389): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: false
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/crossformer-t.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 18:56:45 cros_tiny_patch4_group7_224] (main.py 386): INFO Full config saved to output/log/debug/config.json
[2024-11-10 18:56:45 cros_tiny_patch4_group7_224] (main.py 389): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: false
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/cros_tiny_patch4_group7_224_rpb.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 18:57:02 cros_tiny_patch4_group7_224] (main.py 94): INFO Creating model:cross-scale/cros_tiny_patch4_group7_224
[2024-11-10 18:57:04 cros_tiny_patch4_group7_224] (main.py 97): INFO CrossFormer(
  (patch_embed): PatchEmbed(
    (projs): ModuleList(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(4, 4))
      (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (2): Conv2d(3, 8, kernel_size=(16, 16), stride=(4, 4), padding=(6, 6))
      (3): Conv2d(3, 8, kernel_size=(32, 32), stride=(4, 4), padding=(14, 14))
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): Stage(
      dim=64, input_resolution=(56, 56), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=64, input_resolution=(56, 56), num_heads=2, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=8
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=64, group_size=(7, 7), num_heads=2
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=4, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=2, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=64, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=64
        (reductions): ModuleList(
          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Stage(
      dim=128, input_resolution=(28, 28), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=128, input_resolution=(28, 28), num_heads=4, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=4
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=128, group_size=(7, 7), num_heads=4
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=8, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=4, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.007)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=128
        (reductions): ModuleList(
          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): Stage(
      dim=256, input_resolution=(14, 14), depth=8
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.020)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.047)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.060)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=256
        (reductions): ModuleList(
          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): Stage(
      dim=512, input_resolution=(7, 7), depth=6
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=512, out_features=1000, bias=True)
)
[2024-11-10 18:57:04 cros_tiny_patch4_group7_224] (main.py 107): INFO number of params: 27804848
[2024-11-10 18:57:04 cros_tiny_patch4_group7_224] (main.py 110): INFO number of GFLOPs: 2.858292352
[2024-11-10 18:57:11 cros_tiny_patch4_group7_224] (utils.py 42): INFO ==============> Resuming from ./model_ckpt/cros_tiny_patch4_group7_224_rpb.pth....................
[2024-11-10 19:00:01 cros_tiny_patch4_group7_224] (utils.py 49): INFO <All keys matched successfully>
[2024-11-10 19:00:12 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [0/391]	Time 4.927 (4.927)	Loss 0.9882 (0.9882)	Epoch 0	Acc@1 78.125 (78.125)	Acc@5 94.531 (94.531)	Mem 2725MB
[2024-11-10 19:00:23 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [50/391]	Time 0.198 (0.319)	Loss 0.8524 (0.8499)	Epoch 0	Acc@1 82.812 (81.664)	Acc@5 95.312 (95.358)	Mem 2725MB
[2024-11-10 19:01:56 cros_tiny_patch4_group7_224] (main.py 386): INFO Full config saved to output/log/debug/config.json
[2024-11-10 19:01:56 cros_tiny_patch4_group7_224] (main.py 389): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: false
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/cros_tiny_patch4_group7_224_rpb.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 19:01:59 cros_tiny_patch4_group7_224] (main.py 94): INFO Creating model:cross-scale/cros_tiny_patch4_group7_224
[2024-11-10 19:02:00 cros_tiny_patch4_group7_224] (main.py 97): INFO CrossFormer(
  (patch_embed): PatchEmbed(
    (projs): ModuleList(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(4, 4))
      (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (2): Conv2d(3, 8, kernel_size=(16, 16), stride=(4, 4), padding=(6, 6))
      (3): Conv2d(3, 8, kernel_size=(32, 32), stride=(4, 4), padding=(14, 14))
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): Stage(
      dim=64, input_resolution=(56, 56), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=64, input_resolution=(56, 56), num_heads=2, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=8
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=64, group_size=(7, 7), num_heads=2
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=4, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=2, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=64, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=64
        (reductions): ModuleList(
          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Stage(
      dim=128, input_resolution=(28, 28), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=128, input_resolution=(28, 28), num_heads=4, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=4
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=128, group_size=(7, 7), num_heads=4
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=8, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=4, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.007)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=128
        (reductions): ModuleList(
          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): Stage(
      dim=256, input_resolution=(14, 14), depth=8
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.020)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.047)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.060)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=256
        (reductions): ModuleList(
          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): Stage(
      dim=512, input_resolution=(7, 7), depth=6
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=512, out_features=1000, bias=True)
)
[2024-11-10 19:02:00 cros_tiny_patch4_group7_224] (main.py 107): INFO number of params: 27804848
[2024-11-10 19:02:00 cros_tiny_patch4_group7_224] (main.py 110): INFO number of GFLOPs: 2.858292352
[2024-11-10 19:02:00 cros_tiny_patch4_group7_224] (utils.py 42): INFO ==============> Resuming from ./model_ckpt/cros_tiny_patch4_group7_224_rpb.pth....................
[2024-11-10 19:02:00 cros_tiny_patch4_group7_224] (utils.py 49): INFO <All keys matched successfully>
[2024-11-10 19:02:05 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [0/391]	Time 5.009 (5.009)	Loss 0.9882 (0.9882)	Epoch 0	Acc@1 78.125 (78.125)	Acc@5 94.531 (94.531)	Mem 2725MB
[2024-11-10 19:02:25 cros_tiny_patch4_group7_224] (main.py 386): INFO Full config saved to output/log/debug/config.json
[2024-11-10 19:02:25 cros_tiny_patch4_group7_224] (main.py 389): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: false
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/cros_tiny_patch4_group7_224_rpb.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 19:02:28 cros_tiny_patch4_group7_224] (main.py 94): INFO Creating model:cross-scale/cros_tiny_patch4_group7_224
[2024-11-10 19:02:29 cros_tiny_patch4_group7_224] (main.py 97): INFO CrossFormer(
  (patch_embed): PatchEmbed(
    (projs): ModuleList(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(4, 4))
      (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (2): Conv2d(3, 8, kernel_size=(16, 16), stride=(4, 4), padding=(6, 6))
      (3): Conv2d(3, 8, kernel_size=(32, 32), stride=(4, 4), padding=(14, 14))
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): Stage(
      dim=64, input_resolution=(56, 56), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=64, input_resolution=(56, 56), num_heads=2, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=8
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=64, group_size=(7, 7), num_heads=2
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=4, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=2, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=64, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=64
        (reductions): ModuleList(
          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Stage(
      dim=128, input_resolution=(28, 28), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=128, input_resolution=(28, 28), num_heads=4, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=4
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=128, group_size=(7, 7), num_heads=4
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=8, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=4, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.007)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=128
        (reductions): ModuleList(
          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): Stage(
      dim=256, input_resolution=(14, 14), depth=8
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.020)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.047)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.060)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=256
        (reductions): ModuleList(
          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): Stage(
      dim=512, input_resolution=(7, 7), depth=6
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=512, out_features=1000, bias=True)
)
[2024-11-10 19:02:29 cros_tiny_patch4_group7_224] (main.py 107): INFO number of params: 27804848
[2024-11-10 19:02:29 cros_tiny_patch4_group7_224] (main.py 110): INFO number of GFLOPs: 2.858292352
[2024-11-10 19:02:29 cros_tiny_patch4_group7_224] (utils.py 42): INFO ==============> Resuming from ./model_ckpt/cros_tiny_patch4_group7_224_rpb.pth....................
[2024-11-10 19:02:29 cros_tiny_patch4_group7_224] (utils.py 49): INFO <All keys matched successfully>
[2024-11-10 19:02:34 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [0/391]	Time 4.971 (4.971)	Loss 0.9882 (0.9882)	Epoch 0	Acc@1 78.125 (78.125)	Acc@5 94.531 (94.531)	Mem 2725MB
[2024-11-10 19:02:41 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [50/391]	Time 0.111 (0.238)	Loss 0.8524 (0.8499)	Epoch 0	Acc@1 82.812 (81.664)	Acc@5 95.312 (95.358)	Mem 2725MB
[2024-11-10 19:02:50 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [100/391]	Time 0.137 (0.202)	Loss 0.8090 (0.8467)	Epoch 0	Acc@1 83.594 (81.598)	Acc@5 95.312 (95.421)	Mem 2725MB
[2024-11-10 19:02:58 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [150/391]	Time 0.136 (0.192)	Loss 0.7289 (0.8392)	Epoch 0	Acc@1 84.375 (81.835)	Acc@5 96.875 (95.483)	Mem 2725MB
[2024-11-10 19:03:07 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [200/391]	Time 0.112 (0.186)	Loss 0.8754 (0.8371)	Epoch 0	Acc@1 79.688 (81.697)	Acc@5 96.094 (95.596)	Mem 2725MB
[2024-11-10 19:03:15 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [250/391]	Time 0.113 (0.182)	Loss 0.8395 (0.8355)	Epoch 0	Acc@1 81.250 (81.667)	Acc@5 95.312 (95.642)	Mem 2725MB
[2024-11-10 19:03:24 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [300/391]	Time 0.123 (0.181)	Loss 0.7842 (0.8380)	Epoch 0	Acc@1 83.594 (81.587)	Acc@5 93.750 (95.629)	Mem 2725MB
[2024-11-10 19:03:32 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [350/391]	Time 0.113 (0.180)	Loss 0.8881 (0.8411)	Epoch 0	Acc@1 84.375 (81.517)	Acc@5 94.531 (95.531)	Mem 2725MB
[2024-11-10 19:03:39 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [390/391]	Time 0.262 (0.178)	Loss 0.6126 (0.8402)	Epoch 0	Acc@1 87.500 (81.518)	Acc@5 100.000 (95.526)	Mem 2725MB
[2024-11-10 19:03:39 cros_tiny_patch4_group7_224] (main.py 317): INFO  * Acc@1 81.518 Acc@5 95.526
[2024-11-10 19:03:39 cros_tiny_patch4_group7_224] (main.py 127): INFO Accuracy of the network on the 50000 test images: 81.5%
[2024-11-10 19:05:34 cros_tiny_patch4_group7_224] (main.py 386): INFO Full config saved to output/log/debug/config.json
[2024-11-10 19:05:34 cros_tiny_patch4_group7_224] (main.py 389): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: true
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/cros_tiny_patch4_group7_224_rpb.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 19:05:37 cros_tiny_patch4_group7_224] (main.py 94): INFO Creating model:cross-scale/cros_tiny_patch4_group7_224
[2024-11-10 19:05:38 cros_tiny_patch4_group7_224] (main.py 97): INFO CrossFormer(
  (patch_embed): PatchEmbed(
    (projs): ModuleList(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(4, 4))
      (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (2): Conv2d(3, 8, kernel_size=(16, 16), stride=(4, 4), padding=(6, 6))
      (3): Conv2d(3, 8, kernel_size=(32, 32), stride=(4, 4), padding=(14, 14))
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): Stage(
      dim=64, input_resolution=(56, 56), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=64, input_resolution=(56, 56), num_heads=2, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=8
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=64, group_size=(7, 7), num_heads=2
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=4, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=2, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=64, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=64
        (reductions): ModuleList(
          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Stage(
      dim=128, input_resolution=(28, 28), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=128, input_resolution=(28, 28), num_heads=4, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=4
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=128, group_size=(7, 7), num_heads=4
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=8, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=4, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.007)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=128
        (reductions): ModuleList(
          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): Stage(
      dim=256, input_resolution=(14, 14), depth=8
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.020)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.047)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.060)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=256
        (reductions): ModuleList(
          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): Stage(
      dim=512, input_resolution=(7, 7), depth=6
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=512, out_features=1000, bias=True)
)
[2024-11-10 19:05:38 cros_tiny_patch4_group7_224] (main.py 107): INFO number of params: 27804848
[2024-11-10 19:05:38 cros_tiny_patch4_group7_224] (main.py 110): INFO number of GFLOPs: 2.858292352
[2024-11-10 19:05:38 cros_tiny_patch4_group7_224] (utils.py 42): INFO ==============> Resuming from ./model_ckpt/cros_tiny_patch4_group7_224_rpb.pth....................
[2024-11-10 19:05:38 cros_tiny_patch4_group7_224] (utils.py 49): INFO <All keys matched successfully>
[2024-11-10 19:05:43 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [0/391]	Time 4.943 (4.943)	Loss 0.9881 (0.9881)	Epoch 0	Acc@1 78.125 (78.125)	Acc@5 94.531 (94.531)	Mem 2725MB
[2024-11-10 19:05:50 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [50/391]	Time 0.122 (0.238)	Loss 0.8524 (0.8499)	Epoch 0	Acc@1 82.812 (81.664)	Acc@5 95.312 (95.374)	Mem 2725MB
[2024-11-10 19:05:58 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [100/391]	Time 0.111 (0.202)	Loss 0.8090 (0.8467)	Epoch 0	Acc@1 83.594 (81.590)	Acc@5 95.312 (95.429)	Mem 2725MB
[2024-11-10 19:06:07 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [150/391]	Time 0.111 (0.191)	Loss 0.7287 (0.8392)	Epoch 0	Acc@1 84.375 (81.829)	Acc@5 96.875 (95.499)	Mem 2725MB
[2024-11-10 19:06:15 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [200/391]	Time 0.112 (0.184)	Loss 0.8754 (0.8370)	Epoch 0	Acc@1 79.688 (81.693)	Acc@5 96.094 (95.608)	Mem 2725MB
[2024-11-10 19:06:23 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [250/391]	Time 0.111 (0.181)	Loss 0.8395 (0.8355)	Epoch 0	Acc@1 81.250 (81.667)	Acc@5 95.312 (95.652)	Mem 2725MB
[2024-11-10 19:06:32 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [300/391]	Time 0.110 (0.178)	Loss 0.7842 (0.8380)	Epoch 0	Acc@1 83.594 (81.590)	Acc@5 93.750 (95.634)	Mem 2725MB
[2024-11-10 19:06:40 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [350/391]	Time 0.111 (0.177)	Loss 0.8882 (0.8411)	Epoch 0	Acc@1 84.375 (81.517)	Acc@5 94.531 (95.533)	Mem 2725MB
[2024-11-10 19:06:47 cros_tiny_patch4_group7_224] (main.py 309): INFO Test: [390/391]	Time 0.257 (0.176)	Loss 0.6124 (0.8402)	Epoch 0	Acc@1 87.500 (81.516)	Acc@5 100.000 (95.528)	Mem 2725MB
[2024-11-10 19:06:47 cros_tiny_patch4_group7_224] (main.py 317): INFO  * Acc@1 81.516 Acc@5 95.528
[2024-11-10 19:06:47 cros_tiny_patch4_group7_224] (main.py 127): INFO Accuracy of the network on the 50000 test images: 81.5%
[2024-11-10 19:09:05 cros_tiny_patch4_group7_224] (main.py 389): INFO Full config saved to output/log/debug/config.json
[2024-11-10 19:09:05 cros_tiny_patch4_group7_224] (main.py 392): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: true
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/cros_tiny_patch4_group7_224_rpb.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 19:09:20 cros_tiny_patch4_group7_224] (main.py 389): INFO Full config saved to output/log/debug/config.json
[2024-11-10 19:09:20 cros_tiny_patch4_group7_224] (main.py 392): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: true
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/crossformer-t.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 19:09:23 cros_tiny_patch4_group7_224] (main.py 94): INFO Creating model:cross-scale/cros_tiny_patch4_group7_224
[2024-11-10 19:09:24 cros_tiny_patch4_group7_224] (main.py 97): INFO CrossFormer(
  (patch_embed): PatchEmbed(
    (projs): ModuleList(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(4, 4))
      (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (2): Conv2d(3, 8, kernel_size=(16, 16), stride=(4, 4), padding=(6, 6))
      (3): Conv2d(3, 8, kernel_size=(32, 32), stride=(4, 4), padding=(14, 14))
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): Stage(
      dim=64, input_resolution=(56, 56), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=64, input_resolution=(56, 56), num_heads=2, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=8
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=64, group_size=(7, 7), num_heads=2
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=4, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=2, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=64, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=64
        (reductions): ModuleList(
          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Stage(
      dim=128, input_resolution=(28, 28), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=128, input_resolution=(28, 28), num_heads=4, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=4
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=128, group_size=(7, 7), num_heads=4
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=8, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=4, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.007)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=128
        (reductions): ModuleList(
          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): Stage(
      dim=256, input_resolution=(14, 14), depth=8
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.020)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.047)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.060)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=256
        (reductions): ModuleList(
          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): Stage(
      dim=512, input_resolution=(7, 7), depth=6
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=512, out_features=1000, bias=True)
)
[2024-11-10 19:09:24 cros_tiny_patch4_group7_224] (main.py 107): INFO number of params: 27804848
[2024-11-10 19:09:24 cros_tiny_patch4_group7_224] (main.py 110): INFO number of GFLOPs: 2.858292352
[2024-11-10 19:09:24 cros_tiny_patch4_group7_224] (utils.py 42): INFO ==============> Resuming from ./model_ckpt/crossformer-t.pth....................
[2024-11-10 19:09:24 cros_tiny_patch4_group7_224] (utils.py 49): INFO _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.rpb.relative_position_bias_table', 'layers.1.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.1.attn.rpb.relative_position_bias_table', 'layers.2.blocks.2.attn.rpb.relative_position_bias_table', 'layers.2.blocks.3.attn.rpb.relative_position_bias_table', 'layers.2.blocks.4.attn.rpb.relative_position_bias_table', 'layers.2.blocks.5.attn.rpb.relative_position_bias_table', 'layers.2.blocks.6.attn.rpb.relative_position_bias_table', 'layers.2.blocks.7.attn.rpb.relative_position_bias_table', 'layers.3.blocks.0.attn.rpb.relative_position_bias_table', 'layers.3.blocks.1.attn.rpb.relative_position_bias_table', 'layers.3.blocks.2.attn.rpb.relative_position_bias_table', 'layers.3.blocks.3.attn.rpb.relative_position_bias_table', 'layers.3.blocks.4.attn.rpb.relative_position_bias_table', 'layers.3.blocks.5.attn.rpb.relative_position_bias_table'], unexpected_keys=['layers.0.blocks.0.attn.biases', 'layers.0.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.attn.biases', 'layers.1.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.attn.biases', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.biases', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.biases', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.biases', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.biases', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.biases', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.biases', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.biases', 'layers.2.blocks.7.attn.relative_position_index', 'layers.3.blocks.0.attn.biases', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.biases', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.2.attn.biases', 'layers.3.blocks.2.attn.relative_position_index', 'layers.3.blocks.3.attn.biases', 'layers.3.blocks.3.attn.relative_position_index', 'layers.3.blocks.4.attn.biases', 'layers.3.blocks.4.attn.relative_position_index', 'layers.3.blocks.5.attn.biases', 'layers.3.blocks.5.attn.relative_position_index'])
[2024-11-10 19:10:07 cros_tiny_patch4_group7_224] (main.py 389): INFO Full config saved to output/log/debug/config.json
[2024-11-10 19:10:07 cros_tiny_patch4_group7_224] (main.py 392): INFO AMP_OPT_LEVEL: native
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  CLS_WEIGHT: 1.0
  DATASET: imagenet
  DATA_PATH: /home1/yanweicai/DATA/tta/clip_based_adaptation/imagenet
  DENSE_WEIGHT: 0.5
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MIX_GROUNDTRUTH: false
  NUM_WORKERS: 8
  PIN_MEMORY: true
  PREFETCH: false
  TOKEN_LABEL: false
  TOKEN_LABEL_SIZE: 7
  ZIP_MODE: false
EVAL_MODE: true
LOCAL_RANK: 0
LOG_OUTPUT: output/log/debug
MODEL:
  CONV_BLOCKS: ''
  CROS:
    ADAPT_INTER: false
    APE: false
    DEPTHS:
    - 1
    - 1
    - 8
    - 6
    EMBED_DIM: 64
    GROUP_SIZE:
    - 7
    - 7
    - 7
    - 7
    GROUP_TYPE: constant
    INTERVAL:
    - 8
    - 4
    - 2
    - 1
    IN_CHANS: 3
    MERGE_SIZE:
    - - 2
      - 4
    - - 2
      - 4
    - - 2
      - 4
    MLP_RATIO:
    - 4.0
    - 4.0
    - 4.0
    - 4.0
    NO_MASK: false
    NUM_HEADS:
    - 2
    - 4
    - 8
    - 16
    PAD_TYPE: 0
    PATCH_NORM: true
    PATCH_SIZE:
    - 4
    - 8
    - 16
    - 32
    QKV_BIAS: true
    QK_SCALE: null
    USE_ACL: false
    USE_CPE: false
    USE_DPB: true
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  FROM_PRETRAIN: ''
  IMPL_TYPE: ''
  LABEL_SMOOTHING: 0.1
  LOSS:
    ALPHA2: 0.1
    ALPHA3: 0.1
    ALPHA4: 0.25
  MIX_TOKEN: true
  NAME: cros_tiny_patch4_group7_224
  NUM_CLASSES: 1000
  RESUME: ./model_ckpt/crossformer-t.pth
  RETURN_DENSE: true
  TYPE: cross-scale
OUTPUT: output
PRINT_FREQ: 50
SAVE_FREQ: 1000
SEED: 0
TAG: debug
TEST:
  CROP: true
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 0
  AUTO_RESUME: true
  BASE_LR: 0.000125
  CLIP_GRAD: 5.0
  EPOCHS: 300
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.25e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 20
  WARMUP_LR: 1.25e-07
  WEIGHT_DECAY: 0.05
WEIGHT_OUTPUT: output/weight/debug

[2024-11-10 19:10:10 cros_tiny_patch4_group7_224] (main.py 94): INFO Creating model:cross-scale/cros_tiny_patch4_group7_224
[2024-11-10 19:10:10 cros_tiny_patch4_group7_224] (main.py 97): INFO CrossFormer(
  (patch_embed): PatchEmbed(
    (projs): ModuleList(
      (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(4, 4))
      (1): Conv2d(3, 16, kernel_size=(8, 8), stride=(4, 4), padding=(2, 2))
      (2): Conv2d(3, 8, kernel_size=(16, 16), stride=(4, 4), padding=(6, 6))
      (3): Conv2d(3, 8, kernel_size=(32, 32), stride=(4, 4), padding=(14, 14))
    )
    (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): Stage(
      dim=64, input_resolution=(56, 56), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=64, input_resolution=(56, 56), num_heads=2, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=8
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=64, group_size=(7, 7), num_heads=2
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=4, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=4, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((4,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=4, out_features=2, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=64, out_features=192, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=64, out_features=64, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=64
        (reductions): ModuleList(
          (0): Conv2d(64, 64, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): Stage(
      dim=128, input_resolution=(28, 28), depth=1
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=128, input_resolution=(28, 28), num_heads=4, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=4
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=128, group_size=(7, 7), num_heads=4
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=8, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=8, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=8, out_features=4, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.007)
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=128
        (reductions): ModuleList(
          (0): Conv2d(128, 128, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): Stage(
      dim=256, input_resolution=(14, 14), depth=8
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.013)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.020)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.033)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.040)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.047)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.053)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): CrossFormerBlock(
          dim=256, input_resolution=(14, 14), num_heads=8, group_size=7, lsda_flag=1, mlp_ratio=4.0, interval=2
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=256, group_size=(7, 7), num_heads=8
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=16, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=16, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=16, out_features=8, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.060)
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=256
        (reductions): ModuleList(
          (0): Conv2d(256, 256, kernel_size=(2, 2), stride=(2, 2))
          (1): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
        )
        (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): Stage(
      dim=512, input_resolution=(7, 7), depth=6
      (blocks): ModuleList(
        (0): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.067)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.080)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.087)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.093)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): CrossFormerBlock(
          dim=512, input_resolution=(7, 7), num_heads=16, group_size=7, lsda_flag=0, mlp_ratio=4.0, interval=1
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): Attention(
            dim=512, group_size=(7, 7), num_heads=16
            (pos): DynamicPosBias(
              (pos_proj): Linear(in_features=2, out_features=32, bias=True)
              (pos1): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos2): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=32, bias=True)
              )
              (pos3): Sequential(
                (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)
                (1): ReLU(inplace=True)
                (2): Linear(in_features=32, out_features=16, bias=True)
              )
            )
            (rpb): RelativePosBias()
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate=none)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=512, out_features=1000, bias=True)
)
[2024-11-10 19:10:10 cros_tiny_patch4_group7_224] (main.py 107): INFO number of params: 27804848
[2024-11-10 19:10:10 cros_tiny_patch4_group7_224] (main.py 110): INFO number of GFLOPs: 2.858292352
[2024-11-10 19:10:10 cros_tiny_patch4_group7_224] (utils.py 42): INFO ==============> Resuming from ./model_ckpt/crossformer-t.pth....................
[2024-11-10 19:10:11 cros_tiny_patch4_group7_224] (utils.py 49): INFO _IncompatibleKeys(missing_keys=['layers.0.blocks.0.attn.rpb.relative_position_bias_table', 'layers.1.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.0.attn.rpb.relative_position_bias_table', 'layers.2.blocks.1.attn.rpb.relative_position_bias_table', 'layers.2.blocks.2.attn.rpb.relative_position_bias_table', 'layers.2.blocks.3.attn.rpb.relative_position_bias_table', 'layers.2.blocks.4.attn.rpb.relative_position_bias_table', 'layers.2.blocks.5.attn.rpb.relative_position_bias_table', 'layers.2.blocks.6.attn.rpb.relative_position_bias_table', 'layers.2.blocks.7.attn.rpb.relative_position_bias_table', 'layers.3.blocks.0.attn.rpb.relative_position_bias_table', 'layers.3.blocks.1.attn.rpb.relative_position_bias_table', 'layers.3.blocks.2.attn.rpb.relative_position_bias_table', 'layers.3.blocks.3.attn.rpb.relative_position_bias_table', 'layers.3.blocks.4.attn.rpb.relative_position_bias_table', 'layers.3.blocks.5.attn.rpb.relative_position_bias_table'], unexpected_keys=['layers.0.blocks.0.attn.biases', 'layers.0.blocks.0.attn.relative_position_index', 'layers.1.blocks.0.attn.biases', 'layers.1.blocks.0.attn.relative_position_index', 'layers.2.blocks.0.attn.biases', 'layers.2.blocks.0.attn.relative_position_index', 'layers.2.blocks.1.attn.biases', 'layers.2.blocks.1.attn.relative_position_index', 'layers.2.blocks.2.attn.biases', 'layers.2.blocks.2.attn.relative_position_index', 'layers.2.blocks.3.attn.biases', 'layers.2.blocks.3.attn.relative_position_index', 'layers.2.blocks.4.attn.biases', 'layers.2.blocks.4.attn.relative_position_index', 'layers.2.blocks.5.attn.biases', 'layers.2.blocks.5.attn.relative_position_index', 'layers.2.blocks.6.attn.biases', 'layers.2.blocks.6.attn.relative_position_index', 'layers.2.blocks.7.attn.biases', 'layers.2.blocks.7.attn.relative_position_index', 'layers.3.blocks.0.attn.biases', 'layers.3.blocks.0.attn.relative_position_index', 'layers.3.blocks.1.attn.biases', 'layers.3.blocks.1.attn.relative_position_index', 'layers.3.blocks.2.attn.biases', 'layers.3.blocks.2.attn.relative_position_index', 'layers.3.blocks.3.attn.biases', 'layers.3.blocks.3.attn.relative_position_index', 'layers.3.blocks.4.attn.biases', 'layers.3.blocks.4.attn.relative_position_index', 'layers.3.blocks.5.attn.biases', 'layers.3.blocks.5.attn.relative_position_index'])
[2024-11-10 19:10:15 cros_tiny_patch4_group7_224] (main.py 312): INFO Test: [0/391]	Time 4.664 (4.664)	Loss 0.9882 (0.9882)	Epoch 0	Acc@1 78.125 (78.125)	Acc@5 94.531 (94.531)	Mem 5091MB
[2024-11-10 19:10:22 cros_tiny_patch4_group7_224] (main.py 312): INFO Test: [50/391]	Time 0.140 (0.233)	Loss 0.8525 (0.8500)	Epoch 0	Acc@1 82.812 (81.648)	Acc@5 95.312 (95.374)	Mem 5091MB
[2024-11-10 19:10:31 cros_tiny_patch4_group7_224] (main.py 312): INFO Test: [100/391]	Time 0.142 (0.200)	Loss 0.8087 (0.8467)	Epoch 0	Acc@1 83.594 (81.575)	Acc@5 95.312 (95.413)	Mem 5091MB
[2024-11-10 19:10:39 cros_tiny_patch4_group7_224] (main.py 312): INFO Test: [150/391]	Time 0.142 (0.190)	Loss 0.7289 (0.8392)	Epoch 0	Acc@1 84.375 (81.819)	Acc@5 96.875 (95.473)	Mem 5091MB
[2024-11-10 19:10:48 cros_tiny_patch4_group7_224] (main.py 312): INFO Test: [200/391]	Time 0.143 (0.184)	Loss 0.8754 (0.8371)	Epoch 0	Acc@1 79.688 (81.693)	Acc@5 96.094 (95.585)	Mem 5091MB
[2024-11-10 19:10:56 cros_tiny_patch4_group7_224] (main.py 312): INFO Test: [250/391]	Time 0.143 (0.181)	Loss 0.8398 (0.8355)	Epoch 0	Acc@1 81.250 (81.670)	Acc@5 95.312 (95.633)	Mem 5091MB
[2024-11-10 19:11:05 cros_tiny_patch4_group7_224] (main.py 312): INFO Test: [300/391]	Time 0.143 (0.180)	Loss 0.7841 (0.8381)	Epoch 0	Acc@1 83.594 (81.587)	Acc@5 93.750 (95.621)	Mem 5091MB
[2024-11-10 19:11:13 cros_tiny_patch4_group7_224] (main.py 312): INFO Test: [350/391]	Time 0.143 (0.178)	Loss 0.8881 (0.8412)	Epoch 0	Acc@1 84.375 (81.522)	Acc@5 94.531 (95.524)	Mem 5091MB
[2024-11-10 19:11:20 cros_tiny_patch4_group7_224] (main.py 312): INFO Test: [390/391]	Time 0.192 (0.177)	Loss 0.6127 (0.8402)	Epoch 0	Acc@1 87.500 (81.522)	Acc@5 100.000 (95.520)	Mem 5091MB
[2024-11-10 19:11:20 cros_tiny_patch4_group7_224] (main.py 320): INFO  * Acc@1 81.522 Acc@5 95.520
[2024-11-10 19:11:20 cros_tiny_patch4_group7_224] (main.py 127): INFO Accuracy of the network on the 50000 test images: 81.5%
